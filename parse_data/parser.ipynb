{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Collection - Static Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî® **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Language.build_library method to compile these into a library that's usable from Python. \n",
    "# This function will return immediately if the library has already been compiled since the last \n",
    "# time its source code was modified:\n",
    "\n",
    "from tree_sitter import Language, Parser\n",
    "import os\n",
    "\n",
    "# Ensuring that the library is compiled each time this cell is run.\n",
    "if os.path.exists(\"build/my-languages.so\"):\n",
    "    os.remove(\"build/my-languages.so\")\n",
    "\n",
    "Language.build_library(\n",
    "    # Store the library in the `build` directory\n",
    "    \"build/my-languages.so\",\n",
    "    # Include one or more languages\n",
    "    [\"vendor/tree-sitter-python\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: zz\n",
      "Used:\n",
      "Call: zz\n",
      "with: (PromptTemplate(\"arge\"))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse(filename):\n",
    "    PY_LANGUAGE = Language('build/my-languages.so', 'python')\n",
    "    parser = Parser()\n",
    "    parser.set_language(PY_LANGUAGE)\n",
    "    result = \"\"\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tree = parser.parse(f.read())\n",
    "\n",
    "    query = PY_LANGUAGE.query(\"\"\"(module\n",
    "        (import_from_statement\n",
    "            module_name: (dotted_name) @mod\n",
    "            name: (dotted_name) @llm\n",
    "            (#match? @mod \"^langchain(.llms)?\")\n",
    "        )\n",
    "        (expression_statement\n",
    "            (assignment\n",
    "                left: (identifier) @llmvar\n",
    "                right: (call function: (identifier) @llmname)\n",
    "                (#eq? @llmname @llm)\n",
    "            )\n",
    "        )\n",
    "    )\"\"\")\n",
    "\n",
    "    for llm in filter(lambda x: x[1] == \"llmvar\", query.captures(tree.root_node)):\n",
    "        llm_text = llm[0].text.decode(\"utf-8\")\n",
    "        # print(f\"LLM: {llm_text}\")\n",
    "        result += f\"LLM: {llm_text}\\n\"\n",
    "\n",
    "        query_2 = PY_LANGUAGE.query(\"\"\"(call\n",
    "            function: (identifier) @fn.name\n",
    "            arguments: (argument_list) @fn.args\n",
    "            (#eq? @fn.name \"{llm}\")\n",
    "        )\"\"\".format(llm=llm_text))\n",
    "\n",
    "        # print(\"Used:\")\n",
    "        result += \"Used:\\n\"\n",
    "        for usage in query_2.captures(tree.root_node):\n",
    "            if usage[1] == \"fn.name\":\n",
    "                # print(\"Call: \", usage[0].text.decode(\"utf-8\"))\n",
    "                result += f'Call: {usage[0].text.decode(\"utf-8\")}\\n'\n",
    "            elif usage[1] == \"fn.args\":\n",
    "                # print(\"with: \", usage[0].text.decode(\"utf-8\"))\n",
    "                result += f'with: {usage[0].text.decode(\"utf-8\")}\\n'\n",
    "\n",
    "    return result\n",
    "\n",
    "# Test the parser\n",
    "print(parse(\"othertmp.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Storing Repo Files for Reliable Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to run this cell multiple times if there were exceptions when downloading some files.\n",
    "\n",
    "It will only download files that are not already present in the `repos` folder, and fill up the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import json, requests\n",
    "\n",
    "# Import Raw URLs\n",
    "with open(\"../data/repo_to_rawFileURL_>=4stars.json\", \"r\") as file:\n",
    "    repos_prompts = json.load(file)\n",
    "\n",
    "\n",
    "# NOTE: Refer to this stackoverflow post for issues with requests: \n",
    "# https://stackoverflow.com/questions/62599036/python-requests-is-slow-and-takes-very-long-to-complete-http-or-https-request\n",
    "\n",
    "root_dir = \"repos\"\n",
    "if not os.path.exists(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "\n",
    "# Note: Using '~' instead of '/' as a delimiter for file/dir names \n",
    "# (because I'm not creative enough to come up with a better solution)\n",
    "for repo in repos_prompts:\n",
    "    repo_path = os.path.join(root_dir, repo.replace(\"/\", \"~\"))\n",
    "    if not os.path.exists(repo_path):\n",
    "        os.mkdir(repo_path)\n",
    "\n",
    "    for url in repos_prompts[repo]:\n",
    "        filename = url.split(\"/\")[6:]\n",
    "        filename = \"~\".join(filename)\n",
    "        file_path = os.path.join(repo_path, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            try:\n",
    "                r = requests.get(url, timeout=1)\n",
    "                # Exception thrown before file is created. \n",
    "                # So, if file exists, it's safe to assume that it's been downloaded successfully.\n",
    "                if r.status_code == 200:\n",
    "                    with open(file_path, \"w\") as f:\n",
    "                        f.write(r.text)\n",
    "                else:\n",
    "                    print(\"Error: \", r.status_code, repo_path, filename)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Error: \", repo_path, filename)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repos:  372 ; Expected 372 for repos >=4stars\n",
      "Total number of files:  1444 ; Expected 1444 for repos >=4stars\n"
     ]
    }
   ],
   "source": [
    "# Count the number of repos in the repos directory\n",
    "assert len(os.listdir(root_dir)) == len(repos_prompts)\n",
    "print(\"Number of repos: \", len(os.listdir(root_dir)), \"; Expected 372 for repos >=4stars\")\n",
    "\n",
    "# Count the number of files in each repo\n",
    "count = 0\n",
    "for repo in repos_prompts:\n",
    "    repo_path = os.path.join(root_dir, repo.replace(\"/\", \"~\"))\n",
    "    assert len(os.listdir(repo_path)) == len(repos_prompts[repo])\n",
    "    for file in os.listdir(repo_path):\n",
    "        with open(os.path.join(repo_path, file), \"r\") as f:\n",
    "            assert f.read() != \"\"  # Complain if file is empty\n",
    "    count += len(os.listdir(repo_path))\n",
    "print(\"Total number of files: \", count, \"; Expected 1444 for repos >=4stars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä **Prompt Collection** - Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo:  yym68686~ChatGPT-Telegram-Bot ; File:  test~test_keyword.py\n",
      "LLM: chainllm\n",
      "Used:\n",
      "LLM: keyword_prompt\n",
      "Used:\n",
      "LLM: key_chain\n",
      "Used:\n",
      "\n",
      "Repo:  yym68686~ChatGPT-Telegram-Bot ; File:  test~test_gpt4free_langchain_agent.py\n",
      "LLM: tools\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  wordweb~langchain-ChatGLM-and-TigerBot ; File:  chains~dialogue_answering~prompts.py\n",
      "LLM: SUMMARY_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  webgrip~PuttyGPT ; File:  Eve~main.py\n",
      "LLM: tracer\n",
      "Used:\n",
      "LLM: callback_manager\n",
      "Used:\n",
      "LLM: openai\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: readonlymemory\n",
      "Used:\n",
      "LLM: embeddings_model\n",
      "Used:\n",
      "LLM: vectorstore\n",
      "Used:\n",
      "LLM: retriever\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: todo_chain\n",
      "Used:\n",
      "LLM: vectorstore_info\n",
      "Used:\n",
      "LLM: toolkit\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent_executor\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  webgrip~PuttyGPT ; File:  Eve~old~main.py\n",
      "LLM: vectorstore\n",
      "Used:\n",
      "LLM: embedding_model\n",
      "Used:\n",
      "LLM: longTermMemoryRetriever\n",
      "Used:\n",
      "LLM: midTermMemoryRetriever\n",
      "Used:\n",
      "LLM: shortTermMemoryRetriever\n",
      "Used:\n",
      "LLM: sparseAndDenseRetriever\n",
      "Used:\n",
      "\n",
      "Repo:  PrefectHQ~langchain-prefect ; File:  examples~openai~chroma_docs_ingest.py\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~prompts~langchain_getting_started~test_multiple_inputs.py\n",
      "LLM: multiple_input_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~chains~langchain_how_to~test_sequential_chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: synopsis_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: review_chain\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "Call: overall_chain\n",
      "with: (\n",
      "        {\"title\": \"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"}\n",
      "    )\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~prompts~langchain_getting_started~test_few_shot.py\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: few_shot_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~prompts~langchain_getting_started~test_no_inputs.py\n",
      "LLM: no_input_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~prompts~langchain_getting_started~test_one_input.py\n",
      "LLM: one_input_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~chains~langchain_getting_started~test_llm_chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~chains~langchain_getting_started~test_sequential_chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "LLM: second_prompt\n",
      "Used:\n",
      "LLM: chain_two\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~chains~langchain_getting_started~test_custom_chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_1\n",
      "Used:\n",
      "LLM: chain_1\n",
      "Used:\n",
      "LLM: prompt_2\n",
      "Used:\n",
      "LLM: chain_2\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~chains~langchain_how_to~test_simple_sequential_chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: synopsis_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: review_chain\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~langchain-visualizer ; File:  tests~prompts~langchain_getting_started~test_dynamic_prompt.py\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: example_selector\n",
      "Used:\n",
      "LLM: dynamic_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~LangChain-Cheatsheet ; File:  chains~lc_simpelSequentialChain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template_name\n",
      "Used:\n",
      "LLM: prompt_template_slogan\n",
      "Used:\n",
      "LLM: name_chain\n",
      "Used:\n",
      "LLM: slogan_chain\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~LangChain-Cheatsheet ; File:  other~speech_to_text~lc_whisper.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~LangChain-Cheatsheet ; File:  chains~lc_MultiPromptChain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  kaarthik108~snowChat ; File:  template.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "LLM: LLAMA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  momegas~megabots ; File:  megabots~prompt.py\n",
      "LLM: QA_MEMORY_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  nestordemeure~GPTranslate ; File:  GPTranslate~translation~translator.py\n",
      "LLM: model\n",
      "Used:\n",
      "\n",
      "Repo:  kyegomez~swarms ; File:  playground~swarms~debate.py\n",
      "LLM: player_descriptor_system_message\n",
      "Used:\n",
      "\n",
      "Repo:  kyegomez~swarms ; File:  swarms~agents~profitpilot.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  MarkEdmondson1234~langchain-github ; File:  qna~read_repo.py\n",
      "LLM: chat\n",
      "Used:\n",
      "\n",
      "Repo:  MarkEdmondson1234~langchain-github ; File:  palm.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  toanpv-0639~langchain-demo ; File:  chains-demo.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: video_title_prompt_template\n",
      "Used:\n",
      "LLM: video_outline_prompt_template\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "Call: overall_chain\n",
      "with: (\n",
      "    {\"content\": \"Deep Learning in 1 minutes\", \"style\": \"funny\"}\n",
      ")\n",
      "LLM: video_title_prompt_template\n",
      "Used:\n",
      "LLM: video_outline_prompt_template\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "Call: overall_chain\n",
      "with: (\n",
      "    {\"content\": \"Deep Learning in 1 minutes\", \"style\": \"funny\"}\n",
      ")\n",
      "\n",
      "Repo:  toanpv-0639~langchain-demo ; File:  prompts-demo.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: few_shot_prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  lwangreen~Langchain-ChatGLM ; File:  chains~llmchain_with_history.py\n",
      "LLM: model\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "Call: chain\n",
      "with: ({\"input\": \"ÊÅºÁæûÊàêÊÄí\"})\n",
      "\n",
      "Repo:  nestordemeure~impersonator ; File:  impersonator~prompts_models.py\n",
      "LLM: EMBEDDING_MODEL\n",
      "Used:\n",
      "LLM: OPENAI_MODEL\n",
      "Used:\n",
      "LLM: OPENAI_MODEL_STRICT\n",
      "Used:\n",
      "LLM: OPENAI_MODEL_LONG\n",
      "Used:\n",
      "LLM: answering_chain\n",
      "Used:\n",
      "LLM: strict_answering_chain\n",
      "Used:\n",
      "LLM: embedding_chain\n",
      "Used:\n",
      "LLM: check_chain\n",
      "Used:\n",
      "\n",
      "Repo:  gutfeeling~langsearch ; File:  examples~local_files~webapp~pages~HYDE_Demo.py\n",
      "LLM: hyde_prompt\n",
      "Used:\n",
      "LLM: qa_prompt\n",
      "Used:\n",
      "LLM: qa_chain\n",
      "Used:\n",
      "\n",
      "Repo:  gutfeeling~langsearch ; File:  examples~local_files~webapp~pages~Simple_QA_Demo.py\n",
      "LLM: qa_prompt\n",
      "Used:\n",
      "LLM: qa_chain\n",
      "Used:\n",
      "\n",
      "Repo:  gutfeeling~langsearch ; File:  langsearch~pipelines~common~summary_index.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  techwithtim~LangChain-Quick-Start ; File:  prompt_template.py\n",
      "LLM: chat_model\n",
      "Used:\n",
      "\n",
      "Repo:  techwithtim~LangChain-Quick-Start ; File:  chain.py\n",
      "LLM: chat_model\n",
      "Used:\n",
      "\n",
      "Repo:  techwithtim~LangChain-Quick-Start ; File:  output_parser.py\n",
      "LLM: chat_model\n",
      "Used:\n",
      "\n",
      "Repo:  garyb9~twitter-llm-bot ; File:  examples~prompt_langchain.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  huqianghui~pdf2MySQLByLangchain ; File:  template~azure_open_AI_instance.py\n",
      "LLM: chat\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~indexes~prompts~entity_summarization.py\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~qa_with_sources~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~conversation~prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
      "Used:\n",
      "LLM: SUMMARY_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "LLM: KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~pal~math_prompt.py\n",
      "LLM: MATH_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~question_answering~map_rerank_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~graph_qa~prompts.py\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~summarize~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~question_answering~refine_prompts.py\n",
      "LLM: DEFAULT_REFINE_PROMPT\n",
      "Used:\n",
      "LLM: DEFAULT_TEXT_QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~evaluation~qa~generate_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~qa_with_sources~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~agents~self_ask_with_search~prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~hyde~prompts.py\n",
      "LLM: web_search\n",
      "Used:\n",
      "LLM: sci_fact\n",
      "Used:\n",
      "LLM: arguana\n",
      "Used:\n",
      "LLM: trec_covid\n",
      "Used:\n",
      "LLM: fiqa\n",
      "Used:\n",
      "LLM: dbpedia_entity\n",
      "Used:\n",
      "LLM: trec_news\n",
      "Used:\n",
      "LLM: mr_tydi\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~indexes~prompts~entity_extraction.py\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~question_answering~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~chains~natbot~prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DannyBoy5240~Langchain ; File:  langchain~evaluation~qa~eval_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jayli~langchain-GLM_Agent ; File:  helloworld.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  joshuasundance-swca~langchain-streamlit-demo ; File:  langchain-streamlit-demo~qagen.py\n",
      "LLM: PYDANTIC_PARSER\n",
      "Used:\n",
      "\n",
      "Repo:  joshuasundance-swca~langchain-streamlit-demo ; File:  langchain-streamlit-demo~app.py\n",
      "LLM: STMEMORY\n",
      "Used:\n",
      "LLM: MEMORY\n",
      "Used:\n",
      "LLM: RUN_COLLECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  mark-watson~langchain-book-examples ; File:  langchain_getting_started~country_information.py\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (prompt_text)\n",
      "\n",
      "Repo:  mark-watson~langchain-book-examples ; File:  from_langchain_docs~memory_langchain_test.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  mark-watson~langchain-book-examples ; File:  llama.cpp~test.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: callback_manager\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (prompt)\n",
      "\n",
      "Repo:  mark-watson~langchain-book-examples ; File:  langchain_getting_started~directions_template.py\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (prompt_text)\n",
      "\n",
      "Repo:  mark-watson~langchain-book-examples ; File:  hugging_face~simple_example.py\n",
      "LLM: hub_llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  gkamradt~langchain-streamlit-example ; File:  main.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  Erol444~gpt4-openai-api ; File:  test~test_langchain.py\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  LiamConnell~codelabyrinth ; File:  coder~agents~qa_vectorstore_evaluate.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  LiamConnell~codelabyrinth ; File:  coder~agents~qa_multi_vectorstore.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  LiamConnell~codelabyrinth ; File:  coder~agents~qa_with_vectorstore.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  03_retrieval~chat_3.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: ([\n",
      "        HumanMessage(content=prompt.format(documents=documents_string,\n",
      "                                           query=input_message)) #‚Üê input_message„Å´Â§âÊõ¥\n",
      "    ])\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  03_retrieval~chat_2.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: ([\n",
      "        HumanMessage(content=prompt.format(document=documents_string,\n",
      "                                           query=input_message)) #‚Üê input_message„Å´Â§âÊõ¥\n",
      "    ])\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: database\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  02_mode_io~prompt.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  03_retrieval~query_2.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: database\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: ([\n",
      "    HumanMessage(content=prompt.format(document=documents_string, query=query))\n",
      "])\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  03_retrieval~re_phrase_query.py\n",
      "LLM: retriever\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: re_phrase_query_retriever\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  02_mode_io~datetime_output_parser.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (\n",
      "    [\n",
      "        HumanMessage(content=prompt.format(product=\"iPhone8\")),  #‚Üê iPhone8„ÅÆ„É™„É™„Éº„ÇπÊó•„ÇíËÅû„Åè\n",
      "        HumanMessage(content=output_parser.get_format_instructions()),  #‚Üê output_parser.get_format_instructions()„ÇíÂÆüË°å„Åó„ÄÅË®ÄË™û„É¢„Éá„É´„Å∏„ÅÆÊåáÁ§∫„ÇíËøΩÂä†„Åô„Çã\n",
      "    ]\n",
      ")\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  05_chain~sequential_chain.py\n",
      "LLM: chat\n",
      "Used:\n",
      "LLM: write_article_chain\n",
      "Used:\n",
      "LLM: translate_chain\n",
      "Used:\n",
      "LLM: sequential_chain\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  05_chain~llmchain.py\n",
      "LLM: chat\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  02_mode_io~model_io_few_shot.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: few_shot_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  02_mode_io~prompt_template_from_template_save_sample.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  02_mode_io~prompt_template_from_template_load_sample.py\n",
      "LLM: loaded_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  05_chain~request_chain.py\n",
      "LLM: chat\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "Call: chain\n",
      "with: ({\n",
      "    \"query\": \"Êù±‰∫¨„ÅÆÂ§©Ê∞ó„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶\",\n",
      "    \"url\": \"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/130000.json\",\n",
      "})\n",
      "\n",
      "Repo:  harukaxq~langchain-book ; File:  02_mode_io~prompt_and_language_model.py\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: ( #‚Üê ÂÆüË°å„Åô„Çã\n",
      "    [\n",
      "        HumanMessage(content=prompt.format(product=\"iPhone\")),\n",
      "    ]\n",
      ")\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~question_answering~refine_prompts.py\n",
      "LLM: DEFAULT_REFINE_PROMPT\n",
      "Used:\n",
      "LLM: REFINE_PROMPT_SELECTOR\n",
      "Used:\n",
      "LLM: DEFAULT_TEXT_QA_PROMPT\n",
      "Used:\n",
      "LLM: QUESTION_PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~llm_math~llm_math.py\n",
      "LLM: LLM_MATH_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~summarize~map_reduce_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~llm_bash~prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~summarize~refine_prompts.py\n",
      "LLM: REFINE_PROMPT\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~question_answering~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: QUESTION_PROMPT_SELECTOR\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~question_answering~map_rerank_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~qa_generation~prompt.py\n",
      "LLM: PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~retrieval_qa~prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~question_answering~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~summarize~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jiamingkong~RWKV_chains ; File:  rwkv_chains~__init__.py\n",
      "LLM: rwkv_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  ByronHsu~FlyteGPT ; File:  query_documents.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  airbytehq~tutorial-connector-dev-bot ; File:  localbot_adapted.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: vector_store\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  airbytehq~tutorial-connector-dev-bot ; File:  slackbot.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: vector_store\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  petermartens98~LangChain-AutoGPT-YouTube-Script-Generation-Streamlit-App ; File:  YT_Script_Generator~app.py\n",
      "LLM: title_template\n",
      "Used:\n",
      "LLM: script_template\n",
      "Used:\n",
      "LLM: title_memory\n",
      "Used:\n",
      "LLM: script_memory\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: title_chain\n",
      "Used:\n",
      "LLM: script_chain\n",
      "Used:\n",
      "LLM: wiki\n",
      "Used:\n",
      "\n",
      "Repo:  hwchase17~chat-langchain-readthedocs ; File:  query_data.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  topoteretes~PromethAI-Backend ; File:  examples~level_1~level_1_pdf_vectorstore_dlt_etl.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  topoteretes~PromethAI-Backend ; File:  examples~simple_ETLs.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  zitterbewegung~saturday ; File:  inference.py\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: wolfram\n",
      "Used:\n",
      "LLM: wikipedia\n",
      "Used:\n",
      "LLM: python_repl\n",
      "Used:\n",
      "LLM: bash\n",
      "Used:\n",
      "LLM: requests\n",
      "Used:\n",
      "LLM: message_history\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent_chain\n",
      "Used:\n",
      "\n",
      "Repo:  rishabkumar7~youtube-assistant-langchain ; File:  langchain_helper.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "\n",
      "Repo:  hwchase17~langchain-hub ; File:  prompts~sql_query~relevant_tables~relevant_tables.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  codedog-ai~codedog ; File:  codedog~chains~pr_summary~prompts.py\n",
      "LLM: parser\n",
      "Used:\n",
      "LLM: PR_SUMMARY_PROMPT\n",
      "Used:\n",
      "LLM: CODE_SUMMARY_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  codedog-ai~codedog ; File:  codedog~chains~code_review~prompts.py\n",
      "LLM: CODE_REVIEW_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  codedog-ai~codedog ; File:  codedog~chains~prompts.py\n",
      "LLM: TRANSLATE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jbpayton~llm-auto-forge ; File:  prompts.py\n",
      "LLM: TOOL_MAKER_PROMPT\n",
      "Used:\n",
      "LLM: BROWSER_TOOL_MAKER_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~zamm ; File:  zamm~actions~use_terminal~prompt.py\n",
      "LLM: TerminalPromptTemplate\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~zamm ; File:  zamm~actions~edit_file~prompt.py\n",
      "LLM: WHICH_FILE_PROMPT\n",
      "Used:\n",
      "LLM: NEW_CONTENTS_PROMPT\n",
      "Used:\n",
      "LLM: REPLACE_CONTENTS_PROMPT\n",
      "Used:\n",
      "LLM: NEW_FILE_LOGGER\n",
      "Used:\n",
      "LLM: EDIT_LOGGER\n",
      "Used:\n",
      "LLM: CONDENSED_LOGGER\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~zamm ; File:  zamm~chains~bash_action_prompt.py\n",
      "LLM: MANAGER_TEMPLATE\n",
      "Used:\n",
      "LLM: EMPLOYEE_DOCUMENTATION_PROMPT\n",
      "Used:\n",
      "LLM: EMPLOYEE_TEACHING_INTRO_TEMPLATE\n",
      "Used:\n",
      "LLM: FOLLOW_INSTRUCTIONS_TEMPLATE\n",
      "Used:\n",
      "LLM: SUCCESS_CRITERIA_TEMPLATE\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~zamm ; File:  zamm~actions~follow_tutorial~prompt.py\n",
      "LLM: FOLLOW_TUTORIAL_PROMPT\n",
      "Used:\n",
      "LLM: FOLLOW_TUTORIAL_LOGGER\n",
      "Used:\n",
      "\n",
      "Repo:  amosjyng~zamm ; File:  zamm~actions~note~prompt.py\n",
      "LLM: NOTE_PROMPT\n",
      "Used:\n",
      "LLM: NOTE_LOGGER\n",
      "Used:\n",
      "\n",
      "Repo:  adityabrahmankar~PDF-Chat ; File:  app.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  MarkEdmondson1234~edmonbrain ; File:  qna~summarise.py\n",
      "LLM: MAP_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  MarkEdmondson1234~edmonbrain ; File:  debugger.py\n",
      "LLM: gcp_retriever\n",
      "Used:\n",
      "LLM: model\n",
      "Used:\n",
      "\n",
      "Repo:  ju-bezdek~langchain-decorators ; File:  code_examples~custom_template_block_bulder_llama2.py\n",
      "LLM: llama70\n",
      "Used:\n",
      "LLM: llama70\n",
      "Used:\n",
      "LLM: LLAMA2_PROMPT_TYPE\n",
      "Used:\n",
      "\n",
      "Repo:  leisc~Langchain-SparkLLM ; File:  src~client~app.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "\n",
      "Repo:  venuv~LangSynth ; File:  pop.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain_one\n",
      "Used:\n",
      "LLM: chain_two\n",
      "Used:\n",
      "\n",
      "Repo:  venuv~LangSynth ; File:  chroma_report.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  nicknochnack~Nopenai ; File:  app-chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  nicknochnack~Nopenai ; File:  app.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent_executor\n",
      "Used:\n",
      "\n",
      "Repo:  Ayyodeji~Langchain-LLM-PDF-QA ; File:  app~prompts.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  edrickdch~langchain-101 ; File:  src~prompts~example-selector.py\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: example_selector\n",
      "Used:\n",
      "LLM: dynamic_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  edrickdch~langchain-101 ; File:  src~chains~simple-chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  edrickdch~langchain-101 ; File:  src~chains~combine-chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: first_prompt\n",
      "Used:\n",
      "LLM: chain_one\n",
      "Used:\n",
      "LLM: second_prompt\n",
      "Used:\n",
      "LLM: chain_two\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "\n",
      "Repo:  edrickdch~langchain-101 ; File:  src~prompts~few-shot.py\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: few_shot_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  edrickdch~langchain-101 ; File:  src~prompts~output-parser.py\n",
      "LLM: parser\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (formatted_prompt.to_string())\n",
      "\n",
      "Repo:  edrickdch~langchain-101 ; File:  src~prompts~prompt-templates.py\n",
      "LLM: constructor_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  coolbeevip~langchain_plantuml ; File:  examples~example_1.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  Gamma-Software~AppifyAi ; File:  generative_app~core~chains~prompt.py\n",
      "LLM: CONDENSE_QUESTION_CODE_PROMPT\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~linkedin_retriever_response.py\n",
      "LLM: loader\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~how_to_use_llama_from_ollama.py\n",
      "LLM: chat_model\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  condense_example01.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: condense_chain\n",
      "Used:\n",
      "Call: condense_chain\n",
      "with: ({\n",
      "        \"input_documents\": docs,\n",
      "        \"question\": query\n",
      "    },return_only_outputs=True)\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~few_shot_prompt_example.py\n",
      "LLM: fewshot_prompt\n",
      "Used:\n",
      "LLM: few_shot_prompt_template\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~how_to_do_cot_with_palm.py\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~cobol_code_fix.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~contextual_compression_example.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: COMPRESS_DOC_PROMPT\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~agent_with_custom_search_citing_source.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~chain_of_thoughts_example_01.py\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~opthalmology_essay.py\n",
      "LLM: embedding_function\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~how_to_evaluate_llm_model_graded.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~how_to_do_router_chain_multiple_sources.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: buddhism_agent\n",
      "Used:\n",
      "LLM: branch\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~google_code_bison.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  rajib76~langchain_examples ; File:  examples~how_to_use_expression_language_with_tool.py\n",
      "LLM: model\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain-benchmarks ; File:  csv-qa~pandas_agent_instruct.py\n",
      "LLM: embedding_model\n",
      "Used:\n",
      "LLM: retriever_tool\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain-benchmarks ; File:  csv-qa~custom_agent.py\n",
      "LLM: embedding_model\n",
      "Used:\n",
      "LLM: retriever_tool\n",
      "Used:\n",
      "\n",
      "Repo:  steamship-core~steamship-langchain ; File:  examples~chatbot~server~prompt.py\n",
      "LLM: CHATBOT_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  own-ai~ownai ; File:  aifilemaker.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~indexes~prompts~entity_summarization.py\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~memory~prompt.py\n",
      "LLM: ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
      "Used:\n",
      "LLM: SUMMARY_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "LLM: KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~qa_with_sources~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~pal~math_prompt.py\n",
      "LLM: MATH_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~question_answering~map_rerank_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~graph_qa~prompts.py\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "LLM: GRAPH_QA_PROMPT\n",
      "Used:\n",
      "LLM: CYPHER_GENERATION_PROMPT\n",
      "Used:\n",
      "LLM: NGQL_GENERATION_PROMPT\n",
      "Used:\n",
      "LLM: KUZU_GENERATION_PROMPT\n",
      "Used:\n",
      "LLM: GREMLIN_GENERATION_PROMPT\n",
      "Used:\n",
      "LLM: CYPHER_QA_PROMPT\n",
      "Used:\n",
      "LLM: SPARQL_INTENT_PROMPT\n",
      "Used:\n",
      "LLM: SPARQL_GENERATION_SELECT_PROMPT\n",
      "Used:\n",
      "LLM: SPARQL_GENERATION_UPDATE_PROMPT\n",
      "Used:\n",
      "LLM: SPARQL_QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~evaluation~qa~generate_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~qa_with_sources~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~pal~colored_object_prompt.py\n",
      "LLM: COLORED_OBJECT_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~indexes~prompts~entity_extraction.py\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~question_answering~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: QUESTION_PROMPT_SELECTOR\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~evaluation~qa~eval_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: CONTEXT_PROMPT\n",
      "Used:\n",
      "LLM: COT_PROMPT\n",
      "Used:\n",
      "LLM: SQL_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~agents~agent_toolkits~openapi~planner_prompt.py\n",
      "LLM: PARSING_GET_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_POST_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_PATCH_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_DELETE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  836304831~langchain-anal ; File:  langchain~chains~query_constructor~prompt.py\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  FrancescoSaverioZuppichini~LinkedInGPT ; File:  experiments~bot.py\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  FrancescoSaverioZuppichini~LinkedInGPT ; File:  gurus~linkedin_ai.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  FrancescoSaverioZuppichini~LinkedInGPT ; File:  experiments~agent.py\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  dataelement~bisheng ; File:  src~bisheng-langchain~bisheng_langchain~vectorstores~elastic_keywords_search.py\n",
      "LLM: DEFAULT_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  dataelement~bisheng ; File:  src~bisheng-langchain~experimental~answer_trace.py\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  llm-ai-dev~langchain-wiki ; File:  app~main.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  0ptim~JellyChat ; File:  backend~tools~defichainpython_qa.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "LLM: defichainPythonTool\n",
      "Used:\n",
      "\n",
      "Repo:  0ptim~JellyChat ; File:  backend~tools~wiki_qa.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "LLM: wikiTool\n",
      "Used:\n",
      "\n",
      "Repo:  Coding-Crashkurse~LangChain-On-Azure ; File:  application.py\n",
      "LLM: memory\n",
      "Used:\n",
      "\n",
      "Repo:  sivasurend~langchain_utilities ; File:  langchain%20web%20page%20search.txt\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (final_query_prompt)\n",
      "LLM: loader\n",
      "Used:\n",
      "LLM: query_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  Coding-Crashkurse~LangChain-Intermediate-Project ; File:  app~prompts.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  vishwasg217~finsight ; File:  test_files~parser.py\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (_input.to_string())\n",
      "LLM: parser\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  shroominic~codeinterpreter-api ; File:  codeinterpreterapi~prompts~modifications_check.py\n",
      "LLM: determine_modifications_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  shroominic~codeinterpreter-api ; File:  codeinterpreterapi~prompts~remove_dl_link.py\n",
      "LLM: remove_dl_link_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  GoogleCloudPlatform~solutions-genai-llm-workshop ; File:  LAB004-3-CustomTools~0-custom-tool-functions.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  GoogleCloudPlatform~solutions-genai-llm-workshop ; File:  LAB002-0-PDF~2-load_qa_chain-chain-types.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: loader\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: qa\n",
      "Used:\n",
      "Call: qa\n",
      "with: ({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
      "\n",
      "Repo:  GoogleCloudPlatform~solutions-genai-llm-workshop ; File:  LAB001-2-ChatModel~0-run.py\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (messages)\n",
      "Call: chat\n",
      "with: (chat_prompt.format_prompt(text=\"why is the sky blue.\").to_messages())\n",
      "\n",
      "Repo:  GoogleCloudPlatform~solutions-genai-llm-workshop ; File:  LAB004-1-Agents~1-single-action-agent.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: search\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  GoogleCloudPlatform~solutions-genai-llm-workshop ; File:  LAB003-1-ChainTypes~1-matching-engine.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  GoogleCloudPlatform~solutions-genai-llm-workshop ; File:  LAB002-0-PDF~1-retrivalQAChain-custom-prompt.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: loader\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  chatchat-space~Langchain-Chatchat ; File:  chains~llmchain_with_history.py\n",
      "LLM: chain\n",
      "Used:\n",
      "Call: chain\n",
      "with: ({\"input\": \"ÊÅºÁæûÊàêÊÄí\"})\n",
      "\n",
      "Repo:  chatchat-space~Langchain-Chatchat ; File:  server~agent~tools~search_all_knowledge_more.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  chatchat-space~Langchain-Chatchat ; File:  server~agent~tools~translator.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  chatchat-space~Langchain-Chatchat ; File:  server~agent~tools~weather.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  chatchat-space~Langchain-Chatchat ; File:  tests~agent~test_agent_function.py\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  chatchat-space~Langchain-Chatchat ; File:  server~agent~tools~search_all_knowledge_once.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  chatchat-space~Langchain-Chatchat ; File:  server~agent~tools~calculate.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  staticTao~langchain_llm_demo ; File:  polymerization_ai.py\n",
      "LLM: CONTEXT_QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  staticTao~langchain_llm_demo ; File:  polymerization_ai_func.py\n",
      "LLM: CONTEXT_QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  gustavz~DataChad ; File:  datachad~backend~prompts.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  DataCTE~Camel-local ; File:  Camel-localmodel.py\n",
      "LLM: task_specifier_sys_msg\n",
      "Used:\n",
      "LLM: assistant_msg\n",
      "Used:\n",
      "LLM: user_msg\n",
      "Used:\n",
      "\n",
      "Repo:  Sayvai-io~custom-tools ; File:  src~sayvai_tools~tools~sql_database~prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: DECIDER_PROMPT\n",
      "Used:\n",
      "LLM: CRATEDB_PROMPT\n",
      "Used:\n",
      "LLM: DUCKDB_PROMPT\n",
      "Used:\n",
      "LLM: GOOGLESQL_PROMPT\n",
      "Used:\n",
      "LLM: MSSQL_PROMPT\n",
      "Used:\n",
      "LLM: MYSQL_PROMPT\n",
      "Used:\n",
      "LLM: MARIADB_PROMPT\n",
      "Used:\n",
      "LLM: ORACLE_PROMPT\n",
      "Used:\n",
      "LLM: POSTGRES_PROMPT\n",
      "Used:\n",
      "LLM: SQLITE_PROMPT\n",
      "Used:\n",
      "LLM: CLICKHOUSE_PROMPT\n",
      "Used:\n",
      "LLM: PRESTODB_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  petermartens98~OpenAI-Whisper-Audio-Transcription-And-Summarization-Chatbot ; File:  AppV12~prompts.py\n",
      "LLM: chat_template\n",
      "Used:\n",
      "LLM: sentiment_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  parallel75~AI_Agent ; File:  main.py\n",
      "LLM: system_message\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "Call: agent\n",
      "with: ({\"input\": query})\n",
      "\n",
      "Repo:  Coding-Crashkurse~LangChain-Discord-Bot ; File:  bot.py\n",
      "LLM: loader\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (messages)\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: system_message_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  hwchase17~chat-langchain-notion ; File:  query_data.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  shpetimhaxhiu~agi-taskgenius-gpt ; File:  app.py\n",
      "LLM: embeddings_model\n",
      "Used:\n",
      "LLM: vectorstore\n",
      "Used:\n",
      "\n",
      "Repo:  lalligagger~satgpt-app ; File:  app.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  Amanastel~llm_project ; File:  LLMS_langchain~example.py\n",
      "LLM: first_input_prompt\n",
      "Used:\n",
      "LLM: person_memory\n",
      "Used:\n",
      "LLM: dob_memory\n",
      "Used:\n",
      "LLM: descr_memory\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "LLM: second_input_prompt\n",
      "Used:\n",
      "LLM: chain2\n",
      "Used:\n",
      "LLM: third_input_prompt\n",
      "Used:\n",
      "LLM: chain3\n",
      "Used:\n",
      "LLM: parent_chain\n",
      "Used:\n",
      "Call: parent_chain\n",
      "with: ({'name':input_text})\n",
      "\n",
      "Repo:  alitrack~Chat-GPT-LangChain ; File:  app.py\n",
      "LLM: PROMPT_TEMPLATE\n",
      "Used:\n",
      "\n",
      "Repo:  KareEnges~ToolGPT ; File:  main.py\n",
      "LLM: toolkit\n",
      "Used:\n",
      "LLM: Google\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: tools\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "\n",
      "Repo:  FredGoo~langchain-chinese-chat-models ; File:  test~openai_test.py\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (messages)\n",
      "\n",
      "Repo:  FredGoo~langchain-chinese-chat-models ; File:  test~zhipuai_test.py\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (messages)\n",
      "\n",
      "Repo:  FredGoo~langchain-chinese-chat-models ; File:  test~xfyun_test.py\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (messages)\n",
      "\n",
      "Repo:  houseofbaud~doug ; File:  main.py\n",
      "LLM: dougWorkingMemory\n",
      "Used:\n",
      "LLM: dougSummaryMemory\n",
      "Used:\n",
      "LLM: dougMainMemory\n",
      "Used:\n",
      "LLM: dougChain\n",
      "Used:\n",
      "LLM: dougDB\n",
      "Used:\n",
      "\n",
      "Repo:  Coding-Crashkurse~Langchain-Production-Project ; File:  service3~app.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (messages)\n",
      "LLM: store\n",
      "Used:\n",
      "LLM: store\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: system_message_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  CognitiveLabs~GPT-auto-webscraping ; File:  chains~output_format~templates.py\n",
      "LLM: system_template_output_format\n",
      "Used:\n",
      "LLM: human_template_output_format\n",
      "Used:\n",
      "\n",
      "Repo:  CognitiveLabs~GPT-auto-webscraping ; File:  chains~code_generator~templates.py\n",
      "LLM: system_template_script\n",
      "Used:\n",
      "LLM: human_template_script\n",
      "Used:\n",
      "\n",
      "Repo:  avrabyt~MultiLingual-ChatBot ; File:  app.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  msoedov~langcorn ; File:  examples~ex12.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: parser\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  msoedov~langcorn ; File:  examples~ex5.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "\n",
      "Repo:  msoedov~langcorn ; File:  examples~ex13.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  msoedov~langcorn ; File:  examples~ex2.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: synopsis_prompt_template\n",
      "Used:\n",
      "LLM: synopsis_chain\n",
      "Used:\n",
      "LLM: review_prompt_template\n",
      "Used:\n",
      "LLM: review_chain\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  msoedov~langcorn ; File:  examples~ex1.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  msoedov~langcorn ; File:  examples~ex4.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: sequential_chain\n",
      "Used:\n",
      "\n",
      "Repo:  Taytay~slack-langchain ; File:  src~conversation_utils.py\n",
      "LLM: llm_gpt3_turbo\n",
      "Used:\n",
      "\n",
      "Repo:  ray-project~langchain-ray ; File:  open_source_LLM_retrieval_qa~serve.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~indexes~prompts~entity_summarization.py\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~memory~prompt.py\n",
      "LLM: ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
      "Used:\n",
      "LLM: SUMMARY_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "LLM: KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~llm_checker~prompt.py\n",
      "LLM: CREATE_DRAFT_ANSWER_PROMPT\n",
      "Used:\n",
      "LLM: LIST_ASSERTIONS_PROMPT\n",
      "Used:\n",
      "LLM: CHECK_ASSERTIONS_PROMPT\n",
      "Used:\n",
      "LLM: REVISED_ANSWER_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~constitutional_ai~prompts.py\n",
      "LLM: critique_example\n",
      "Used:\n",
      "LLM: CRITIQUE_PROMPT\n",
      "Used:\n",
      "LLM: REVISION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~qa_with_sources~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~pal~math_prompt.py\n",
      "LLM: MATH_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~question_answering~map_rerank_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~qa_with_sources~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~pal~colored_object_prompt.py\n",
      "LLM: COLORED_OBJECT_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~hyde~prompts.py\n",
      "LLM: web_search\n",
      "Used:\n",
      "LLM: sci_fact\n",
      "Used:\n",
      "LLM: arguana\n",
      "Used:\n",
      "LLM: trec_covid\n",
      "Used:\n",
      "LLM: fiqa\n",
      "Used:\n",
      "LLM: dbpedia_entity\n",
      "Used:\n",
      "LLM: trec_news\n",
      "Used:\n",
      "LLM: mr_tydi\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~question_answering~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: QUESTION_PROMPT_SELECTOR\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~chains~natbot~prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zapier~langchain-nla-util ; File:  langchain~evaluation~qa~eval_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  ChobPT~oobaboogas-webui-langchain_agent ; File:  script.py\n",
      "LLM: searchWrapper\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  admineral~PDF-Pilot ; File:  Developers~Basic-dev-Scripts~Langchain-QA-dev.py\n",
      "LLM: document\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  itamargol~openai ; File:  cold_mailer.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: zapier\n",
      "Used:\n",
      "LLM: tools\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  Saik0s~DevAssistant ; File:  modules~learning.py\n",
      "LLM: learning_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  Saik0s~DevAssistant ; File:  modules~evaluation.py\n",
      "LLM: evaluation_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  prompt_template~prompt_few_shots.py\n",
      "LLM: example_prompt\n",
      "Used:\n",
      "LLM: few_shot_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  chains~chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  chains~simple_sequential_chains.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: var_chain\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: loop_chain\n",
      "Used:\n",
      "LLM: conversa_chain\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  prompt_template~manage_prompt_template.py\n",
      "LLM: multiple_input_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  prompt_template~prompt_template.py\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (prompt.format(name=\"Fernando\"))\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  judini~agent_request_completion.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  llm~huggingface.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  prompt_template~prompt_validation.py\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "\n",
      "Repo:  davila7~langchain-101 ; File:  prompt_template~load_promtp.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  DorsaRoh~Custom-AI ; File:  PatientGPT.AI~patientgpt.py\n",
      "LLM: title_template\n",
      "Used:\n",
      "LLM: script_template\n",
      "Used:\n",
      "LLM: wiki\n",
      "Used:\n",
      "\n",
      "Repo:  DorsaRoh~Custom-AI ; File:  RealizeAI~app.py\n",
      "LLM: title_template\n",
      "Used:\n",
      "LLM: script_template\n",
      "Used:\n",
      "LLM: wiki\n",
      "Used:\n",
      "\n",
      "Repo:  DorsaRoh~Custom-AI ; File:  app.py\n",
      "LLM: title_template\n",
      "Used:\n",
      "LLM: script_template\n",
      "Used:\n",
      "LLM: wiki\n",
      "Used:\n",
      "\n",
      "Repo:  Magic-Emerge~know-more ; File:  app~prompts~map_rerank_prompts_extends.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  Magic-Emerge~know-more ; File:  app~prompts~refine_prompts_extends.py\n",
      "LLM: DEFAULT_REFINE_PROMPT\n",
      "Used:\n",
      "LLM: REFINE_PROMPT_SELECTOR\n",
      "Used:\n",
      "LLM: DEFAULT_TEXT_QA_PROMPT\n",
      "Used:\n",
      "LLM: QUESTION_PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  Magic-Emerge~know-more ; File:  app~prompts~stuff_prompts_extends.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  Magic-Emerge~know-more ; File:  app~prompts~map_reduce_prompts_extends.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: QUESTION_PROMPT_SELECTOR\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  Magic-Emerge~know-more ; File:  app~prompts~prompts.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jainsid24~know-my-doc ; File:  chat.py\n",
      "LLM: loader\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "Call: chain\n",
      "with: (\n",
      "            {\n",
      "                \"input_documents\": documents,\n",
      "                \"human_input\": question,\n",
      "                \"tone\": tone,\n",
      "                \"persona\": persona,\n",
      "            },\n",
      "            return_only_outputs=True,\n",
      "        )\n",
      "\n",
      "Repo:  vidalmaxime~chat-langchain-telegram ; File:  query_data.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  zilliztech~GPTCache ; File:  examples~integrate~langchain~langchain_similaritycache_openai.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  zilliztech~GPTCache ; File:  examples~integrate~langchain~langchain_prompt_openai.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~7-Days-of-LangChain ; File:  day_2~voice_to_meeting_notes.py\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: PROMPT_SUMMARY\n",
      "Used:\n",
      "LLM: refine_prompt_summary\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: sum_chain\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~7-Days-of-LangChain ; File:  day_7~learning_path.py\n",
      "LLM: PROMPT_EXTRACT_TOPICS\n",
      "Used:\n",
      "LLM: PROMPT_EXTRACT_TOPICS_REFINE\n",
      "Used:\n",
      "LLM: PROMPT_SUBSKILLS\n",
      "Used:\n",
      "LLM: PROMPT_FIND_VIDEO\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~7-Days-of-LangChain ; File:  day_3~mindmap.py\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: PROMPT_MINDMAP\n",
      "Used:\n",
      "LLM: REFINE_PROMPT_MINDMAP\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~coder_cat_joke.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: code_editor\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~self_healing_code.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: tools\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~snippets~hf_chain_example.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~coder_plot_chart_mixin_test.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~custom_agent_with_memory.py\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: python_tool\n",
      "Used:\n",
      "LLM: multi_line\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~coder_plot_chart.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: code_editor\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~coder_agent_hello_world.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: code_editor\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~custom_tool.py\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: python_tool\n",
      "Used:\n",
      "LLM: multi_line\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  paolorechia~learn-langchain ; File:  langchain_app~agents~coder_chuck.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: code_editor\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  clairelovesgravy~slack_bot_demo ; File:  app.py\n",
      "LLM: CHATAI\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: chat_chain\n",
      "Used:\n",
      "\n",
      "Repo:  Saik0s~SwiftDocAutomator ; File:  summarize.py\n",
      "LLM: REFINE_PROMPT\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: chat_model\n",
      "Used:\n",
      "LLM: sum_chain\n",
      "Used:\n",
      "\n",
      "Repo:  PromptEngineer48~Sales_Agent_using_LangChain ; File:  sales_pen_git.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  mazzzystar~teach-show-consult ; File:  teach.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  mazzzystar~teach-show-consult ; File:  consult.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  shiyindaxiaojie~eden-aigc-qna ; File:  example~01_langchain~how_to_use_prompt_template.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  shiyindaxiaojie~eden-aigc-qna ; File:  code~utilities~custom_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  shiyindaxiaojie~eden-aigc-qna ; File:  example~01_langchain~how_to_use_chains.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  templates~neo4j-cypher-ft~neo4j_cypher_ft~chain.py\n",
      "LLM: graph\n",
      "Used:\n",
      "LLM: cypher_validation\n",
      "Used:\n",
      "Call: cypher_validation\n",
      "with: (x[\"query\"])\n",
      "LLM: cypher_llm\n",
      "Used:\n",
      "LLM: qa_llm\n",
      "Used:\n",
      "LLM: entity_chain\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~chains~question_answering~map_rerank_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~chains~qa_with_sources~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~memory~prompt.py\n",
      "LLM: ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
      "Used:\n",
      "LLM: SUMMARY_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "LLM: KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~indexes~prompts~entity_extraction.py\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~indexes~prompts~entity_summarization.py\n",
      "LLM: ENTITY_SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~agents~agent_toolkits~openapi~planner_prompt.py\n",
      "LLM: PARSING_GET_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_POST_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_PATCH_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_PUT_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_DELETE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~chains~question_answering~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: QUESTION_PROMPT_SELECTOR\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~chains~qa_with_sources~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~experimental~langchain_experimental~pal_chain~colored_object_prompt.py\n",
      "LLM: COLORED_OBJECT_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~langchain~langchain~evaluation~qa~eval_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: CONTEXT_PROMPT\n",
      "Used:\n",
      "LLM: COT_PROMPT\n",
      "Used:\n",
      "LLM: SQL_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain ; File:  libs~experimental~langchain_experimental~pal_chain~math_prompt.py\n",
      "LLM: MATH_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  voxel51~voxelgpt ; File:  links~run_selector.py\n",
      "LLM: RUN_EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  voxel51~voxelgpt ; File:  links~view_stage_example_selector.py\n",
      "LLM: VIEW_STAGE_EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  voxel51~voxelgpt ; File:  links~dataset_view_generator.py\n",
      "LLM: mistakenness_field_prompt\n",
      "Used:\n",
      "LLM: missing_field_prompt\n",
      "Used:\n",
      "LLM: spurious_field_prompt\n",
      "Used:\n",
      "LLM: EVAL_FIELDS_PROMPT_TEMPLATE\n",
      "Used:\n",
      "LLM: UNIQUENESS_PROMPT\n",
      "Used:\n",
      "LLM: HARDNESS_PROMPT\n",
      "Used:\n",
      "LLM: IMAGE_SIMILARITY_PROMPT\n",
      "Used:\n",
      "LLM: TEXT_SIMILARITY_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langserve-replit-template ; File:  packages~pirate-speak~pirate_speak~chain.py\n",
      "LLM: _model\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  08_%E9%93%BE%E4%B8%8A~04_SequentialChain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: introduction_chain\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: review_chain\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: social_post_chain\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "Call: overall_chain\n",
      "with: ({\n",
      "    \"name\": \"Áé´Áë∞\",\n",
      "    \"color\": \"ÈªëËâ≤\"\n",
      "})\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  07_%E8%A7%A3%E6%9E%90%E8%BE%93%E5%87%BA~01_Pydantic_Parser.py\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (input)\n",
      "LLM: output_parser\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  18_CAMEL~CAMEL_CN.py\n",
      "LLM: task_specifier_sys_msg\n",
      "Used:\n",
      "LLM: assistant_msg\n",
      "Used:\n",
      "LLM: user_msg\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  06_%E8%B0%83%E7%94%A8%E6%A8%A1%E5%9E%8B~02_LangChain_HFHub.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  06_%E8%B0%83%E7%94%A8%E6%A8%A1%E5%9E%8B~03_LangChain_HFPipeline.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  04_%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF%E4%B8%8A~01_PromptTemplate.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  08_%E9%93%BE%E4%B8%8A~01_Without_Chain.py\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (prompt)\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  19_BabyAGI~BabyAGI_CN.py\n",
      "LLM: embeddings_model\n",
      "Used:\n",
      "LLM: vectorstore\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  10_%E8%AE%B0%E5%BF%86~02_ConversationBufferMemory.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "Call: conversation\n",
      "with: (\"ÊàëÂßêÂßêÊòéÂ§©Ë¶ÅËøáÁîüÊó•ÔºåÊàëÈúÄË¶Å‰∏ÄÊùüÁîüÊó•Ëä±Êùü„ÄÇ\")\n",
      "Call: conversation\n",
      "with: (\"Â•πÂñúÊ¨¢Á≤âËâ≤Áé´Áë∞ÔºåÈ¢úËâ≤ÊòØÁ≤âËâ≤ÁöÑ„ÄÇ\")\n",
      "Call: conversation\n",
      "with: (\"ÊàëÂèàÊù•‰∫ÜÔºåËøòËÆ∞ÂæóÊàëÊò®Â§©‰∏∫‰ªÄ‰πàË¶ÅÊù•‰π∞Ëä±ÂêóÔºü\")\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  10_%E8%AE%B0%E5%BF%86~01_ConversationChain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: conv_chain\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  08_%E9%93%BE%E4%B8%8A~03_Running_Chain.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "Call: llm_chain\n",
      "with: ({\n",
      "    'flower': \"Áé´Áë∞\",\n",
      "    'season': \"Â§èÂ≠£\"\n",
      "})\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  05_%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF%E4%B8%8B~CoT.py\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (prompt)\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  04_%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF%E4%B8%8A~03_FewShotPrompt.py\n",
      "LLM: prompt_sample\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (prompt.format(flower_type=\"ÈáéÁé´Áë∞\", occasion=\"Áà±ÊÉÖ\"))\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  03_%E6%A8%A1%E5%9E%8BIO~04_%E6%A8%A1%E5%9E%8BIO_HuggingFace.py\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (input)\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  03_%E6%A8%A1%E5%9E%8BIO~05_%E6%A8%A1%E5%9E%8BIO_%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90.py\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (input)\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  03_%E6%A8%A1%E5%9E%8BIO~02_%E6%A8%A1%E5%9E%8BIO_%E5%BE%AA%E7%8E%AF%E8%B0%83%E7%94%A8.py\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (input_prompt)\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  08_%E9%93%BE%E4%B8%8A~02_With_LLMChain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "Call: llm_chain\n",
      "with: (\"Áé´Áë∞\")\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  03_%E6%A8%A1%E5%9E%8BIO~01_%E6%A8%A1%E5%9E%8BIO.py\n",
      "LLM: model\n",
      "Used:\n",
      "Call: model\n",
      "with: (input)\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  04_%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF%E4%B8%8A~02_ChatPromptTemplate.py\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (prompt)\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  09_%E9%93%BE%E4%B8%8B~Rounter_Chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: router_prompt\n",
      "Used:\n",
      "LLM: default_chain\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  07_%E8%A7%A3%E6%9E%90%E8%BE%93%E5%87%BA~03_RetryParser.py\n",
      "LLM: parser\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  huangjia2019~langchain ; File:  17_%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0~03_LangChainOpenAICallback.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "Call: conversation\n",
      "with: (\"ÊàëÂßêÂßêÊòéÂ§©Ë¶ÅËøáÁîüÊó•ÔºåÊàëÈúÄË¶Å‰∏ÄÊùüÁîüÊó•Ëä±Êùü„ÄÇ\")\n",
      "Call: conversation\n",
      "with: (\"Â•πÂñúÊ¨¢Á≤âËâ≤Áé´Áë∞ÔºåÈ¢úËâ≤ÊòØÁ≤âËâ≤ÁöÑ„ÄÇ\")\n",
      "Call: conversation\n",
      "with: (\"ÊàëÂèàÊù•‰∫ÜÔºåËøòËÆ∞ÂæóÊàëÊò®Â§©‰∏∫‰ªÄ‰πàË¶ÅÊù•‰π∞Ëä±ÂêóÔºü\")\n",
      "\n",
      "Repo:  CharlesSQ~conversational-agent-with-QA-tool ; File:  app.py\n",
      "LLM: llm_chat\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  log10-io~log10 ; File:  examples~logging~get_url.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  log10-io~log10 ; File:  examples~logging~langchain_babyagi.py\n",
      "LLM: embeddings_model\n",
      "Used:\n",
      "LLM: vectorstore\n",
      "Used:\n",
      "LLM: todo_chain\n",
      "Used:\n",
      "LLM: todo_chain\n",
      "Used:\n",
      "LLM: search\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  log10-io~log10 ; File:  examples~logging~multiple_sessions.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  log10-io~log10 ; File:  examples~logging~langchain_simple_sequential.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "LLM: second_prompt\n",
      "Used:\n",
      "LLM: chain_two\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "\n",
      "Repo:  retr0reg~Ret2GPT ; File:  langchain_preprocess~prompt_builder.py\n",
      "LLM: model\n",
      "Used:\n",
      "\n",
      "Repo:  ravsau~langchain-notes ; File:  local-llama-langchain~llama_langchain.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: callback_manager\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  Sxela~WarpAIBot ; File:  server.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  Elite-AI-August~PDF-Pilot ; File:  Developers~Basic-dev-Scripts~Langchain-QA-dev.py\n",
      "LLM: document\n",
      "Used:\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  minkj1992~jarvis ; File:  src~infra~llm.py\n",
      "LLM: refine_prompt\n",
      "Used:\n",
      "LLM: initial_qa_prompt\n",
      "Used:\n",
      "LLM: REPORT_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  allient~create-fastapi-project ; File:  create_fastapi_project~templates~langchain_basic~backend~app~app~api~v1~endpoints~chat.py\n",
      "LLM: memory\n",
      "Used:\n",
      "\n",
      "Repo:  tdolan21~miniAGI ; File:  pages~huggingface_agi.py\n",
      "LLM: history\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  smaameri~private-llm ; File:  local-llm-chain.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "Call: llm_chain\n",
      "with: (query)\n",
      "\n",
      "Repo:  smaameri~private-llm ; File:  cloud-llm.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "Call: llm_chain\n",
      "with: (query)\n",
      "\n",
      "Repo:  langchain-ai~langchain-teacher ; File:  lc_guides~getting_started_guide.txt\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain-teacher ; File:  main.py\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain-teacher ; File:  lc_guides~prompt_guide.txt\n",
      "LLM: template\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain-teacher ; File:  lc_guides~chains_guide.txt\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~langchain-teacher ; File:  guide.txt\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: model\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  AutoLLM~AutoAgents ; File:  autoagents~agents~tools~tools.py\n",
      "LLM: docstore\n",
      "Used:\n",
      "LLM: search_tool\n",
      "Used:\n",
      "LLM: note_tool\n",
      "Used:\n",
      "LLM: wiki_note_tool\n",
      "Used:\n",
      "LLM: wiki_search_tool\n",
      "Used:\n",
      "LLM: wiki_lookup_tool\n",
      "Used:\n",
      "LLM: wiki_dump_search_tool\n",
      "Used:\n",
      "LLM: finish_tool\n",
      "Used:\n",
      "LLM: search_tool_v3\n",
      "Used:\n",
      "LLM: note_tool_v3\n",
      "Used:\n",
      "LLM: finish_tool_v3\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~LangChain-Unchained ; File:  day_1~prompts.py\n",
      "LLM: PROMPT_STRATEGY\n",
      "Used:\n",
      "LLM: PROMPT_STRATEGY_REFINE\n",
      "Used:\n",
      "LLM: PROMPT_PLAN\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~LangChain-Unchained ; File:  day_4~part_2~prompts.py\n",
      "LLM: PROMPT_QUESTIONS\n",
      "Used:\n",
      "LLM: REFINE_PROMPT_QUESTIONS\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~LangChain-Unchained ; File:  day_4~part_1~prompts.py\n",
      "LLM: PROMPT_QUESTIONS\n",
      "Used:\n",
      "LLM: REFINE_PROMPT_QUESTIONS\n",
      "Used:\n",
      "\n",
      "Repo:  JorisdeJong123~LangChain-Unchained ; File:  day_3~prompts.py\n",
      "LLM: PROMPT_SUMMARY\n",
      "Used:\n",
      "LLM: REFINE_PROMPT_SUMMARY\n",
      "Used:\n",
      "\n",
      "Repo:  Umi7899~langchain-ChatGLM-My ; File:  chains~dialogue_answering~prompts.py\n",
      "LLM: SUMMARY_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  jbpayton~langchain-stock-screener ; File:  BabyAGITest.py\n",
      "LLM: embeddings_model\n",
      "Used:\n",
      "LLM: vectorstore\n",
      "Used:\n",
      "LLM: todo_chain\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "\n",
      "Repo:  tleers~llm-api-starterkit ; File:  app~main_local_lamacpp.py\n",
      "LLM: callback_manager\n",
      "Used:\n",
      "LLM: local_llm\n",
      "Used:\n",
      "LLM: summarize_prompt\n",
      "Used:\n",
      "LLM: summarize_chain\n",
      "Used:\n",
      "\n",
      "Repo:  tleers~llm-api-starterkit ; File:  app~main_local_gpt_4_all.py\n",
      "LLM: local_llm\n",
      "Used:\n",
      "LLM: summarize_prompt\n",
      "Used:\n",
      "LLM: summarize_chain\n",
      "Used:\n",
      "\n",
      "Repo:  tleers~llm-api-starterkit ; File:  app~main_local_gpt_4_all_ner_blog_example.py\n",
      "LLM: local_llm\n",
      "Used:\n",
      "LLM: ner_graph_prompt\n",
      "Used:\n",
      "LLM: ner_graph_chain\n",
      "Used:\n",
      "\n",
      "Repo:  tleers~llm-api-starterkit ; File:  app~main_openai.py\n",
      "LLM: langchain_llm\n",
      "Used:\n",
      "LLM: summarize_prompt\n",
      "Used:\n",
      "LLM: summarize_chain\n",
      "Used:\n",
      "\n",
      "Repo:  tleers~llm-api-starterkit ; File:  app~main_local_gpt_4_all_openai_ner_blog_example.py\n",
      "LLM: local_llm\n",
      "Used:\n",
      "LLM: ner_graph_prompt\n",
      "Used:\n",
      "LLM: ner_graph_chain\n",
      "Used:\n",
      "LLM: langchain_llm\n",
      "Used:\n",
      "LLM: ner_graph_openai_chain\n",
      "Used:\n",
      "\n",
      "Repo:  ushakrishnan~SearchWithOpenAI ; File:  pages~230_Talk_with_Azure_Open_AI.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "\n",
      "Repo:  ushakrishnan~SearchWithOpenAI ; File:  pages~130_Talk_with_Open_AI.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: conversation\n",
      "Used:\n",
      "\n",
      "Repo:  showlab~VLog ; File:  models~gpt_model.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~chat-langchain ; File:  main.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  codemaker2015~sqldatabasechain-langchain-demo ; File:  langchain_demo.py\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (question)\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  DJcodess~Flipchat ; File:  chatbot.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: streaming_llm\n",
      "Used:\n",
      "LLM: question_generator\n",
      "Used:\n",
      "LLM: doc_chain\n",
      "Used:\n",
      "LLM: chatbot\n",
      "Used:\n",
      "\n",
      "Repo:  DonGuillotine~langchain-claude-chatbot ; File:  test_two.py\n",
      "LLM: chat\n",
      "Used:\n",
      "Call: chat\n",
      "with: (messages)\n",
      "\n",
      "Repo:  DonGuillotine~langchain-claude-chatbot ; File:  conversation_chain.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: streaming_llm\n",
      "Used:\n",
      "LLM: question_generator\n",
      "Used:\n",
      "LLM: doc_chain\n",
      "Used:\n",
      "LLM: chatbot\n",
      "Used:\n",
      "\n",
      "Repo:  bigsky77~twitter-agent ; File:  src~strategy~media~gif_reply.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: gif_prompt\n",
      "Used:\n",
      "LLM: gif_chain\n",
      "Used:\n",
      "LLM: reply_prompt\n",
      "Used:\n",
      "LLM: reply_chain\n",
      "Used:\n",
      "\n",
      "Repo:  bigsky77~twitter-agent ; File:  src~strategy~prompt.py\n",
      "LLM: reply_prompt\n",
      "Used:\n",
      "LLM: tweet_prompt\n",
      "Used:\n",
      "LLM: gif_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  ibiscp~LLM-IMDB ; File:  backend~movie_database_tool.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  Madhav-MKNC~admin-portal ; File:  test~chatbot_test.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "Call: chain\n",
      "with: (prompt, return_only_outputs=True)\n",
      "\n",
      "Repo:  Madhav-MKNC~admin-portal ; File:  test~memory.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  Madhav-MKNC~admin-portal ; File:  chatbot.py\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "Call: chain\n",
      "with: (\n",
      "        prompt,\n",
      "        return_only_outputs=True)\n",
      "\n",
      "Repo:  EswarDivi~DocuConverse ; File:  Talkwithpdf.py\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "\n",
      "Repo:  onlyphantom~llm-python ; File:  05_hf.py\n",
      "LLM: hub_llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: hub_chain\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~question_answering~stuff_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: PROMPT_SELECTOR\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~qa_with_sources~map_reduce_prompt.py\n",
      "LLM: QUESTION_PROMPT\n",
      "Used:\n",
      "LLM: COMBINE_PROMPT\n",
      "Used:\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~pal~math_prompt.py\n",
      "LLM: MATH_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~constitutional_ai~prompts.py\n",
      "LLM: critique_example\n",
      "Used:\n",
      "LLM: CRITIQUE_PROMPT\n",
      "Used:\n",
      "LLM: REVISION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~agents~agent_toolkits~openapi~planner_prompt.py\n",
      "LLM: PARSING_GET_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_POST_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_PATCH_PROMPT\n",
      "Used:\n",
      "LLM: PARSING_DELETE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~evaluation~qa~eval_prompt.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "LLM: CONTEXT_PROMPT\n",
      "Used:\n",
      "LLM: COT_PROMPT\n",
      "Used:\n",
      "LLM: SQL_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~indexes~prompts~entity_extraction.py\n",
      "LLM: ENTITY_EXTRACTION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~pal~colored_object_prompt.py\n",
      "LLM: COLORED_OBJECT_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~llm_checker~prompt.py\n",
      "LLM: CREATE_DRAFT_ANSWER_PROMPT\n",
      "Used:\n",
      "LLM: LIST_ASSERTIONS_PROMPT\n",
      "Used:\n",
      "LLM: CHECK_ASSERTIONS_PROMPT\n",
      "Used:\n",
      "LLM: REVISED_ANSWER_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~question_answering~map_rerank_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~evaluation~qa~generate_prompt.py\n",
      "LLM: output_parser\n",
      "Used:\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  aws-solutions-library-samples~guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws ; File:  lambda~langchain_processor_qa~langchain~chains~query_constructor~prompt.py\n",
      "LLM: EXAMPLE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  TheCurryMan~LangChain-101-For-Beginners-Python ; File:  lesson-05-simple-sequential-chains.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: first_chain\n",
      "Used:\n",
      "LLM: second_chain\n",
      "Used:\n",
      "LLM: overall_chain\n",
      "Used:\n",
      "\n",
      "Repo:  TheCurryMan~LangChain-101-For-Beginners-Python ; File:  lesson-04-prompt-templates.py\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  MikeBorman1~LangchainResearchAgent ; File:  app.py\n",
      "LLM: system_message\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: agent\n",
      "Used:\n",
      "Call: agent\n",
      "with: ({\"input\": query})\n",
      "\n",
      "Repo:  krishnaik06~Langchain-Tutorials ; File:  example1.py\n",
      "LLM: first_input_prompt\n",
      "Used:\n",
      "LLM: person_memory\n",
      "Used:\n",
      "LLM: dob_memory\n",
      "Used:\n",
      "LLM: descr_memory\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "LLM: second_input_prompt\n",
      "Used:\n",
      "LLM: chain2\n",
      "Used:\n",
      "LLM: third_input_prompt\n",
      "Used:\n",
      "LLM: chain3\n",
      "Used:\n",
      "LLM: parent_chain\n",
      "Used:\n",
      "Call: parent_chain\n",
      "with: ({'name':input_text})\n",
      "\n",
      "Repo:  davidshtian~Bedrock-ChatBot-with-LangChain-and-Streamlit ; File:  streaming~bedrock_simple.py\n",
      "LLM: CLAUDE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  davidshtian~Bedrock-ChatBot-with-LangChain-and-Streamlit ; File:  simple~bedrock_chatbot.py\n",
      "LLM: CLAUDE_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  SSK-14~chatbot-guardrails ; File:  app.py\n",
      "LLM: PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  Parassharmaa~mom-ai ; File:  generate_mom.py\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  wombyz~gpt4all_langchain_chatbots ; File:  custom_knowledge_chatbot.py\n",
      "LLM: llm\n",
      "Used:\n",
      "\n",
      "Repo:  wombyz~gpt4all_langchain_chatbots ; File:  basic_langchain_setup.py\n",
      "LLM: callback_manager\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  wombyz~gpt4all_langchain_chatbots ; File:  custom_knowledge_query.py\n",
      "LLM: callback_manager\n",
      "Used:\n",
      "LLM: loader\n",
      "Used:\n",
      "LLM: embeddings\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Product%20Recommendation.py\n",
      "LLM: recommendation_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: recommendation_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Sentiment_prompt.py\n",
      "LLM: sentiment_analysis_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: sentiment_analysis_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  plain_text_Generator.py\n",
      "LLM: llm\n",
      "Used:\n",
      "Call: llm\n",
      "with: (text)\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: prompt_template\n",
      "Used:\n",
      "LLM: chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Car%20Maintenance%20Tips.py\n",
      "LLM: maintenance_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: maintenance_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Movie%20Recommendation.py\n",
      "LLM: recommendation_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: recommendation_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Crop%20Disease%20Diagnosis.py\n",
      "LLM: diagnosis_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: diagnosis_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Patient%20Diagnosis.py\n",
      "LLM: diagnosis_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: diagnosis_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Summarization_propmt.py\n",
      "LLM: summarization_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: summarization_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Investment%20Recommendation.py\n",
      "LLM: recommendation_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: recommendation_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Personalized%20Recipe%20Recommendation.py\n",
      "LLM: recipe_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: recipe_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Study%20Plan%20Recommendation.py\n",
      "LLM: study_plan_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: study_plan_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Campaign%20Idea%20Generation.py\n",
      "LLM: campaign_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: campaign_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Weapon%20Selection%20Recommendation.py\n",
      "LLM: weapon_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: weapon_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Outfit%20Recommendation.py\n",
      "LLM: recommendation_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: recommendation_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Travel%20Itinerary%20Planning.py\n",
      "LLM: itinerary_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: itinerary_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Customer%20Support.py\n",
      "LLM: resolution_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: resolution_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Meeting%20Scheduler.py\n",
      "LLM: meeting_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: meeting_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Training%20Routine%20Recommendation.py\n",
      "LLM: recommendation_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: recommendation_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Equipment%20Troubleshooting.py\n",
      "LLM: troubleshooting_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: troubleshooting_chain\n",
      "Used:\n",
      "\n",
      "Repo:  saqib772~Prompt-Engineering-LangChain ; File:  Recipe%20Recommendation.py\n",
      "LLM: recommendation_prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: recommendation_chain\n",
      "Used:\n",
      "\n",
      "Repo:  ruankie~ecrivai ; File:  ecrivai~prompt_templates.py\n",
      "LLM: topic_prompt\n",
      "Used:\n",
      "LLM: keyword_prompt\n",
      "Used:\n",
      "LLM: content_prompt\n",
      "Used:\n",
      "\n",
      "Repo:  Antony90~arxiv-discord ; File:  ai~prompts.py\n",
      "LLM: MAP_PROMPT\n",
      "Used:\n",
      "LLM: REDUCE_KEYPOINTS_PROMPT\n",
      "Used:\n",
      "LLM: REDUCE_LAYMANS_PROMPT\n",
      "Used:\n",
      "LLM: REDUCE_COMPREHENSIVE_PROMPT\n",
      "Used:\n",
      "LLM: ABSTRACT_SUMMARY_PROMPT\n",
      "Used:\n",
      "LLM: ABSTRACT_QS_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  waseemhnyc~langchain-huggingface-template ; File:  main.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  nicknochnack~Langchain-Crash-Course ; File:  app.py\n",
      "LLM: title_template\n",
      "Used:\n",
      "LLM: script_template\n",
      "Used:\n",
      "LLM: title_memory\n",
      "Used:\n",
      "LLM: script_memory\n",
      "Used:\n",
      "LLM: llm\n",
      "Used:\n",
      "LLM: title_chain\n",
      "Used:\n",
      "LLM: script_chain\n",
      "Used:\n",
      "LLM: wiki\n",
      "Used:\n",
      "\n",
      "Repo:  langchain-ai~streamlit-agent ; File:  streamlit_agent~basic_memory.py\n",
      "LLM: msgs\n",
      "Used:\n",
      "LLM: memory\n",
      "Used:\n",
      "LLM: prompt\n",
      "Used:\n",
      "LLM: llm_chain\n",
      "Used:\n",
      "\n",
      "Repo:  homanp~gcp-langchain ; File:  utils.py\n",
      "LLM: prompt\n",
      "Used:\n",
      "\n",
      "Repo:  karakuri-ai~gptuber-by-langchain ; File:  src~lib~chains.py\n",
      "LLM: streamer_chain\n",
      "Used:\n",
      "LLM: concretizer_chain\n",
      "Used:\n",
      "LLM: cm_chain\n",
      "Used:\n",
      "LLM: news_chain\n",
      "Used:\n",
      "\n",
      "Repo:  umbertogriffo~contextual-chatbot-gpt4all ; File:  chat~conversation~prompts.py\n",
      "LLM: QA_PROMPT\n",
      "Used:\n",
      "LLM: SUMMARIZATION_PROMPT\n",
      "Used:\n",
      "\n",
      "Repo:  sudarshan-koirala~langchain-openai-chainlit ; File:  pdf_qa.py\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "\n",
      "Repo:  sudarshan-koirala~langchain-openai-chainlit ; File:  txt_qa.py\n",
      "LLM: text_splitter\n",
      "Used:\n",
      "\n",
      "Parser Returns result for 415 files out of 1444 files\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"repos\"\n",
    "\n",
    "count = 0\n",
    "for repo in os.listdir(root_dir):\n",
    "    repo_path = os.path.join(root_dir, repo)\n",
    "    for file in os.listdir(repo_path):\n",
    "        file_path = os.path.join(repo_path, file)\n",
    "        try:\n",
    "            prompt = parse(file_path)\n",
    "            if prompt != \"\":\n",
    "                count += 1\n",
    "                print(\"Repo: \", repo, \"; File: \", file)\n",
    "                print(prompt)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error: \", repo_path, file_path)\n",
    "\n",
    "print(f\"Parser Returns result for {count} files out of 1444 files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
