from repolya._const import WORKSPACE_AUTOGEN, AUTOGEN_JD
from repolya._log import logger_yj

from repolya.autogen.organizer import (
    Organizer,
    ConversationResult,
)

from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
)
from langchain.schema import StrOutputParser
from langchain.callbacks import get_openai_callback
from langchain.document_loaders import WebBaseLoader
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain

from repolya.toolset.tool_langchain import (
    bing,
    ddg,
    google,
)
from repolya.toolset.util import calc_token_cost
from repolya.rag.digest_dir import (
    calculate_md5,
    dir_to_faiss_OpenAI,
)
from repolya.rag.doc_loader import clean_txt
from repolya.rag.digest_urls import (
    urls_to_faiss_OpenAI,
    urls_to_faiss_HuggingFace,
)
from repolya.rag.vdb_faiss import (
    get_faiss_OpenAI,
    get_faiss_HuggingFace,
)
from repolya.rag.qa_chain import (
    qa_vdb_multi_query,
    qa_vdb_multi_query_textgen,
    qa_with_context_as_go,
)

from repolya.autogen.workflow import (
    create_rag_task_list_zh,
    search_faiss_openai,
)

import shutil
import json
import re
import os


def clean_filename(text, max_length=10):
    # ç§»é™¤éæ³•æ–‡ä»¶åå­—ç¬¦ï¼ˆä¾‹å¦‚: \ / : * ? " < > |ï¼‰
    _clean = re.sub(r'[\\/*?:"<>|]', '', text)
    # æ›¿æ¢æ“ä½œç³»ç»Ÿæ•æ„Ÿçš„å­—ç¬¦
    _clean = _clean.replace(' ', '_')  # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
    # å–å‰ max_length ä¸ªå­—ç¬¦ä½œä¸ºæ–‡ä»¶å
    return _clean[:max_length]


def search_all(_query):
    _all = []
    _all.extend(google(_query, n=1))
    _all.extend(bing(_query, n=1))
    _all.extend(ddg(_query, n=1))
    return _all


def print_search_all(_all):
    _str = []
    for i in _all:
        _str.append(f"{i['link']}\n{i['title']}")
        # _str.append(f"{i['link']}\n{i['title']}\n{i['snippet']}")
    return "\n" + "\n".join(_str)


def task_with_context_template(_task, _context, template):
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
    prompt = PromptTemplate.from_template(template)
    with get_openai_callback() as cb:
        chain = prompt | llm | StrOutputParser()
        _ans = chain.invoke({"_task": _task, "_context": _context})
        _token_cost = f"Tokens: {cb.total_tokens} = (Prompt {cb.prompt_tokens} + Completion {cb.completion_tokens}) Cost: ${format(cb.total_cost, '.5f')}"
    return [_ans, _token_cost]


##### event context
yj_keyword = {
    "åŸºæœ¬æƒ…å†µ_1": "å‘ç”Ÿå‘å±•æ—¶é—´çº¿",
    "åŸºæœ¬æƒ…å†µ_2": "ç¾å®³è§„æ¨¡å’Œå¼ºåº¦",
    "å¤„ç½®è¿‡ç¨‹_1": "å®æ–½åº”æ€¥å“åº”å’Œæ•‘æ´æªæ–½çš„æ—¶é—´çº¿",
    "å¤„ç½®è¿‡ç¨‹_2": "æ”¿åºœå’Œéæ”¿åºœç»„ç»‡è§’è‰²",#
    "å†›æ°‘åä½œ_1": "å†›é˜Ÿå‚ä¸æ•‘ç¾è¡ŒåŠ¨çš„æ—¶é—´çº¿",
    "å†›æ°‘åä½œ_2": "å†›é˜Ÿæ•‘æ´è¡ŒåŠ¨å’Œåä½œç»†èŠ‚",
    "æ³•è§„ä¾æ®_1": "å†›é˜ŸååŠ©æ•‘ç¾çš„æ³•å¾‹ä¾æ®",
    "æ³•è§„ä¾æ®_2": "åº”æ€¥æ•‘æ´å†›åœ°è”åŠ¨æœºåˆ¶",
    "å½±å“è¯„ä¼°_1": "ç¾å®³å¯¹ç»æµå’Œç¤¾ä¼šçš„å½±å“",
    "å½±å“è¯„ä¼°_2": "å—ç¾ç¾¤ä½“å’Œåœ°åŒºçš„æ¢å¤è¿›ç¨‹",
    "åæ€å¯ç¤º_1": "ç¾å®³ç®¡ç†å’Œåº”å¯¹çš„æœ‰æ•ˆæ€§è¯„ä¼°",
    "åæ€å¯ç¤º_2": "ç¾å®³çš„ç»éªŒæ•™è®­å’Œæ”¹è¿›æªæ–½",
}


def generate_search_dict_for_event(_event: str) -> dict[str]:
    _event_name = clean_filename(_event, 20)
    _event_dir = str(AUTOGEN_JD / _event_name)
    _dict = {}
    logger_yj.info("generate_search_dict_for_eventï¼šå¼€å§‹")
    for i in yj_keyword.keys():
        _i = f"{_event} AND {yj_keyword[i]}"
        _dict[i] = _i
        logger_yj.info(_i)
    logger_yj.info("generate_search_dict_for_eventï¼šå®Œæˆ")
    return _dict


def generate_context_for_each_query(_query: str, _db_name: str, _clean_txt_dir: str):
    _context, _token_cost = "", "Tokens: 0 = (Prompt 0 + Completion 0) Cost: $0"
    _all = search_all(_query)
    logger_yj.info(print_search_all(_all))
    _all_link = [i['link'] for i in _all]
    _urls = list(set(_all_link))
    if not os.path.exists(_db_name):
        urls_to_faiss_OpenAI(_urls, _db_name, _clean_txt_dir)
    else:
        logger_yj.info(f"'{_db_name}'å·²å­˜åœ¨ï¼Œæ— éœ€ urls_to_faiss_OpenAI")
    ### multi query
    _vdb = get_faiss_OpenAI(_db_name)
    _ask = _query.replace(' AND ', ' ')
    _key = _query.split(" AND ")[1]
    _context_fp = os.path.join(os.path.dirname(_db_name), "_context.txt")
    _ans_fp = os.path.join(os.path.dirname(_db_name), "_ans.txt")
    if not os.path.exists(_ans_fp):
        _ans, _step, _token_cost = qa_vdb_multi_query(_ask, _vdb, 'stuff')
        with open(_ans_fp, "w") as f:
            f.write(_ans)
        ##### _context
        _context = clean_txt(_ans)
        ### å»é™¤ _context ä¸­çš„ [1] [50.14] ç­‰æ ‡è®°
        _context = re.sub(r'\[\d+\]', '', _context)
        _context = re.sub(r'\[\d+\.\d+\]', '', _context)
        with open(_context_fp, "w") as f:
            f.write(_context)
    else:
        with open(_ans_fp, "r") as f:
            _ans = f.read()
        _context = clean_txt(_ans)
        ### å»é™¤ _context ä¸­çš„ [1] [50.14] [81-82] ç­‰æ ‡è®°
        _context = re.sub(r'\[\d+\]', '', _context)
        _context = re.sub(r'\[\d+\.\d+\]', '', _context)
        _context = re.sub(r'\[\d+\-\d+\]', '', _context)
        ### å»é™¤ _context ä¸­é‚£äº›åªåŒ…å«ç©ºç™½ï¼ˆå¦‚ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰çš„è¡Œ
        _context = re.sub(r'^\s+\n$', '\n', _context, flags=re.MULTILINE)
        if '- ' in _context:
            _clean = []
            _li = _context.split('\n')
            for i in _li:
                if '- ' in i:
                    _clean.append(i)
            _context = '\n'.join(_clean)
        with open(_context_fp, "w") as f:
            f.write(_context)
    logger_yj.info(_ask)
    # logger_yj.info(_ans)
    # logger_yj.info(_matches)
    logger_yj.info(_context)
    logger_yj.info(_token_cost)
    return _context, _token_cost, _urls


def generate_context_for_each_query_textgen(_query: str, _db_name: str, _clean_txt_dir: str, _textgen_url: str):
    _context, _token_cost, _urls = "", "Tokens: 0 = (Prompt 0 + Completion 0) Cost: $0", ""
    if not os.path.exists(_db_name):
        _all = search_all(_query)
        logger_yj.info(print_search_all(_all))
        _all_link = [i['link'] for i in _all]
        _urls = list(set(_all_link))
        urls_to_faiss_HuggingFace(_urls, _db_name, _clean_txt_dir)
    else:
        logger_yj.info(f"'{_db_name}'å·²å­˜åœ¨ï¼Œæ— éœ€ urls_to_faiss_HuggingFace")
    ### multi query
    _vdb = get_faiss_HuggingFace(_db_name)
    _ask = _query.replace(' AND ', ' ')
    _key = _query.split(" AND ")[1]
    _context_fp = os.path.join(os.path.dirname(_db_name), "_context.txt")
    _ans_fp = os.path.join(os.path.dirname(_db_name), "_ans.txt")
    if not os.path.exists(_ans_fp):
        _ans, _step, _token_cost = qa_vdb_multi_query_textgen(_ask, _vdb, 'stuff', _textgen_url)
        if _token_cost == "":
            _token_cost = "Tokens: 0 = (Prompt 0 + Completion 0) Cost: $0"
        with open(_ans_fp, "w") as f:
            f.write(_ans)
        ##### _context
        _context = clean_txt(_ans)
        ### å»é™¤ _context ä¸­çš„ [1] [50.14] ç­‰æ ‡è®°
        _context = re.sub(r'\[\d+\]', '', _context)
        _context = re.sub(r'\[\d+\.\d+\]', '', _context)
        with open(_context_fp, "w") as f:
            f.write(_context)
    else:
        with open(_ans_fp, "r") as f:
            _ans = f.read()
        _context = clean_txt(_ans)
        ### å»é™¤ _context ä¸­çš„ [1] [50.14] ç­‰æ ‡è®°
        _context = re.sub(r'\[\d+\]', '', _context)
        _context = re.sub(r'\[\d+\.\d+\]', '', _context)
        if '- ' in _context:
            _clean = []
            _li = _context.split('\n')
            for i in _li:
                if '- ' in i:
                    _clean.append(i)
            _context = '\n'.join(_clean)
        with open(_context_fp, "w") as f:
            f.write(_context)
    logger_yj.info(_ask)
    # logger_yj.info(_ans)
    # logger_yj.info(_matches)
    logger_yj.info(_context)
    logger_yj.info(_token_cost)
    return _context, _token_cost, _urls


def context_report(_event: str, _title: str, _context: dict, _urls: dict):
    _report = []
    _report.append(_title)
    _section = [
        "åŸºæœ¬æƒ…å†µ",
        "å¤„ç½®è¿‡ç¨‹",
        "å†›æ°‘åä½œ",
        "æ³•è§„ä¾æ®",
        "å½±å“è¯„ä¼°",
        "åæ€å¯ç¤º",
    ]
    for i in _section:
        _section = []
        _section.append(f"# {i}")
        for j in _context.keys():
            if i in j:
                _section.append(f"## [{yj_keyword[j]}]({_urls[j][0]})\n{_context[j]}")
        _report.append("\n\n".join(_section))
    # _report.append(f"# {_event}ç›¸å…³é“¾æ¥" + "\n\n" + "\n".join(_urls)
    return "\n\n\n".join(_report)


def generate_event_context(_event: str, _dict: dict[str]) -> dict[str]:
    _event_name = clean_filename(_event, 20)
    _event_dir = str(AUTOGEN_JD / _event_name)
    _context = {}
    if not os.path.exists(_event_dir):
        os.makedirs(_event_dir)
    logger_yj.info("generate_context_for_search_listï¼šå¼€å§‹")
    _tc = []
    _urls = {}
    for i in _dict.keys():
        i_key = _dict[i].split(" AND ")[1]
        i_db_name = os.path.join(_event_dir, f"{i_key}/yj_rag_openai")
        i_clean_txt_dir = os.path.join(_event_dir, f"{i_key}/yj_rag_clean_txt")
        i_context, i_token_cost, i_urls = generate_context_for_each_query(_dict[i], i_db_name, i_clean_txt_dir)
        _context[i] = i_context
        _tc.append(i_token_cost)
        _urls[i] = i_urls
    _token_cost = calc_token_cost(_tc)
    logger_yj.info(_token_cost)
    logger_yj.info("generate_context_for_search_listï¼šå®Œæˆ")
    _title = f"'{_event}'äº‹ä»¶è„‰ç»œæ¢³ç†æŠ¥å‘Š"
    _report_fp = os.path.join(_event_dir, f"{_title}.md")
    if not os.path.exists(_report_fp):
        _context_str = json.dumps(_context, ensure_ascii=False, indent=4)
        _report = context_report(_event, _context, _urls)
        with open(_report_fp, "w") as f:
            f.write(_report)
    else:
        with open(_report_fp, "r") as f:
            _report = f.read()
    return _report, _report_fp


def generate_event_context_textgen(_event: str, _dict: dict[str], _textgen_url: str) -> dict[str]:
    _event_name = clean_filename(_event, 20)
    _event_dir = str(AUTOGEN_JD / _event_name)
    _context = {}
    if not os.path.exists(_event_dir):
        os.makedirs(_event_dir)
    logger_yj.info("generate_context_for_search_listï¼šå¼€å§‹")
    _tc = []
    _urls = {}
    for i in _dict.keys():
        i_key = _dict[i].split(" AND ")[1]
        i_db_name = os.path.join(_event_dir, f"{i_key}/yj_rag_hf")
        i_clean_txt_dir = os.path.join(_event_dir, f"{i_key}/yj_rag_clean_txt")
        i_context, i_token_cost, i_urls = generate_context_for_each_query_textgen(_dict[i], i_db_name, i_clean_txt_dir, _textgen_url)
        _context[i] = i_context
        _tc.append(i_token_cost)
        _urls[i] = i_urls
    _token_cost = calc_token_cost(_tc)
    logger_yj.info(_token_cost)
    logger_yj.info("generate_context_for_search_listï¼šå®Œæˆ")
    _title = f"'{_event}'äº‹ä»¶è„‰ç»œæ¢³ç†æŠ¥å‘Š"
    _report_fp = os.path.join(_event_dir, f"{_title}.md")
    if not os.path.exists(_report_fp):
        _context_str = json.dumps(_context, ensure_ascii=False, indent=4)
        _report = context_report(_event, _title, _context, _urls)
        with open(_report_fp, "w") as f:
            f.write(_report)
    else:
        with open(_report_fp, "r") as f:
            _report = f.read()
    return _report, _report_fp


##### event plan
def generate_event_plan(_event: str, _context:str) -> str:
    _event_name = clean_filename(_event, 20)
    _event_dir = str(AUTOGEN_JD / _event_name)
    # _db_name = os.path.join(_event_dir, f"yj_rag_openai")
    logger_yj.info("generate_event_planï¼šå¼€å§‹")
    _plan = f"{_context}\n\nã€planã€‘"
    logger_yj.info("generate_event_planï¼šå®Œæˆ")
    return _plan



def fetch_all_link(_all, _event_dir):
    _txt_fp = []
    _all_link = [i['link'] for i in _all]
    _all_title = [i['title'] for i in _all]
    # print(_all_link)
    loader = WebBaseLoader()
    _re = loader.scrape_all(_all_link)
    for i in range(len(_re)):
        _fn = clean_filename(_all_title[i])
        _fp = os.path.join(_event_dir, f"{_fn}.txt")
        with open(_fp, "w") as wf:
            # _txt = _re[i].get_text()
            _txt = _re[i].get_text()
            wf.write(_txt)
        logger_yj.info(f"{_all_link[i]} -> {_fn}.txt")
        _txt_fp.append(_fp)
    return _txt_fp


def handle_fetch(_event_dir, _db_name, _clean_txt_dir):
    logger_yj.info(f"generate faiss_openaiï¼šå¼€å§‹")
    dir_to_faiss_OpenAI(_event_dir, _db_name, _clean_txt_dir)
    logger_yj.info(f"generate faiss_openaiï¼š{_db_name}")
    logger_yj.info(f"generate faiss_openaiï¼šå®Œæˆ")



# def generate_vdb_for_search_query(_query: list[str], _event_name: str):
#     _event_dir = str(AUTOGEN_JD / _event_name)
#     if not os.path.exists(_event_dir):
#         os.makedirs(_event_dir)
#         _db_name = str(AUTOGEN_JD / _event_dir / f"yj_rag_openai")
#         _clean_txt_dir = str(AUTOGEN_JD / _event_dir / f"yj_rag_clean_txt")
#         logger_yj.info("generate_vdb_for_search_queryï¼šå¼€å§‹")
#         # ### search, fetch, vdb
#         # for i in _query:
#         #     i_all = search_all(i)
#         #     i_psa = print_search_all(i_all)
#         #     logger_yj.info(f"'{i}'")
#         #     logger_yj.info(i_psa)
#         #     fetch_all_link(i_all, _event_dir)
#         # handle_fetch(_event_dir, _db_name, _clean_txt_dir)
#         ### search, load, vdb
#         _all = []
#         for i in _query:
#             i_all = search_all(i)
#             _all.extend(i_all)
#         _psa = print_search_all(_all)
#         logger_yj.info(_psa)
#         _all_link = [i['link'] for i in _all]
#         _urls = list(set(_all_link))
#         urls_to_faiss(_urls, _db_name, _clean_txt_dir)
#         logger_yj.info("generate_vdb_for_search_queryï¼šå®Œæˆ")
#     else:
#         logger_yj.info(f"'{_event_name}'ä¸“é¢˜å·²å­˜åœ¨ï¼Œæ— éœ€ generate_vdb_for_search_query")
#         # shutil.rmtree(_event_dir)
#         # os.makedirs(_event_dir)


# def ask_vdb_with_source(_ask, _vdb):
#     llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo-16k")
#     chain = RetrievalQAWithSourcesChain.from_chain_type(
#         llm=llm,
#         chain_type="stuff",
#         retriever=_vdb.as_retriever(),
#     )
#     with get_openai_callback() as cb:
#         _res = chain({"question": _ask}, return_only_outputs=True)
#         _token_cost = f"Tokens: {cb.total_tokens} = (Prompt {cb.prompt_tokens} + Completion {cb.completion_tokens}) Cost: ${format(cb.total_cost, '.5f')}"
#         logger_yj.info(_token_cost)
#     return _res['answer'], _res['sources'], _token_cost


# def generate_event_context(_event: str, _event_name: str) -> str:
#     _db_name = str(AUTOGEN_JD / _event_name / f"yj_rag_openai")
#     _ask_vdb = str(AUTOGEN_JD / _event_name / 'ask_vdb.txt')
#     if os.path.exists(_ask_vdb):
#         os.remove(_ask_vdb)
#     logger_yj.info("generate_event_contextï¼šå¼€å§‹")
#     _task = "è¯·ç”Ÿæˆå…³äº'{_context}'çš„è¯¦ç»†ä¿¡æ¯æŸ¥è¯¢é—®é¢˜åˆ—è¡¨ï¼ˆä¸€ä¸ªé—®é¢˜ä¸€è¡Œï¼‰ã€‚ç¡®ä¿åªåˆ—å‡ºå…·ä½“çš„é—®é¢˜ï¼Œè€Œä¸åŒ…æ‹¬ä»»ä½•ç« èŠ‚æ ‡é¢˜æˆ–ç¼–å·ã€‚"
#     _ans, _tc = task_with_context_template(_task, _event, _gec_ask)
#     # logger_yj.info(f"\n{_ans}")
#     _extracted = re.findall(r'\s+-\s+(.*)\n', _ans)
#     for i in _extracted:
#         logger_yj.info(i)
#     logger_yj.info(_tc)
#     ### ask_vdb
#     logger_yj.info("ask_vdbï¼šå¼€å§‹")
#     _vdb = get_faiss_OpenAI(_db_name)
#     _qas = []
#     _tc = []
#     for i in _extracted:
#         _ask = i + "å¦‚æœæ‰¾ä¸åˆ°ç¡®åˆ‡ç­”æ¡ˆï¼Œè¯·å›ç­”'æ— 'ã€‚ç”¨ç®€æ´ä¸­æ–‡å›ç­”ã€‚"
#         ### with source
#         # i_ans, i_source, i_token_cost = ask_vdb_with_source(_ask, _vdb)
#         # i_qas = f"Q: {i}\nA: {i_ans.strip()}\nSource: {i_source}"
#         ### multi query
#         # i_ans, i_step, i_token_cost = qa_vdb_multi_query(_ask, _vdb, 'stuff')
#         ### autogen
#         # _task_list, c_token_cost = create_rag_task_list_zh(i)
#         # _context, s_token_cost = search_faiss_openai(_task_list, _vdb)
#         # i_ans, i_token_cost = qa_with_context_as_go(_ask, _context)
#         ###
#         i_ans = clean_txt(i_ans)
#         i_ans = i_ans.strip()
#         if i_ans != '':
#             i_qas = f"Q: {i}\nA: {i_ans}"
#             logger_yj.info(i_qas)
#             _qas.append(i_qas)
#             with open(_ask_vdb, "w") as f:
#                 f.write("\n\n".join(_qas))
#         _tc.append(c_token_cost)
#         _tc.append(s_token_cost)
#         _tc.append(i_token_cost)
#     _token_cost = calc_token_cost(_tc)
#     logger_yj.info(_token_cost)
#     logger_yj.info("ask_vdbï¼šå®Œæˆ")
#     ### 
#     _context = "ã€_contextã€‘"
#     logger_yj.info("generate_event_contextï¼šå®Œæˆ")
#     return _context


# def generate_search_query_for_event(_event: str, _event_name: str) -> list[str]:
#     _event_dir = str(AUTOGEN_JD / _event_name)
#     _query = []
#     if not os.path.exists(_event_dir):
#         logger_yj.info("generate_search_query_for_eventï¼šå¼€å§‹")
#         _task = "è¯·ç”Ÿæˆå…³äº'{_context}'çš„ä¿¡æ¯æŸ¥è¯¢åˆ—è¡¨ã€‚ç¡®ä¿åªåˆ—å‡ºå…·ä½“çš„æŸ¥è¯¢è¯­å¥ï¼Œè€Œä¸åŒ…æ‹¬ä»»ä½•ç« èŠ‚æ ‡é¢˜æˆ–ç¼–å·ã€‚"
#         _ans, _tc = task_with_context_template(_task, _event, _gsqfe)
#         # logger_yj.info(f"\n{_ans}")
#         _extracted = re.findall(r'\s+-\s+(.*)\n', _ans)
#         for i in _extracted:
#             logger_yj.info(i)
#         logger_yj.info(_tc)
#         _query = _extracted
#         logger_yj.info("generate_search_query_for_eventï¼šå®Œæˆ")
#     else:
#         logger_yj.info(f"'{_event_name}'ä¸“é¢˜å·²å­˜åœ¨ï¼Œæ— éœ€ generate_search_query_for_event")
#     return _query


# _gsqfe = """ä½ çš„ä»»åŠ¡æ˜¯ä½¿ç”¨æœç´¢å¼•æ“ï¼ˆä¾‹å¦‚è°·æ­Œï¼‰æ¥æœé›†å…³äºç‰¹å®šè‡ªç„¶ç¾å®³äº‹ä»¶çš„å…¨é¢ä¿¡æ¯ã€‚ä½ éœ€è¦å¯»æ‰¾çš„ä¿¡æ¯åŒ…æ‹¬äº‹ä»¶çš„åŸºæœ¬æ¦‚å†µã€å¤„ç½®è¿‡ç¨‹ã€å†›æ°‘åä½œã€æ³•è§„ï¼ˆæ”¿ç­–ï¼‰ä¾æ®ã€å½±å“è¯„ä¼°ä»¥åŠåæ€å’Œå¯ç¤ºã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æŒ‡å—ç”Ÿæˆè¯¦å°½çš„æŸ¥è¯¢ï¼š

# 1. åŸºæœ¬æ¦‚å†µï¼š
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å‘ç”Ÿå‘å±•çš„æ—¶é—´çº¿
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³å¼ºåº¦å’Œè§„æ¨¡

# 2. å¤„ç½®è¿‡ç¨‹ï¼š
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å®æ–½çš„åº”æ€¥å“åº”å’Œæ•‘æ´æªæ–½çš„æ—¶é—´çº¿
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³å“åº”ç‰©èµ„å’Œäººå‘˜éƒ¨ç½²

# 3. å†›æ°‘åä½œï¼š
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å†›é˜Ÿå‚ä¸æ•‘ç¾è¡ŒåŠ¨çš„æ—¶é—´çº¿
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å†›é˜Ÿæ•‘æ´è¡ŒåŠ¨å’Œåä½œç»†èŠ‚

# 4. æ³•è§„ï¼ˆæ”¿ç­–ï¼‰ä¾æ®ï¼š
#    - [å…·ä½“åº”æ€¥äº‹ä»¶åç§°] å†›é˜ŸååŠ©æ•‘ç¾çš„æ³•å¾‹ä¾æ®
#    - [å…·ä½“åº”æ€¥äº‹ä»¶åç§°] åœ°æ–¹æ”¿åºœå’Œå†›é˜Ÿæ•‘ç¾åˆä½œçš„æ”¿ç­–æ¡†æ¶
   
# 4. å½±å“è¯„ä¼°ï¼š
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³å¯¹ç»æµå’Œç¤¾ä¼šçš„å½±å“
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å—ç¾ç¾¤ä½“å’Œåœ°åŒºçš„æ¢å¤è¿›ç¨‹

# 5. åæ€å’Œå¯ç¤ºï¼š
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³ç®¡ç†å’Œåº”å¯¹çš„æœ‰æ•ˆæ€§è¯„ä¼°
#    - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ä»ç¾å®³ä¸­å­¦åˆ°çš„æ•™è®­å’Œæ”¹è¿›æªæ–½

# è¯·ç¡®ä¿æ›¿æ¢[_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_]ä¸ºä½ æ­£åœ¨ç ”ç©¶çš„äº‹ä»¶åç§°ï¼Œä»¥å®šä½å‡†ç¡®ç›¸å…³çš„èµ„æ–™ã€‚æ ¹æ®æœç´¢ç»“æœçš„è¯¦å°½ç¨‹åº¦ï¼Œé€‚æ—¶è°ƒæ•´æˆ–ç»†åŒ–ä½ çš„æŸ¥è¯¢å…³é”®è¯ã€‚
# [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_]: {_context}
# {_task}:
# """


# _gec_ask = """é¢å¯¹'{_context}'çš„ä¸¥å³»æŒ‘æˆ˜ï¼Œå›½å®¶å’Œåœ°æ–¹åº”æ€¥ç®¡ç†éƒ¨é—¨éœ€è¿…é€ŸæŒæ¡å…¨é¢è€Œè¯¦å°½çš„ä¿¡æ¯ï¼Œä»¥ä¾¿å½¢æˆæœ‰æ•ˆçš„åº”å¯¹ç­–ç•¥ã€‚ç°åœ¨ï¼Œè¯·ä½ æ ¹æ®ä»¥ä¸‹é˜¶æ®µæ€§ä¿¡æ¯éœ€æ±‚ï¼Œç”Ÿæˆä¸€ç³»åˆ—çš„æŸ¥è¯¢é—®é¢˜ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬æœé›†æœ‰å…³è¯¥äº‹ä»¶å…¨ç¨‹çš„å…³é”®ä¿¡æ¯ï¼š

# 1. ç¾å®³å‘ç”Ÿä¸åˆæœŸå“åº”ï¼š
#    - è¯¥è‡ªç„¶ç¾å®³ç¡®åˆ‡çš„å‘ç”Ÿæ—¶é—´å’Œåœ°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
#    - ç¾å®³çš„å¼ºåº¦å’Œè§„æ¨¡ï¼Ÿ
#    - ç¾å®³å‘ç”Ÿåï¼Œé¦–æ‰¹é‡‡å–çš„åº”æ€¥å“åº”æªæ–½åŒ…æ‹¬å“ªäº›ï¼Ÿ

# 2. åº”æ€¥ååº”ä¸å¤„ç½®ç»è¿‡ï¼š
#    - ç¾å®³å‘å±•æ‰©æ•£çš„è¿‡ç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ
#    - ç¾å®³å‘å±•æ‰©æ•£çš„æ—¶é—´çº¿å¦‚ä½•ï¼Ÿ
#    - å®æ–½çš„åº”æ€¥å“åº”å’Œæ•‘æ´æªæ–½å…·ä½“åŒ…æ‹¬å“ªäº›ï¼Ÿ
#    - å®æ–½åº”æ€¥å“åº”å’Œæ•‘æ´æªæ–½çš„æ—¶é—´çº¿å¦‚ä½•ï¼Ÿ
#    - ç¾å®³çš„å…³é”®è½¬æŠ˜ç‚¹æœ‰å“ªäº›ï¼Ÿ
#    - åœ¨ç¾å®³åº”å¯¹ä¸­ï¼Œæ¶‰åŠçš„å…³é”®äººç‰©å’Œç»„ç»‡æ‰®æ¼”äº†ä½•ç§è§’è‰²ï¼Ÿ
#    - å“åº”è¿‡ç¨‹ä¸­ä¾æ®äº†å“ªäº›æ”¿ç­–æ³•è§„æ¥æŒ‡å¯¼è¡ŒåŠ¨ï¼Ÿ
#    - ç¾å®³å¯¹ç¯å¢ƒã€å…¬å…±å¥åº·ã€ç»æµä»¥åŠç¤¾ä¼šç§©åºäº§ç”Ÿäº†å“ªäº›æ·±è¿œå½±å“ï¼Ÿ

# 3. ç¾æƒ…æ§åˆ¶ä¸è¿‡æ¸¡æ€§æªæ–½ï¼š
#    - ç¾æƒ…å¾—åˆ°æ§åˆ¶çš„æ—¶é—´ç‚¹åŠå…¶æ ‡å¿—æ€§äº‹ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ
#    - æ§åˆ¶ç¾æƒ…åé‡‡å–äº†å“ªäº›å…³é”®æªæ–½å’Œç­–ç•¥ï¼Ÿ
#    - é’ˆå¯¹ç¾å®³å—å½±å“äººç¾¤å®æ–½äº†å“ªäº›è¿‡æ¸¡æ€§æ•‘åŠ©å’Œæ”¯æŒï¼Ÿ

# 4. æ¢å¤é‡å»ºä¸æ³•è§„æ”¿ç­–å®æ–½ï¼š
#    - ç¾åæ¢å¤å’Œé‡å»ºå·¥ä½œçš„èµ·å§‹æ—¶é—´å’Œé˜¶æ®µæ€§ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
#    - é‡å»ºæœŸé—´ï¼Œåˆ¶å®šæˆ–ä¿®æ”¹äº†å“ªäº›ç›¸å…³æ³•å¾‹æ³•è§„ï¼Ÿ
#    - æ¢å¤å’Œé‡å»ºæªæ–½çš„é•¿æœŸç¤¾ä¼šè¯„ä»·å’Œç”Ÿæ€å½±å“å¦‚ä½•ï¼Ÿ

# 5. æ•´ä½“è¯„ä¼°ä¸æ•™è®­å¸å–ï¼š
#    - å¦‚ä½•ç³»ç»Ÿè¯„ä»·æœ¬æ¬¡è‡ªç„¶ç¾å®³çš„å¤„ç½®æ•ˆæœå’Œåº”æ€¥ç®¡ç†èƒ½åŠ›ï¼Ÿ
#    - è¿™æ¬¡äº‹ä»¶å¯¹æœªæ¥ç¾å®³é£é™©è¯„ä¼°å’Œé¢„é˜²è®¡åˆ’æœ‰å“ªäº›å¯ç¤ºï¼Ÿ
#    - ä¸ºæé«˜æœªæ¥åº”æ€¥å“åº”å’Œç¾å®³ç®¡ç†èƒ½åŠ›ï¼Œæå‡ºå“ªäº›å…·ä½“çš„å»ºè®®å’Œç­–ç•¥ï¼Ÿ

# é€šè¿‡å›ç­”è¿™äº›è¯¦ç»†çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå»ºç«‹ä¸€ä¸ªå…³äº'{_context}'çš„å…¨æ–¹ä½ã€æ·±å…¥çš„äº‹ä»¶æŠ¥å‘Šã€‚è¯·ä¸ºæ¯ä¸ªä¿¡æ¯ç‚¹ç”Ÿæˆå…·ä½“çš„æŸ¥è¯¢é—®é¢˜ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯¹çŸ¥è¯†åº“è¿›è¡Œè¯¦ç»†çš„äº‹å®æœç´¢å’Œæ•°æ®åˆ†æã€‚
# {_task}:
# """


# def do_multi_search(msg):
#     _agents = [
#     ]
#     _out = str(WORKSPACE_AUTOGEN / "organizer_output.txt")
#     def validate_results_func():
#         with open(_out, "r") as f:
#             content = f.read()
#         return bool(content)
#     _organizer = Organizer(
#         name="Search Team",
#         agents=_agents,
#         validate_results_func=validate_results_func,
#     )
#     _organizer_conversation_result = _organizer.broadcast_conversation(msg)
#     match _organizer_conversation_result:
#         case ConversationResult(success=True, cost=_cost, tokens=_tokens):
#             print(f"âœ… Organizer.Broadcast was successful. Team: {_organizer.name}")
#             print(f"ğŸ“Š Name: {_organizer.name} Cost: {_cost}, tokens: {_tokens}")
#             with open(_out, "r") as f:
#                 content = f.read()
#             return content
#         case _:
#             print(f"âŒ Organizer.Broadcast failed. Team: {_organizer.name}")

