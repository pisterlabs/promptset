import re
import os
import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader
from pytube import Playlist
from youtube_transcript_api import YouTubeTranscriptApi
import streamlit as st
from st_chat_message import message
#from streamlit_chat import message
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)




# //ë°ì´í„° ì¶”ì¶œ============================================
def extract_title_and_content(url):
    # Sending a GET request to the URL
    response = requests.get(url)

    # Parsing the HTML content of the page with BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Using CSS selectors to extract the title and content
    title_element = soup.find("dd", class_="fc1").find("strong")
    content_element = soup.find("div", class_="scrollY", attrs={"tabindex": "0"})

    # If title_element exists, get the text
    if title_element:
        title = title_element.get_text(strip=True)
        forbidden_chars = r'\\/:*?"<>|'

        title = ''.join(char for char in title if char not in forbidden_chars)
    else:
        title = ""

    # If content_element exists, get the text
    if content_element:
        content = content_element.get_text(strip=True)
    else:
        content = "Content not found"

    return title, content

def load_single_document(file_path):
    loader = TextLoader(file_path, encoding="utf-8")
    return loader.load()[0]

def load_documents(source_dir):
    all_files = os.listdir(source_dir)
    return [load_single_document(f"{source_dir}/{file_name}") for file_name in all_files]

def get_response_from_query(vector_db, query, target, k):
    docs = vector_db.similarity_search(query, k=k)
    docs_page_content = " ".join([d.page_content for d in docs])

    chat = ChatOpenAI(model_name="gpt-3.5-turbo-16k", temperature=1)

    # Template to use for the system message prompt
    template = """
        You are a helpful assistant that that can ALL answer or explain  to {target}.
        Document retrieved from your DB : {docs}

        Answer the questions referring to the documents which you Retrieved from DB as much as possible.
        If you feel like you don't have enough information to answer the question, say "I don't know".

        Since your answer targets {target}, you should return an answer that is optimized for understanding by {target}.
        """

    system_message_prompt = SystemMessagePromptTemplate.from_template(template)

    # Human question prompt
    human_template = "Answer the following question IN KOREAN: {question}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )

    chain = LLMChain(llm=chat, prompt=chat_prompt)

    response = chain.run(question=query, docs=docs_page_content, target=target)
    response = response.replace("\n", "")
    return response, docs
#=========================================================



# //ë§ì¶¤í˜• DB ìƒì„±============================================
def Crawling_DB_Child():
  # Start and end ids of '21ì¼ê°„ì˜ ê²½ì œì—¬í–‰(21í¸)'
  child_1_start_id = 165557
  child_1_end_id = 165577

  # Start and end ids of 'ë§ˆë²•ì†Œë…„ ì¬ë¯¼ì´ì˜ ê²½ì œì„¸ìƒ ì ì‘ê¸°(11í¸)'
  child_2_start_id = 10000017
  child_2_end_id = 10000027

  # Start and end ids of 'ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì•Œê¸° ì‰¬ìš´ ê²½ì œì´ì•¼ê¸°'
  child_3_id = 236250

  # Base URL
  child_base_url = 'https://www.bok.or.kr/portal/bbs/B0000216/view.do?nttId={id}&type=CHILD&searchOptn8=01&menuNo=200646&listType=G&searchOptn4=CHILD&pageIndex=1'

  # List to store all URLs
  child_vod_url_list = []
  child_pdf_url_list = []

  # [ì–´ë¦°ì´] - '21ì¼ê°„ì˜ ê²½ì œì—¬í–‰(21í¸)'
  for id in range(child_1_start_id, child_1_end_id + 1):
      # Inserting id into the base url
      url = child_base_url.format(id=id)
      # Adding url to the list
      child_vod_url_list.append(url)

  # [ì–´ë¦°ì´] - 'ë§ˆë²•ì†Œë…„ ì¬ë¯¼ì´ì˜ ê²½ì œì„¸ìƒ ì ì‘ê¸°(11í¸)'
  for id in range(child_2_start_id, child_2_end_id + 1):
      # Inserting id into the base url
      url = child_base_url.format(id=id)
      # Adding url to the list
      child_vod_url_list.append(url)

  # [ì–´ë¦°ì´] - 'ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì•Œê¸° ì‰¬ìš´ ê²½ì œì´ì•¼ê¸°'
  child_3_url = child_base_url.format(id=child_3_id)
  child_pdf_url_list.append(child_3_url)

  # Ensure "child" directory exists
  if not os.path.exists("DB/text/Child"):
      os.makedirs("DB/text/Child")


  # [ì–´ë¦°ì´]DB : VOD -> txt íŒŒì¼
  for url in child_vod_url_list:
      # Extract title and content
      title, content = extract_title_and_content(url)

      # Save the content in a text file with the title as the name
      with open(f"DB/text/Child/{title}.txt", 'w', encoding='utf-8') as f:
          f.write(content)

  # [ì–´ë¦°ì´]DB : PDF -> txt íŒŒì¼
  for url in child_pdf_url_list:
      # URL of the webpage
      webpage_url = 'https://www.bok.or.kr/portal/bbs/B0000216/view.do?nttId=236250&type=CHILD&searchOptn8=22&menuNo=200646&listType=G&searchOptn4=CHILD&pageIndex=1'

      # Base URL
      base_url = 'https://www.bok.or.kr'

      # Send a GET request to the webpage URL
      response = requests.get(webpage_url)
      soup = BeautifulSoup(response.content, 'html.parser')

      # Find the URL of the PDF file
      pdf_url_suffix = soup.find('div', {'class': 'addfile'})
      pdf_url_suffix = pdf_url_suffix.find('a')['href']
      pdf_url = base_url + pdf_url_suffix
      
      # Send a GET request to the PDF URL
      response = requests.get(pdf_url)

      # Save the PDF in a file
      with open("DB/text/Child/Economic_Story.pdf", 'wb') as f:
          f.write(response.content)

      reader = PdfReader("DB/text/Child/Economic_Story.pdf")
      number_of_pages = len(reader.pages)

      pdf_to_text=''
      for i in range(15,number_of_pages):
        page = reader.pages[i]
        extract_text = page.extract_text()
        pdf_to_text += extract_text.strip()
      pdf_to_text = pdf_to_text.replace('\n', ' ')

      with open('DB/text/Child/ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì•Œê¸° ì‰¬ìš´ ê²½ì œì´ì•¼ê¸°.txt', 'w', encoding='utf-8') as f:
          f.write(pdf_to_text)

      # ì›ë³¸ pdf ì‚­ì œ
      if os.path.exists("DB/text/Child/Economic_Story.pdf"):
        os.remove('DB/text/Child/Economic_Story.pdf')

def Crawling_DB_Student():
  # this fixes the empty playlist.videos list
  playlist = Playlist('https://www.youtube.com/playlist?list=PL80z1RKB1KmwsvqBiDLspG9YqXu3Xz_ON')
  playlist._video_regex = re.compile(r"\"url\":\"(/watch\?v=[\w-]*)")

  # Ensure "student" directory exists
  if not os.path.exists("DB/text/Student"):
      os.makedirs("DB/text/Student")

  for url in playlist.video_urls:
      yt = requests.get(url)
      yt_text = BeautifulSoup(yt.text, 'lxml')
      title = yt_text.select_one('meta[itemprop="name"][content]')['content']
      title = title.replace(":", "_")
      
      url_id = url.split('v=')[-1]
      try:
        transcript_list = YouTubeTranscriptApi.get_transcript(url_id,languages=['ko'])
        srt =[transcript['text'] for transcript in transcript_list]
        transcript = ''.join(srt)

        # Save the content in a text file with the title as the name
        with open(f"DB/text/Student/{title}.txt", 'w', encoding='utf-8') as f:
            f.write(transcript)

      except:
        continue


def Crawling_DB_Adult():

  adult_list=['https://www.youtube.com/playlist?list=PL80z1RKB1Kmy-LMsm1MRR4NKiPeI6R14R','https://www.youtube.com/playlist?list=PL80z1RKB1KmymSwpImyjMR4fsUKP1Z9WH']

  for adult_url in adult_list:
    # this fixes the empty playlist.videos list
    playlist = Playlist(adult_url)
    playlist._video_regex = re.compile(r"\"url\":\"(/watch\?v=[\w-]*)")

    # Ensure "student" directory exists
    if not os.path.exists("DB/text/Adult"):
        os.makedirs("DB/text/Adult")

    for url in playlist.video_urls:
        yt = requests.get(url)
        yt_text = BeautifulSoup(yt.text, 'lxml')
        title = yt_text.select_one('meta[itemprop="name"][content]')['content']

        url_id = url.split('v=')[-1]
        try:
          transcript_list = YouTubeTranscriptApi.get_transcript(url_id,languages=['ko'])
          srt =[transcript['text'] for transcript in transcript_list]
          transcript = ''.join(srt)

          # Save the content in a text file with the title as the name
          with open(f"DB/text/Adult/{title}.txt", 'w', encoding='utf-8') as f:
              f.write(transcript)

        except:
          continue
#=========================================================



# //ì„¤ì •ì°½=============================================
def init(): # Web App ì„¤ì •

    st.set_page_config(
        page_title="SAFFY ê¸ˆìœµ/ê²½ì œ ì§€ì‹êµìœ¡ GPT"
    )


def init_db(): # [ì–´ë¦°ì´/ì²­ì†Œë…„/ì„±ì¸] ë§ì¶¤í˜• VectorDB êµ¬ì¶•

    # //Text DB êµ¬ì¶•====================================
    Crawling_DB_Child()
    print("ì–´ë¦°ì´ìš© ê¸ˆìœµ/ê²½ì œ DB êµ¬ì¶• ì™„ë£Œ")

    Crawling_DB_Student()
    print("ì²­ì†Œë…„ìš© ê¸ˆìœµ/ê²½ì œ DB êµ¬ì¶• ì™„ë£Œ")

    Crawling_DB_Adult()
    print("ì„±ì¸ìš© ê¸ˆìœµ/ê²½ì œ DB êµ¬ì¶• ì™„ë£Œ")
    # #===================================================


    # # //Vector DB êµ¬ì¶•==================================
    if not os.path.exists(f"DB/vector"):
        os.makedirs(f"DB/vector")

    # # //init_db()í•¨ìˆ˜ë§Œì„ í˜¸ì¶œ í• ë•Œ í•¨ìˆ˜ ë‚´ì—ì„œ openai_api_keyì§€ì •===========
    embedding = OpenAIEmbeddings(openai_api_key='ë°œê¸‰ë°›ì€ OPENAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”')
    
    list_en=['Child','Student','Adult']
    list_kr=['ì–´ë¦°ì´','í•™ìƒ','ì„±ì¸']

    for level_en,level_kr in zip(list_en,list_kr):   
        file_path = 'DB/text/' + level_en
        transcript = load_documents(file_path)
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.split_documents(transcript)

        db = FAISS.from_documents(docs, embedding)
        db.save_local(f"DB/vector/{level_en}")
        print(f"{level_kr} VectorDB êµ¬ì¶• ì™„ë£Œ")
    #===================================================
#=========================================================




def finance_gpt(user_name,user_input,refer_db):
    st.header('')
    k = int(refer_db[0])
    
    # ì§ˆë¬¸ ì…ë ¥ì‹œ,
    with st.container():
        # ì‚¬ìš©ì ì§ˆë¬¸ GUI í‘œì‹œ
        message(user_input,is_user=True)
        st.subheader('')

        # LLM , Embedding ì„¸íŒ…
        embedding = OpenAIEmbeddings()
        level_dict = {'ì–´ë¦°ì´':'Child','ì²­ì†Œë…„':'Student','ì„±ì¸':'Adult'}

        level_kr = user_name
        level_en = level_dict[level_kr]

        st.subheader(" ")
        st.subheader(f'{level_kr} ë§ì¶¤ ë‹µë³€')
        with st.spinner(f"{level_kr} ë§ì¶¤í˜• ë‹µë³€ ìƒì„±ì¤‘..."):
            vector_db = FAISS.load_local(f"DB/vector/{level_en}",embedding)
            response, docs = get_response_from_query(vector_db, user_input, level_en, k)

        # GPT ë‹µë³€
        message(response, is_user=False)
        st.title('')
        st.title('')


        st.subheader(f'{level_kr} ë§ì¶¤ ë‹µë³€ ì°¸ê³  ë¬¸í—Œ ({refer_db})')
        doc_name_list = [d.metadata['source'].split("/")[-1] for d in docs]
        doc_content_list = [d.page_content for d in docs]

        for i in range(k):
            idx = i+1
            st.text('')
            st.text(f'ì°¸ê³  ë¬¸í—Œ {idx}.')
            with st.expander(f'{doc_name_list[i]}'):
                st.info(doc_content_list[i])
            st.header('')

        


# Web App ì‹¤í–‰ í•¨ìˆ˜
def PJT1():
    init()

    # ë©”ì¸ í™”ë©´ GUI
    st.title("SSAFY PJT I")
    st.subheader(" : ê¸ˆìœµ/ê²½ì œ ì§€ì‹êµìœ¡ RetrievalGPT")
    
    with st.sidebar:
        st.header('ì‚¬ìš©ì ì •ë³´ ì…ë ¥')
        st.text('')

        user_name = st.selectbox("ğŸ¯ êµìœ¡ ëŒ€ìƒ", ('','ì–´ë¦°ì´','ì²­ì†Œë…„','ì„±ì¸',))
        st.caption('')

        finance_db = st.selectbox("ğŸ’° ê¸ˆìœµ/ê²½ì œ ì§€ì‹ DB", ('','í•œêµ­ì€í–‰'))
        st.caption('')

        refer_db = st.selectbox("ğŸ“š ì°¸ê³ ë¬¸í—Œ ê±´ìˆ˜", ('','3ê±´','4ê±´','5ê±´',))
        st.caption('')

        st.header('ì±—ë´‡ëª¨ë¸ ì •ë³´ ì…ë ¥')
        st.text('')

        
        chatgpt_api = st.text_input('ChatGPT API Key:', type='password')
        if chatgpt_api:
            st.success('API Key í™•ì¸ ì™„ë£Œ!', icon='âœ…')
            os.environ["OPENAI_API_KEY"] = chatgpt_api
        else:
            st.warning('API keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.', icon='âš ï¸')

        st.text('')
        st.text('')
        st.subheader('ğŸ“‹ ì˜µì…˜')
        gpt_visualize = st.checkbox('ğŸ¤– ì±—ë´‡ ì‹œì‘í•˜ê¸°')


    st.divider()
    st.title(f"ğŸ¯ {user_name} ë§ì¶¤ ê¸ˆìœµ/ê²½ì œ êµìœ¡")
    st.caption('')
    st.title(" ")
    st.title(" ")
    

    if gpt_visualize:
        with st.form("my_form"):
            user_input = st.text_input('ê¸ˆìœµ/ê²½ì œ ê´€ë ¨ ì§ˆë¬¸', 'ì˜ˆì‹œ) ê¸ˆìœµê³µë¶€ë¥¼ í•´ì•¼í•˜ëŠ” ì´ìœ ë¥¼ ì•Œë ¤ì¤˜')
            submitted = st.form_submit_button("ì§ˆë¬¸ ì…ë ¥")

        st.divider()
        st.title(" ")
        finance_gpt(user_name,user_input,refer_db)




# # // VectorDB êµ¬ì¶•==============================================================
# init_db()   # Text -> VectorDB êµ¬ì¶•ì„ ìœ„í•´ ìµœì´ˆ ì‹¤í–‰ (ì²¨ë¶€í•œ DB.ZipíŒŒì¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
# #===============================================================================



# .venv\Scripts\activate.bat
PJT1()