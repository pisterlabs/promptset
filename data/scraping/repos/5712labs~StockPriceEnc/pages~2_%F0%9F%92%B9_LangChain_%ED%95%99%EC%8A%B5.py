# pip install langchain=0.0.142
# pip install openai=0.27.4
# pip install tiktoken=0.3.3
# pip install pinecone-client
# https://blog.futuresmart.ai/building-a-document-based-question-answering-system-with-langchain-pinecone-and-llms-like-gpt-4-and-chatgpt#heading-7-embedding-documents-with-openai
import streamlit as st
import os
import pinecone
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings 
from langchain.text_splitter import RecursiveCharacterTextSplitter 
from langchain.document_loaders import TextLoader
from langchain.document_loaders import DirectoryLoader
from langchain.vectorstores import Pinecone
import tiktoken
import re
from components import convert
import pandas as pd
import numpy as np
import time
import requests
# import pprint
import json


title = 'LLM Learning'
st.set_page_config(page_title=title, page_icon="ğŸ", layout="wide")
st.title(title)

if convert.check_password() == False:
    st.stop()

os.environ['OPENAI_API_KEY'] = st.secrets["api_key"]

# tab1, tab2, tab3, tab4 , tab5 = st.tabs(
#     ["í•™ìŠµ(txt)_Pinecone", 
#     "ì „ì²´ëª©ë¡_Pinecone",
#     "ğŸ—ƒ í•™ìŠµ(txt)_Chroma",
#     "ì „ì²´ëª©ë¡_Chroma",
#     "í•™ìŠµ(csv)_FAISS"]
#     )

tab3, tab4, tab1, tab2, tab5 = st.tabs(
    [
        "ğŸ—ƒ í•™ìŠµ(txt)_Chroma",
        "ì „ì²´ëª©ë¡_Chroma",
        "í•™ìŠµ(txt)_Pinecone", 
        "ì „ì²´ëª©ë¡_Pinecone",
        "í•™ìŠµ(csv)_FAISS"
        ]
    )

# llm_learn_type = st.radio(label = 'select llm Learning', options = ['Pinecone', 'Chroma', 'FAISS'], index=0)
# st.write('<style>div.row-widget.stRadio > div{flex-direction:row;}</style>', unsafe_allow_html=True)


#########################################################################
### í•™ìŠµ(txt)_Chroma #####################################################
#########################################################################
with tab3:
    # print(f'start tab3 {tab3}')
    st.write(""" ### ğŸ§ ì „ì²´ ëª©ë¡ ë³´ê¸° """)
    # persist_directory="db"
    # embedding = OpenAIEmbeddings()
    
    readAll_button = st.button("Read All Data From Chroma Local DB", key="readAllChroma", type='secondary')
    if readAll_button:
        persist_directory="db"
        embedding = OpenAIEmbeddings()
        chromadb = Chroma(
        embedding_function=embedding,
        persist_directory=persist_directory)
        ids_df = pd.DataFrame(chromadb.get())
        st.dataframe(
            data=ids_df,
            # height=1000,
            width=1200,
            hide_index=False,
        )
        chromadb.persist()
        chromadb = None

    st.write("""  """)
    st.write(""" ### ğŸƒ txt íŒŒì¼ í•™ìŠµí•˜ê¸° """)
    # file_glob = 'DTSM-IR-203_008.txt'
    file_glob = 'DTSM-PU*'

    # txt_chroma_button = st.button(f"Read {file_glob}", key="txtChroma", type='secondary')
    # if txt_chroma_button:
    loader = DirectoryLoader('./sources', glob=file_glob, loader_cls=TextLoader)
    documents_chroma = loader.load()

    def num_tokens_from_string(string: str, encoding_name: str) -> int:  
        """Returns the number of tokens in a text string."""  
        encoding = tiktoken.get_encoding(encoding_name)  
        num_tokens = len(encoding.encode(string))  
        return num_tokens
    # st.write(f'{num_tokens_from_string(documents[0].page_content, encoding_name="cl100k_base")} í† ê·¼ì´ ì˜ˆìƒë©ë‹ˆë‹¤')

    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬(chunk) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê¸°
    chunk_size = 1000
    text_chroma_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, 
        chunk_overlap=20,
        add_start_index = True,
        )
    texts_chroma = text_chroma_splitter.split_documents(documents_chroma)
    st.info(f""" 
            {len(documents_chroma)}ê°œì˜ ë¬¸ì„œì— í¬í•¨ëœ {len(documents_chroma[0].page_content)}ê°œì˜ ë‹¨ì–´ë¥¼ {chunk_size} ì²­í¬ ë‹¨ìœ„ë¡œ {len(texts_chroma)}ê°œì˜ ë¬¸ì„œë¡œ ë¶„í•  í•˜ì˜€ìŠµë‹ˆë‹¤.

            
            """)
    # {num_tokens_from_string(documents[0].page_content, encoding_name="cl100k_base")} í† ê·¼ì´ ì˜ˆìƒë©ë‹ˆë‹¤.

    # ì—…ë¡œë“œ í…ìŠ¤íŠ¸ Dataframe í˜•ì‹ìœ¼ë¡œ í™•ì¸í•˜ê¸°
    texts_chroma_df = pd.DataFrame()
    for text_chroma in texts_chroma :
        text_chroma_df = pd.DataFrame({'page_content': [text_chroma.page_content], 'metadata': [text_chroma.metadata]})
        texts_chroma_df = pd.concat([texts_chroma_df, text_chroma_df])
    texts_chroma_df.reset_index(drop=True, inplace=True)
    st.dataframe(
        data=texts_chroma_df,
        # height=1000,
        width=1200,
        hide_index=False,
    )

    upsert_button = st.button("Upsert to Chroma Local DB", key="txtUpsertChroma", type='primary')
    if upsert_button :
        # persist_directory="/content/drive/My Drive/Colab Notebooks/chroma/romeo"
        persist_directory="db"
        # os.environ['OPENAI_API_KEY'] = st.secrets["api_key"]
        embedding = OpenAIEmbeddings()
        vectordb = Chroma.from_documents(
            documents=texts_chroma,
            embedding=embedding, 
            persist_directory=persist_directory)  
        vectordb.persist()
        vectordb = None
        st.info(f""" 
        ### ì—…ë¡œë“œë¥¼ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤.
        #### Chroma
        [https://docs.trychroma.com/getting-started/](https://docs.trychroma.com/getting-started/)
        """)

#########################################################################
### í•™ìŠµ(txt)_Pinecone ###################################################
#########################################################################
with tab1:
    # print(f'start tab1 {tab1}')
    st.header("í•™ìŠµ(txt)_Pinecone")
    pinecone.init(api_key=f"{st.secrets['api_pine']}", environment='gcp-starter')
    index_name = 'dwlangchain'
    # st.write('pinecone.list_indexes()')
    # st.write(pinecone.list_indexes())
    # loader = DirectoryLoader('./sources', glob='*.txt', loader_cls=TextLoader)

    st.write("""  """)
    st.write(""" ### ğŸƒ txt íŒŒì¼ í•™ìŠµí•˜ê¸° """)
    # file_glob = 'DTSM-IR-203_009.txt'
    # txt_pinecone_button = st.button(f"Read {file_glob}", key="txtPinecone", type='secondary')
    # if txt_pinecone_button:
    loader_pinecone = DirectoryLoader('./sources', glob='DTSM-IR-203_009.txt', loader_cls=TextLoader)
    documents_pinecone = loader_pinecone.load()
    # text ì •ì œ
    output = []
    # https://study-easy-coding.tistory.com/67
    for page_pinecone in documents_pinecone:
        text = page_pinecone.page_content
        text = re.sub(r'(\w+)-\n((\w+))', r'\1\2', text) # ì•ˆë…•-\ní•˜ì„¸ìš” -> ì•ˆë…•í•˜ì„¸ìš”
        text = re.sub(r'(?<!\n\s)\n(?!\s\n)', ' ' , text.strip()) # "ì¸\nê³µ\n\nì§€ëŠ¥íŒ©í† ë¦¬ -> ì¸ê³µì§€ëŠ¥íŒ©í† ë¦¬
        text = re.sub(r'\n\s*\n', '\n\n' , text) # "\në²„\n\nê±°\n\ní‚¹\n => ë²„\nê±°\ní‚¹
        delete_word = re.sub(r'[\(]+[\w\s]+[\)]+','',text) #ìŠ¤íŠ¸ë§ ë³‘ê¸°ë¬¸ìë¥¼ ì‚­ì œí•˜ê³  ê·¸ ê°’ì„ replace_wordì— ìŠ¤íŠ¸ë§ìœ¼ë¡œ ë‹´ê¹€
        output.append(text)
    # st.write(documents)
    # st.write(output)

    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬(chunk) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê¸°
    chunk_size = 1000
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=20)
    texts = text_splitter.split_documents(documents_pinecone)

    st.info(f""" 
            {len(documents_pinecone)}ê°œì˜ ë¬¸ì„œì— í¬í•¨ëœ {len(documents_pinecone[0].page_content)}ê°œì˜ ë‹¨ì–´ë¥¼ {chunk_size} ì²­í¬ ë‹¨ìœ„ë¡œ {len(texts)}ê°œì˜ ë¬¸ì„œë¡œ ë¶„í•  í•˜ì˜€ìŠµë‹ˆë‹¤.

            
            """)
    texts_df = pd.DataFrame()
    for text in texts :
        text_df = pd.DataFrame({'page_content': [text.page_content], 'metadata': [text.metadata]})
        texts_df = pd.concat([texts_df, text_df])
    texts_df.reset_index(drop=True, inplace=True)
    st.dataframe(
        data=texts_df,
        # height=1000,
        width=1200,
        hide_index=False,
    )

    upsert_button = st.button("Upsert to Pinecone DB", key="upsertPinecone", type='primary')
    if upsert_button:
        # index = pinecone.Index("dwlangchain")
        index_name = 'dwlangchain'
        embedding = OpenAIEmbeddings()
        index = Pinecone.from_documents(texts, embedding, index_name=index_name)
        st.info(f""" 
        ### ì—…ë¡œë“œë¥¼ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤.
        #### Pinecone
        [https://app.pinecone.io/](https://app.pinecone.io/).
        """)

#########################################################################
### ì „ì²´ëª©ë¡_Pinecone #####################################################
#########################################################################
with tab2:
    # print(f'start tab2 {tab2}')
    pinecone.init(api_key=f"{st.secrets['api_pine']}", environment='gcp-starter')
    index_name = 'dwlangchain'

    st.info(f""" 
        ###### Pinecone
        [https://app.pinecone.io/](https://app.pinecone.io/)
        """)
    
    def all_ids_data():
        indexquery = pinecone.Index(index_name)
        namespace = ''
        if len(indexquery.describe_index_stats()["namespaces"]) == 0:
            st.write('ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
            st.stop()
        num_vectors = indexquery.describe_index_stats()["namespaces"][namespace]['vector_count']
        st.write(num_vectors)

        num_dimensions = 1536
        all_ids_df = pd.DataFrame()
        while len(all_ids_df) < num_vectors:
            input_vector = np.random.rand(num_dimensions).tolist()
            results = indexquery.query(
                vector=input_vector, 
                top_k=10000,
                include_values=False,
                include_metadata=True
                )
            for result in results['matches']:
                ids_df = pd.DataFrame([[result['id'], result['metadata']['text'], result['metadata']['source']]])
                all_ids_df = pd.concat([all_ids_df, ids_df])
        all_ids_df.reset_index(drop=True, inplace=True)
        all_ids_df.columns = ['id', 'text', 'source']
        return all_ids_df

    with st.spinner('Wait for it...'):
        all_ids_df = all_ids_data().sort_values(by=['text'], ascending=True)
        st.write(all_ids_df)

        # ë²¡í„° ì¤‘ë³µê°’ ì œê±°
        delete_dup_ids_button = st.button("ì¤‘ë³µê°’ ì œê±°", key="deleteadup", type='primary')
        if delete_dup_ids_button:
            all_ids_df = all_ids_data().sort_values(by=['text'], ascending=True)
            dup = all_ids_df.duplicated(['text'], keep='first')
            all_dup_df = pd.concat([all_ids_df, dup], axis=1)
            all_dup_df.rename(columns = {0 : 'dup'}, inplace = True)
            st.write(all_dup_df)
            remain_first_df = all_dup_df[all_dup_df['dup'] == True]['id'].values.tolist()
            st.write(remain_first_df)
            indexquery = pinecone.Index(index_name)
            indexquery.delete(ids=remain_first_df, namespace='')
            st.info(f""" ì¤‘ë³µê°’ì„ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤. """)  
            time.sleep(2)
        
        # ë²¡í„° ì „ì²´ê°’ ì œê±°
        delete_all_ids_button = st.button("ì „ì²´ê°’ ì œê±°", key="deleteall", type='primary')
        if delete_all_ids_button:
            all_ids_df = all_ids_data()
            all_ids = all_ids_df['id'].values.tolist()
            indexquery = pinecone.Index(index_name)
            indexquery.delete(ids=all_ids, namespace='')
            st.info(f""" ì „ì²´ ë°ì´í„°ë¥¼ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤. """)
            time.sleep(2)

#########################################################################
### ì „ì²´ëª©ë¡_Chroma #######################################################
#########################################################################
with tab4:
    # cli = chromadb.Client()
    # client = chromadb.PersistentClient(path='http://scal.daewooenc.com:8501/flydw/chroma/')
    # client = chromadb.HttpClient(host="http://scal.daewooenc.com:8501/chroma/", port=8501)
    # client = chromadb.HttpClient(host="http:/wooenc.com:8501/chroma/", port=8501)
    # st.write(client)
    # collection2 = client.list_collections()
    # st.write(collection2)

    # ë¶ˆëŸ¬ì˜¤ê¸°
    persist_directory="db"
    embedding = OpenAIEmbeddings()
    vectordb = Chroma(
        embedding_function=embedding, 
        persist_directory=persist_directory)  
    
    ids_df = pd.DataFrame(vectordb.get())
    st.dataframe(
        data=ids_df,
        height=2000,
        width=2000,
        hide_index=False,
        column_config={
            # "documents": st.column_config.LinkColumn(
            #     # "Trending apps",
            #     # help="The top trending Streamlit apps",
            #     # validate="^https://[a-z]+\.streamlit\.app$",
            #     max_chars=150,
            # ),
            "documents": st.column_config.TextColumn(
                # "ë³¸ë¬¸ ë‚´ìš©", ì´ë¦„ ë³€ê²½
                width=900,
                # help="Streamlit **widget** commands ğŸˆ",
                # default="st.",
                # max_chars=500,
                # validate="^st\.[a-z_]+$",
            ),
            "widgets": st.column_config.Column(
                width='large'
            )
        }

    )

    # Using embedded DuckDB with persistence: data will be stored in: ./chroma_db
    # dict_keys(['ids', 'embeddings', 'documents', 'metadatas'])
    # 7580

    # retriever = vectordb.as_retriever()
    # retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    # docs = retriever.get_relevant_documents("í‡´ì‚¬ì ìˆ˜ ì•Œë ¤ì¤˜")

    # for doc in docs:
    #     st.write(doc.metadata["source"])

# st.write(pinecone.list_indexes())
# st.write(index)

# Upsert sample data (5 8-dimensional vectors)
# index.upsert([
#     ("A", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),
#     ("E", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])
# ])

# index_name = pinecone.Index("dwlangchain")


# import openai

# # get api key from platform.openai.com
# openai.api_key = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'

# embed_model = "text-embedding-ada-002"
# query = (
#     "Which training method should I use for sentence transformers when " +
#     "I only have pairs of related sentences?"
# )
# res = openai.Embedding.create(
#     input=[query],
#     engine=embed_model
# )

# # retrieve from Pinecone
# xq = res['data'][0]['embedding']
# # get relevant contexts (including the questions)
# res = index.query(xq, top_k=2, include_metadata=True)

#########################################################################
### í•™ìŠµ(csv)_FAISS ######################################################
#########################################################################
with tab5:
    st.stop()
    from langchain.document_loaders.csv_loader import CSVLoader
    from langchain.vectorstores import FAISS
    from langchain.prompts import PromptTemplate
    from langchain.chat_models import ChatOpenAI
    from langchain.chains import LLMChain
    # loader = DirectoryLoader('./sources', glob='DTSM-PU-310_002.txt', loader_cls=TextLoader)
    loader = CSVLoader(file_path="./sources/sales_response.csv")
    documents = loader.load()

    st.write(documents)
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(documents, embeddings)


    def retrieve_info(query):
        similar_response = db.similarity_search(query, k=3)
        page_contents_array = [doc.page_content for doc in similar_response]
        # print(page_contents_array)
        return page_contents_array

    # 3. Setup LLMChain & prompts
    llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-16k-0613")

    template = """
    You are a world class business development representative. 
    I will share a prospect's message with you and you will give me the best answer that 
    I should send to this prospect based on past best practies, 
    and you will follow ALL of the rules below:

    1/ Response should be very similar or even identical to the past best practies, 
    in terms of length, ton of voice, logical arguments and other details

    2/ If the best practice are irrelevant, then try to mimic the style of the best practice to prospect's message

    Below is a message I received from the prospect:
    {message}

    Here is a list of best practies of how we normally respond to prospect in similar scenarios:
    {best_practice}

    Please write the best response that I should send to this prospect:
    """

    prompt = PromptTemplate(
        input_variables=["message", "best_practice"],
        template=template
    )
    st.write(prompt)
    chain = LLMChain(llm=llm, prompt=prompt)


    # 4. Retrieval augmented generation
    def generate_response(message):
        best_practice = retrieve_info(message)
        response = chain.run(message=message, best_practice=best_practice)
        return response
    st.write('5712')
    message = st.text_area("customer message")

    if message:
        st.write("Generating best practice message...")

        result = generate_response(message)

        st.info(result)