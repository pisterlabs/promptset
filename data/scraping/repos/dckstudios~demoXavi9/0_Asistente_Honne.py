import json
import pandas as pd
import streamlit as st
from pathlib import Path
import core.templates as pr
from langchain import OpenAI, SQLDatabase
from langchain import  LLMChain
import re
import core.interface as it
import os
import html
from langchain.document_loaders import UnstructuredPowerPointLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores.pgvector import PGVector
from langchain.docstore.document import Document
import logging
from langchain.vectorstores import PGEmbedding
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.prompts import PromptTemplate

st.set_page_config(
    page_title="Honne Services APP",
    page_icon="üëã",
)


#connection string "postgresql+psycopg2://harrisonchase@localhost:5432/test3"
CONNECTION_STRING = st.secrets['CONNECTION_STRING']
#collection name
COLLECTION_NAME = st.secrets['COLLECTION_NAME']

os.environ["OPENAI_API_KEY"] = st.secrets['OPENAI_API_KEY']

#EMBEDING
embeddings = OpenAIEmbeddings()

llm = ChatOpenAI(temperature = 0.0, model = 'gpt-3.5-turbo-16k', verbose=True)
#llm = OpenAI(model_name="gpt-3.5-turbo-16k", temperature=0, verbose=True)

#Store
store = PGVector(
    collection_name=COLLECTION_NAME,
    connection_string=CONNECTION_STRING,
    embedding_function=embeddings,
)

retriever = store.as_retriever()

st.title("üë®‚Äçüíª Honne atenci√≥n al cliente")
st.write (":warning: esto es una versi√≥n preliminar a falta de refinamientos.")


PROMPT = pr.prompts_honne_default()



question = st.text_area("Inserta tu consulta")
boton = st.button('Enviar')
if question and  boton:
   
    with st.spinner('Cargando...'):
        llm_chain =   RetrievalQAWithSourcesChain.from_chain_type(
                          llm=llm,   
                          chain_type="stuff", 
                          retriever=retriever,
                          verbose=True,                         
                         
                          
                      )
        simple_prompt = str(PROMPT) + str(question)
        respuesta =  llm_chain({"question": simple_prompt})
        
        st.write("### Respuesta")   
        print(respuesta)
          
        # Decode the response.       
        #decoded_response = it.decode_response(respuesta)
        # Write the response to the Streamlit app.
        
        it.write_response(respuesta)
        try: 
            st.write("---")
        except  ValueError:
            st.markdown(
                "No se ha podido procesar la pregunta, por favor provea de m√°s contexto para poder procesar la informaci√≥n. Si el problema persiste, contecte con soporte"
            )
            st.markdown(
                "Detalle"      )
            st.markdown(
                ValueError      )