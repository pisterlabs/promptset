import os
import shutil

from app_modules.overwrites import postprocess
from app_modules.presets import *
from check_then_answer import ask_gpt, query_vector_store
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS


embedding_model_name = "models/embedding_models/text2vec"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)


def predict(query, history):
    vector_store_path = "data/vector_stores/laws_vector_store"
    retrieved_laws = query_vector_store(query=query, vector_store_path=vector_store_path, embeddings=embeddings)
    input = (
        f"é—®é¢˜ï¼š{query} \n "
        f"ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ£€ç´¢åˆ°ç›¸å…³æ³•æ¡å¦‚ä¸‹ï¼š\n"
        f"{''.join(retrieved_laws)}\n"
        f"åˆ©ç”¨ä»¥ä¸Šæ£€ç´¢åˆ°çš„æ³•æ¡ï¼Œè¯·å›ç­”é—®é¢˜ï¼š{query}\n"
        f"è¦æ±‚é€»è¾‘å®Œå–„ï¼Œæœ‰ç†æœ‰æ®ï¼Œä¸å…è®¸ä¼ªé€ äº‹å®ã€‚"
    )
    completion = ask_gpt(input=input)
    if history == None:
        history = []
    history.append((query, completion))
    chatbot = history
    print("ç­”æ¡ˆï¼š", completion)
    message = ""

    return [message, chatbot, state, "\n-----------------------------\n\n".join(retrieved_laws)]


with open("assets/custom.css", "r", encoding="utf-8") as f:
    customCSS = f.read()

with gr.Blocks(css=customCSS, theme=small_and_beautiful_theme) as demo:
    gr.Markdown("""<h1><center>AssistantLawer-å¤§æ¨¡å‹æ³•å¾‹åŠ©æ‰‹</center></h1>
        <center><font size=3>
        </center></font>
        """)
    state = gr.State()

    with gr.Row():
        with gr.Column(scale=1):
            embedding_model = gr.Dropdown([
                "text2vec-large"
            ],
                label="Embedding model",
                value="text2vec-large")


        with gr.Column(scale=4):
            with gr.Row():
                chatbot = gr.Chatbot(label='AssistantLawyer').style(height=400)
            with gr.Row():
                message = gr.Textbox(label='è¯·è¾“å…¥é—®é¢˜')
            with gr.Row():
                send = gr.Button("ğŸš€ å‘é€")
            with gr.Row():
                gr.Markdown("""æé†’ï¼š<br>
                                        AssistantLawyer æ˜¯åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯æ„å»ºçš„ï¼Œå®ƒå¯ä»¥æä¾›æœ‰ä»·å€¼çš„æ³•å¾‹å»ºè®®å’Œè§£é‡Šï¼Œä½†ä¸åº”è§†ä¸ºæ³•å¾‹ä¸“å®¶çš„æ›¿ä»£å“ã€‚åœ¨é‡è¦çš„æ³•å¾‹äº‹åŠ¡ä¸­ï¼Œå»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šçš„æ³•å¾‹é¡¾é—®æˆ–å¾‹å¸ˆã€‚ <br>
                                        """)
        with gr.Column(scale=2):
            search = gr.Textbox(label='æœç´¢ç»“æœ')

        # å‘é€æŒ‰é’® æäº¤
        send.click(predict,
                   inputs=[
                       message,
                       chatbot,
                       # state
                   ],
                   outputs=[message, chatbot, state, search])


        # è¾“å…¥æ¡† å›è½¦
        message.submit(predict,
                       inputs=[
                           message,
                           chatbot
                       ],
                       outputs=[message, chatbot, state, search])

demo.queue(concurrency_count=2).launch(
    server_name='0.0.0.0',
    # server_port=8888,
    share=False,
    show_error=True,
    debug=True,
    enable_queue=True,
    inbrowser=True,
)


    #query = "è°å¯ä»¥ç”³è¯·æ’¤é”€ç›‘æŠ¤äººçš„ç›‘æŠ¤èµ„æ ¼?"
