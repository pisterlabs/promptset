import streamlit as st
import numpy as np
import pandas as pd
import tempfile
import re
import os
import time
import pinecone
from typing import List, Union,Callable,Dict, Optional, Any
from langchain.prompts import PromptTemplate
from langchain.document_loaders import PyPDFLoader,Docx2txtLoader,UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain import LLMChain
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain.vectorstores import Pinecone
import asyncio

st.set_page_config(
    page_title="ChatReport",
    page_icon="https://raw.githubusercontent.com/dengxinkai/cpanlp_streamlit/main/app/%E6%9C%AA%E5%91%BD%E5%90%8D.png",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://www.cpanlp.com/',
        'Report a bug': "https://www.cpanlp.com/",
        'About': "æ™ºèƒ½æŠ¥å‘Š"
    }
)
def web_file_pdf(input_text):
    loader = PyPDFLoader(input_text)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    texts = text_splitter.split_documents(documents)
    Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
    st.success(f"å·²ä¸Šä¼ æ¥è‡ª PDF æ–‡ä»¶çš„ {len(texts)} ä¸ªæ–‡æ¡£ã€‚â€")
    st.cache_data.clear()
def web_file_pptx(input_text):
    loader = UnstructuredPowerPointLoader(input_text)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    texts = text_splitter.split_documents(documents)
    Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
    st.success(f"å·²ä¸Šä¼ æ¥è‡ª PPTX æ–‡ä»¶çš„ {len(texts)} ä¸ªæ–‡æ¡£ã€‚â€")
    st.cache_data.clear()
def web_file_docx(input_text):
    loader = Docx2txtLoader(input_text)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    texts = text_splitter.split_documents(documents)
    Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
    st.success(f"å·²ä¸Šä¼ æ¥è‡ª DOCX æ–‡ä»¶çš„ {len(texts)} ä¸ªæ–‡æ¡£ã€‚â€")
    st.cache_data.clear()    

#     @st.cache_resource
def upload_file_pdf():
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(file.read())
        tmp_file.flush()
        loader = PyPDFLoader(tmp_file.name)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        texts = text_splitter.split_documents(documents)
        Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
        st.success(f"å·²ä¸Šä¼ æ¥è‡ª PDF æ–‡ä»¶çš„ {len(texts)} ä¸ªæ–‡æ¡£ã€‚â€")
        st.cache_data.clear()
def upload_file_pptx():
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(file.read())
        tmp_file.flush()
        loader = UnstructuredPowerPointLoader(tmp_file.name)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        texts = text_splitter.split_documents(documents)
        Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
        st.success(f"å·²ä¸Šä¼ æ¥è‡ª PPTX æ–‡ä»¶çš„ {len(texts)} ä¸ªæ–‡æ¡£ã€‚â€")
        st.cache_data.clear()
def upload_file_docx():
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(file.read())
        tmp_file.flush()
        loader = Docx2txtLoader(tmp_file.name)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        texts = text_splitter.split_documents(documents)
        Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
        st.success(f"å·²ä¸Šä¼ æ¥è‡ª DOCX æ–‡ä»¶çš„ {len(texts)} ä¸ªæ–‡æ¡£ã€‚â€")
        st.cache_data.clear()
def upload_file():
    file_ext = os.path.splitext(file.name)[1].lower()
    if file_ext == ".pptx":
        upload_file_pptx()
    elif file_ext == ".pdf":
        upload_file_pdf()
    elif file_ext == ".docx":
        upload_file_docx()
    else:
        st.warning("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·ä¸Šä¼  PPTXã€DOCX æˆ– PDF æ–‡ä»¶ã€‚")
def web_file(input_text):
    if input_text.lower().endswith('.pptx'):
        web_file_pptx(input_text)
    elif input_text.lower().endswith('.pdf'):
        web_file_pdf(input_text)
    elif input_text.lower().endswith('.docx'):
        web_file_docx(input_text)
    else:
        st.warning("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·ä¸Šä¼  PPTX ã€DOCX æˆ– PDF æ–‡ä»¶ã€‚")
def upload_query(input_file):
    ww=""
    pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
          environment="us-west1-gcp-free", 
          namespace=pinename
          )
    index = pinecone.Index(index_name="kedu")
    a=embeddings_cho.embed_query(input_file)
    www=index.query(vector=a, top_k=top_k, namespace=pinename, include_metadata=True)
    for i in range(top_k):
        ww+=www["matches"][i]["metadata"]["text"]
    return ww

logo_url = "https://raw.githubusercontent.com/dengxinkai/cpanlp_streamlit/main/app/%E6%9C%AA%E5%91%BD%E5%90%8D.png"
with st.sidebar:
    with st.expander("ğŸ‘‡ :blue[**ç¬¬ä¸€æ­¥ï¼šè¾“å…¥ OpenAI API å¯†é’¥**]"):
        if 'input_api' in st.session_state:
            st.text_input("api-key",st.session_state["input_api"], key="input_api")
        else:
            st.info('è¯·å…ˆè¾“å…¥æ­£ç¡®çš„OpenAI API å¯†é’¥')
            st.text_input('api-key','', key="input_api",type="password")
        st.write("ChatOpenAIå±æ€§é…ç½®")
        temperature = st.slider("`temperature`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬éšæœºæ€§å’Œå¤šæ ·æ€§çš„å‚æ•°ã€‚è¾ƒé«˜çš„æ¸©åº¦å€¼é€šå¸¸é€‚ç”¨äºç”Ÿæˆè¾ƒä¸ºè‡ªç”±æµç•…çš„æ–‡æœ¬ï¼Œè€Œè¾ƒä½çš„æ¸©åº¦å€¼åˆ™é€‚ç”¨äºç”Ÿæˆæ›´åŠ ç¡®å®šæ€§çš„æ–‡æœ¬ã€‚")
        frequency_penalty = st.slider("`frequency_penalty`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬ä¸­å•è¯é‡å¤é¢‘ç‡çš„æŠ€æœ¯ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹å¯¹å•è¯é‡å¤ä½¿ç”¨çš„æƒ©ç½šå°±è¶Šä¸¥æ ¼ï¼Œç”Ÿæˆæ–‡æœ¬ä¸­å‡ºç°ç›¸åŒå•è¯çš„æ¦‚ç‡å°±è¶Šä½ï¼›æ•°å€¼è¶Šå°ï¼Œç”Ÿæˆæ–‡æœ¬ä¸­å‡ºç°ç›¸åŒå•è¯çš„æ¦‚ç‡å°±è¶Šé«˜ã€‚")
        presence_penalty = st.slider("`presence_penalty`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶è¯­è¨€ç”Ÿæˆæ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶å¯¹è¾“å…¥æç¤ºçš„é‡è§†ç¨‹åº¦çš„å‚æ•°ã€‚presence_penaltyçš„å€¼è¾ƒä½ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å¯èƒ½ä¸è¾“å…¥æç¤ºéå¸¸æ¥è¿‘ï¼Œä½†ç¼ºä¹åˆ›æ„æˆ–åŸåˆ›æ€§ã€‚presence_penaltyè®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼Œæ¨¡å‹å¯èƒ½ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€æ›´å…·åŸåˆ›æ€§ä½†ä¸è¾“å…¥æç¤ºè¾ƒè¿œçš„æ–‡æœ¬ã€‚")
        top_p = st.slider("`top_p`", 0.01, 0.99, 0.9,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œè¾ƒå°çš„top_på€¼ä¼šè®©æ¨¡å‹é€‰æ‹©çš„è¯æ›´åŠ ç¡®å®šï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ›´åŠ ä¸€è‡´ï¼Œè€Œè¾ƒå¤§çš„top_på€¼åˆ™ä¼šè®©æ¨¡å‹é€‰æ‹©çš„è¯æ›´åŠ å¤šæ ·ï¼Œç”Ÿæˆçš„æ–‡æœ¬åˆ™æ›´åŠ å¤šæ ·åŒ–ã€‚")
        model = st.radio("`æ¨¡å‹é€‰æ‹©`",
                                ("gpt-3.5-turbo",
                                "gpt-4"),
                                index=0,key="main_model")
    llm=ChatOpenAI(
        model_name=model,
        temperature=temperature,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        top_p=top_p,
        openai_api_key=st.session_state.input_api
    )
    with st.expander("ğŸ‘‡ :blue[**ç¬¬äºŒæ­¥ï¼šåˆ›å»ºè‡ªå·±çš„æ•°æ®åº“æˆ–è¿æ¥åˆ°å·²æœ‰æ•°æ®åº“**]"):
        pinename = st.text_input('**æ•°æ®åº“åç§°**','report',key="pinename",help="ç³»ç»Ÿæ¯å¤©å®šæœŸæ¸…ç†æ•°æ®åº“")
        pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                          environment="us-west1-gcp-free", 
                          namespace=pinename
                          )
        index = pinecone.Index(index_name="kedu")
        if st.button('åˆ é™¤æ•°æ®åº“',key="deletepine"):
            index = pinecone.Index(index_name="kedu")
            index.delete(deleteAll='true', namespace=pinename)
        st.warning("â¬†ï¸ åˆ«å¿˜äº†åˆ é™¤ä¸å†ä½¿ç”¨çš„æ•°æ®åº“")
    with st.expander("ğŸ‘‡ :blue[**ç¬¬ä¸‰æ­¥ï¼šé€‰æ‹©æ•°æ®åº“æ–‡ä»¶ä¸Šä¼ æ–¹å¼**]"):
        fileoption = st.radio('**æ•°æ®åº“åˆ›å»ºæ–¹å¼**',('æœ¬åœ°ä¸Šä¼ ', 'URL'),key="fileoption")
        chunk_size = st.number_input('chunk_size',value=800,min_value=200,max_value=2500,step=100,key="chunk_size",help='æ¯ä¸ªæ–‡æœ¬æ•°æ®å—çš„å¤§å°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå°†chunk_sizeè®¾ç½®ä¸º1000ï¼Œåˆ™å°†è¾“å…¥æ–‡æœ¬æ•°æ®åˆ†æˆ1000ä¸ªå­—ç¬¦çš„å—ã€‚')
        chunk_overlap = st.number_input('chunk_overlap',value=0,min_value=0,max_value=500,step=50,key="chunk_overlap",help='æ¯ä¸ªæ–‡æœ¬æ•°æ®å—ä¹‹é—´é‡å çš„å­—ç¬¦æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå°†chunk_overlapè®¾ç½®ä¸º200ï¼Œåˆ™ç›¸é‚»çš„ä¸¤ä¸ªå—å°†æœ‰200ä¸ªå­—ç¬¦çš„é‡å ã€‚è¿™å¯ä»¥ç¡®ä¿åœ¨å—ä¹‹é—´æ²¡æœ‰ä¸¢å¤±çš„æ•°æ®ï¼ŒåŒæ—¶è¿˜å¯ä»¥é¿å…é‡å¤å¤„ç†ç›¸é‚»å—ä¹‹é—´çš„æ•°æ®ã€‚')
        top_k = st.number_input('top_k',value=3,min_value=0,max_value=10,step=1,key="top_k",help="ç”¨äºæ§åˆ¶æŸ¥è¯¢çš„ç»“æœæ•°é‡ï¼ŒæŒ‡å®šä»æ•°æ®åº“ä¸­è¿”å›çš„ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„å‰ k ä¸ªå‘é‡")
        embeddings_cho = OpenAIEmbeddings(openai_api_key=st.session_state.input_api)
    with st.expander("ğŸ‘‡ :blue[**ç¬¬å››æ­¥ï¼šæ•°æ®ä¸Šä¼ **]"):
        if fileoption=="æœ¬åœ°ä¸Šä¼ ":
            file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ï¼ˆæ”¯æŒæ ¼å¼åŒ…æ‹¬ï¼šPPTXã€DOCXå’ŒPDFï¼‰", type=("pptx",'pdf','docx'),key="upload")
            if file is not None:
                with st.spinner('Wait for it...'):
                    upload_file()
        else:
            input_text = st.text_input('æ–‡ä»¶ç½‘å€ï¼ˆæ”¯æŒæ ¼å¼åŒ…æ‹¬ï¼šPPTXã€DOCX å’Œ PDFï¼‰', 'http://static.cninfo.com.cn/finalpage/2023-04-29/1216712300.PDF',key="webupload")
            if st.button('è½½å…¥æ•°æ®åº“',key="pdfw"):
                with st.spinner('Wait for it...'):
                    pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                          environment="us-west1-gcp-free", 
                          namespace=pinename
                          )
                    index = pinecone.Index(index_name="kedu")
                    web_file(input_text)
                    st.cache_data.clear()
@st.cache_data(persist="disk")
def convert_df(df):
   return df.to_csv(index=False).encode('utf-8')
if st.button('åˆ·æ–°é¡µé¢',key="rerun"):
    st.experimental_rerun()
    st.cache_data.clear()

if st.session_state.input_api:
    do_question=[]
    do_answer=[]
    with get_openai_callback() as cb:  
#ä¸»è¦åŠŸèƒ½                
        input_files = st.text_input('**æ‰¹é‡æŸ¥è¯¢**','#å…¬å¸åç§°#å…¬å¸äº§å“',key="file_webss",help="ä¸åŒé—®é¢˜ç”¨#éš”å¼€ï¼Œæ¯”å¦‚ï¼šå…¬å¸æ”¶å…¥#å…¬å¸åç§°#å…¬å¸å‰æ™¯")
        if st.button('æ•°æ®åº“æ‰¹é‡æŸ¥è¯¢',key="file_uploads1"):
            input_list = re.split(r'#', input_files)[1:]
            async def upload_all_files_async(input_list):
                do_question, do_answer = [], []
                tasks = []
                for input_file in input_list:
                    task = asyncio.create_task(upload_query_async(input_file))
                    tasks.append(task)
                results = await asyncio.gather(*tasks)
                with st.expander("æ•°æ®åº“æŸ¥è¯¢ç»“æœ", expanded=True):
                    for key, inter_result in zip(input_list, results):
                        st.write(key)
                        st.success(inter_result)
                        do_question.append(key)
                        do_answer.append(inter_result)
                return do_question,do_answer
            async def upload_query_async(input_file):
                ww=""
                pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                              environment="us-west1-gcp-free", 
                              namespace=pinename
                              )
                index = pinecone.Index(index_name="kedu")
                a=embeddings_cho.embed_query(input_file)
                www=index.query(vector=a, top_k=top_k, namespace=pinename, include_metadata=True)
                for i in range(top_k):
                    ww+=www["matches"][i]["metadata"]["text"]
                return ww
            do_question, do_answer=asyncio.run(upload_all_files_async(input_list))
        if st.button('AIæ‰¹é‡æŸ¥è¯¢',key="aifile_uploadss",type="primary"):
            with st.spinner('Wait for it...'):
                start_time = time.time()
                input_list = re.split(r'#', input_files)[1:]
                async def upload_all_files_async(input_list):
                    do_question, do_answer = [], []
                    tasks = []
                    for input_file in input_list:
                        task = asyncio.create_task(upload_query_async(input_file))
                        tasks.append(task)
                    results = await asyncio.gather(*tasks)
                    with st.expander("AIæŸ¥è¯¢ç»“æœ", expanded=True):
                        for key, inter_result in zip(input_list, results):
                            st.write(key)
                            st.success(inter_result)
                            do_question.append(key)
                            do_answer.append(inter_result)
                    return do_question,do_answer
                async def upload_query_async(input_file):
                    ww=""
                    ww1=""
                    pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                          environment="us-west1-gcp-free", 
                          namespace='ceshi'
                          )
                    index = pinecone.Index(index_name="kedu")
                    start_time = time.time()
                    a=embeddings_cho.embed_query(input_file)
                    www=index.query(vector=a, top_k=top_k, namespace=pinename, include_metadata=True)
                    for i in range(top_k):
                        ww+=www["matches"][i]["metadata"]["text"]
                    template = """Use the following portion of a long document to see if any of the text is relevant to answer the question. 
                    Return any relevant text verbatim.
                    Respond in Chinese.
                    QUESTION: {question}
                    =========
                    {summaries}
                    =========
                    FINAL ANSWER IN CHINESE:"""
                    prompt = PromptTemplate(
                        input_variables=["summaries", "question"],
                        template=template,
                    )
                    chain = LLMChain(prompt=prompt, llm=llm)
                    ww1=chain.predict(summaries=ww, question=input_file)
                    return ww1
                do_question, do_answer=asyncio.run(upload_all_files_async(input_list))
                end_time = time.time()
                elapsed_time = end_time - start_time
                st.write(f"é¡¹ç›®å®Œæˆæ‰€éœ€æ—¶é—´: {elapsed_time:.2f} ç§’")  
                with st.expander("è´¹ç”¨"):
                        st.success(f"Total Tokens: {cb.total_tokens}")
                        st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                        st.success(f"Completion Tokens: {cb.completion_tokens}")
                        st.success(f"Total Cost (USD): ${cb.total_cost}")
        st.warning("â¬†ï¸ ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢åªéœ€è¦é€šè¿‡ API æ¥å£è·å–åµŒå…¥å‘é‡,ä½¿ç”¨ AI æŸ¥è¯¢éœ€è¦ä½¿ç”¨ API æ¥å£å›ç­”é—®é¢˜ï¼Œå¹¶ä¸”ä¼šäº§ç”Ÿä¸€å®šè´¹ç”¨")
        df_inter = pd.DataFrame({
            'é—®é¢˜':do_question,
            'å›ç­”':do_answer,
             })
        if do_answer:
            with st.expander("å›ç­”è®°å½•"):
                st.dataframe(df_inter, use_container_width=True)
            csv_inter = convert_df(df_inter)
            st.download_button(
               "ä¸‹è½½å›ç­”è®°å½•",
               csv_inter,
               "file.csv",
               "text/csv",
               key='download-csv_inter'
            )
            
else:
    st.header("è¯·å…ˆè¾“å…¥æ­£ç¡®çš„Openai api-key")
    
