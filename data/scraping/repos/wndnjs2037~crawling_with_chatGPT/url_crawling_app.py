import streamlit as st
import pandas as pd
import googletrans
import openai
import time
import requests
import urllib.request
import json
import urllib
import ssl
import re
from newspaper import Config
from newspaper import Article
from langdetect import detect
from youtube_transcript_api import YouTubeTranscriptApi
from konlpy.tag import Kkma
from pykospacing import Spacing
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup


#### ì½”ë“œì—ì„œ ì£¼ì˜ í•  ë¶€ë¶„
# ë¡œì»¬ë³€ìˆ˜, ì „ì—­ë³€ìˆ˜ ì´ë¦„ ë™ì¼í•œê±° ìˆ˜ì • -> ë¡œì»¬ë³€ìˆ˜ ì•ì— _ ì–¸ë”ë°” í•˜ë‚˜ë‚˜ ë‘ê°œ
# ë³€ìˆ˜ê°€ í•˜ëŠ” ì¼ì— ëŒ€í•œ ì„¤ëª…ì´ ë˜ë„ë¡ ì´ë¦„ ì„¤ì • í•„ìš”


# í”„ë¡œê·¸ë¨ ì‹¤í–‰ ìˆœì„œ
# 1. í¬ë¡¤ë§í•˜ê³ ì í•˜ëŠ” URL ì…ë ¥
# 2. í¬ë¡¤ë§ ì§„í–‰
# 3. í¬ë¡¤ë§ ëœ ë°ì´í„°ê°€ ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° googletransë¡œ ë²ˆì—­
# 4. ë²ˆì—­ëœ ê²°ê³¼ë¥¼ chatGPT promptì— ë‹´ì•„ì„œ ì „ì†¡
# (ì „ì†¡í˜•íƒœ : [ì‚¬ìš©ìê°€ ì…ë ¥í•œ prompt] + í¬ë¡¤ë§ ëœ en ë°ì´í„° )
# 5. ìš”ì²­ ê²°ê³¼ ì¶œë ¥ (kr)

ssl._create_default_https_context = ssl._create_unverified_context
user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
config = Config()
translator = googletrans.Translator()
final_result = []

# openAPI ê°œì¸ìš© ì‹œí¬ë¦¿í‚¤
YOUR_API_KEY = 'ë³´ì•ˆìƒì˜ ì´ìœ ë¡œ ê¹ƒí—ˆë¸Œì—ëŠ” ì˜¬ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤'

#  chatGPTì— ëª…ë ¹ì–´ë¥¼ ë‚ ë¦¬ê¸° ìœ„í•œ í•¨ìˆ˜
def chatGPT(prompt, API_KEY=YOUR_API_KEY):

    # set api key
    openai.api_key = API_KEY

    # Call the chat GPT API
    completion = openai.Completion.create(
        # 'text-curie-001'  # 'text-babbage-001' #'text-ada-001'
        engine='text-davinci-003', prompt=prompt, temperature=0.5, max_tokens=2048, top_p=1, frequency_penalty=0, presence_penalty=0)

    return completion['choices'][0]['text']


# ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ í¬ë¡¤ë§ìš© í•¨ìˆ˜
def youtube_script_crawling(id):
    st.write("ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ì¤‘...")

    # 1. ì˜ìƒ ì œëª© ê°’ ê°€ì ¸ì˜¤ê¸°
    # urlê³¼ urlì— í¬í•¨ëœ video idë¥¼ í†µí•´ ì œëª©ì„ í¬ë¡¤ë§ í•´ì˜¬ ë™ì˜ìƒ ì£¼ì†Œì— ì ‘ê·¼
    params = {"format": "json", "url": "https://www.youtube.com/watch?v=%s" % id}
    url = "https://www.youtube.com/oembed"
    query_string = urllib.parse.urlencode(params)
    url = url + "?" + query_string

    with urllib.request.urlopen(url) as response:
        response_text = response.read()
        data = json.loads(response_text.decode())
        video_title = data['title']
        # ì‘ë‹µë°›ì€ ê°’ì—ì„œ ì œëª© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        st.write("ì˜ìƒ ì œëª© : ", video_title)

    # 2.ì˜ìƒì˜ ìë™ìƒì„±ëœ ìë§‰ ê°€ì ¸ì˜¤ê¸° - í•œêµ­ì–´ ìŠ¤í¬ë¦½íŠ¸ë§Œ ê°€ëŠ¥
    # ìë™ìë§‰ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜ìƒì¸ ê²½ìš° ì—ëŸ¬ë‚¨
    youtube_script = YouTubeTranscriptApi.get_transcript(id, languages=['ko']) 

    text = ''

    for i in range(len(youtube_script)):
        text += youtube_script[i]['text'] + ' '  # text ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´

    # ê³µë°± ì œê±°
    text_ = text.replace(' ', '')

    kkma = Kkma()
    text_sentences = kkma.sentences(text_)

    # ë¬¸ì¥ì´ ëë‚˜ëŠ” ì–´ì ˆì„ ì„ì˜ë¡œ ì§€ì •í•´ì„œ ë¶„ë¦¬í•  ìš©ë„ë¡œ ì‚¬ìš©
    lst = ['ì£ ', 'ë‹¤', 'ìš”', 'ì‹œì˜¤', 'ìŠµë‹ˆê¹Œ', 'ì‹­ë‹ˆê¹Œ', 'ë©ë‹ˆê¹Œ', 'ì˜µë‹ˆê¹Œ', 'ë­¡ë‹ˆê¹Œ',]

    # ì œì™¸í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ íŒŒì¼ì„ ì½ì–´ë“¤ì—¬ì„œ forë¬¸ì— ì‚¬ìš© - ì¶”í›„ ë‹¨ì–´ë°ì´í„° set ì¶”ê°€ í•„ìš”
    df = pd.read_csv('not2.csv', encoding='utf-8')
    not_verb = df.loc[:, 'col1'].to_list()

    text_all = ' '.join(text_sentences).split(' ')

    for n in range(len(text_all)):
        i = text_all[n]
        if len(i) == 1:  # í•œê¸€ìì¼ ê²½ìš° ì¶”ê°€ë¡œ ì‘ì—…í•˜ì§€ ì•ŠìŒ
            continue

        else:
            for j in lst:  # ì¢…ê²° ë‹¨ì–´
                # ì§ˆë¬¸í˜•
                if j in lst[4:]:
                    i += '?'

                # ëª…ë ¹í˜•
                elif j == 'ì‹œì˜¤':
                    i += '!'

                # ë§ˆì¹¨í‘œ
                else:
                    if i in not_verb:  # íŠ¹ì • ë‹¨ì–´ ì œì™¸
                        continue

                    else:
                        if j == i[len(i)-1]:  # ì¢…ê²°
                            text_all[n] += '.'
                            # print("\n", text_all[n], end='/ ')

    # ë‚˜ë‰œ ë¬¸ì¥ì„ ë‹¤ì‹œ ì¡°ì¸
    spacing = Spacing()
    text_all_in_one = ' '.join(text_all)
    result = spacing(text_all_in_one.replace(' ', ''))
    st.write(result) # ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„° ì¶œë ¥

    return result

# ë¸ŒëŸ°ì¹˜ ë§í¬ í¬ë¡¤ëŸ¬
# êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ë¥¼ í¬ë¡¤ë§í•´ì˜¤ëŠ” ê³¼ì •ì—ì„œ ì„ ì–¸í•œ browser ê·¸ëŒ€ë¡œ ì‚¬ìš©
# <br> íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬
def brunch_crawler(url):
    st.write("ë¸ŒëŸ°ì¹˜ ë§í¬ì…ë‹ˆë‹¤.")

    # 1. url ì ‘ì†ì„ ìœ„í•œ playwirght ì‹œì‘
    playwright = sync_playwright().start() # í¬ë¡¤ë§ìš© í”Œë ˆì´ë¼ì´íŠ¸ ì‹œì‘
    browser = playwright.chromium.launch(headless=True) 
    page = browser.new_page() # ì›¹ë¸Œë¼ìš°ì € ì—´ê¸°
    page.goto(url) # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ url ì£¼ì†Œë¡œ ì´ë™
    ctx = page.locator('xpath=/html/body') # ì˜¤í”ˆí•œ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ html code path ì§€ì •(bodyë¡œ ì§€ì •í•´ì„œ ì „ì²´ ê°€ì ¸ì˜´)
    ctx_inner_html = ctx.inner_html() # ì¡°íšŒí•œ í˜ì´ì§€ì—ì„œ html ì½”ë“œ ê°€ì ¸ì˜¤ê¸°

    # ë¸ŒëŸ°ì¹˜ ê²Œì‹œê¸€ì˜ ì œëª©ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
    # ì œëª© íƒœê·¸ëª…ì„ ì„ íƒí•œ ë’¤ class ì´ë¦„ ë¶™ì—¬ì„œ ì ‘ê·¼
    title_tag = page.locator('h1.cover_title')
    title_text = title_tag.inner_html().replace("  ","")
    st.write("ê²Œì‹œê¸€ ì œëª© : ", title_text)

    # ì•ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ ë¸ŒëŸ°ì¹˜ start íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ ë‹¤ìŒë¶€í„° ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì‹œì‘ëœë‹¤(ì œëª©, ë³¸ë¬¸)
    under_start_tag_location = ctx_inner_html.find("<div class=\"wrap_body text_align_left finish_txt")

    # ë’·ë¶€ë¶„ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ ë¸ŒëŸ°ì¹˜ end íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ê°€ ìœ„ì¹˜í•œ ê³³ë¶€í„° ë‚´ìš©ì„ ì˜ë¼ë‚¸ë‹¤(ë³¸ë¬¸ì´ ëë‚œ ì´í›„ ë°”ë¡œ ë‹¤ìŒì— ë“±ì¥í•˜ëŠ” íƒœê·¸ë¡œ ì§€ì •)
    over_end_tag_location = ctx_inner_html.find("<div class=\"wrap_body_info") 

    # ì¡°íšŒí•œ íƒœê·¸ì˜ ìœ„ì¹˜ë§Œí¼ html ì½”ë“œë¥¼ ì˜ë¼ë‚´ê¸°
    tag_data = ctx_inner_html[under_start_tag_location:over_end_tag_location]

    # re (ì •ê·œí‘œí˜„ì‹??) ì‚¬ìš©í•´ì„œ ì›í•˜ëŠ” ë¬¸ìì—´ì´ ì „ì²´ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸
    get_br_lotation_list = [m.start() for m in re.finditer('<br>', str(tag_data))]
    print("<br>íƒœê·¸ ìœ„ì¹˜ í™•ì¸:",get_br_lotation_list)

    st.write("ë¸ŒëŸ°ì¹˜ ê²Œì‹œê¸€ì˜ ë¬¸ë‹¨ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.")

    # ë°˜ë³µë¬¸ìœ¼ë¡œ ìœ„ì˜ listë¥¼ ëŒë©´ì„œ, <br> íƒœê·¸ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¼ëƒ„
    # ìë¥¸ ë°ì´í„°ë¥¼ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì„œ ê²°ê³¼ listì— ë”°ë¡œ ì €ì¥
    brunch_iteration_count = 0
    sentence_separation_result_list = []
    for i in range(len(get_br_lotation_list)):
        
        # ë§¨ ì²˜ìŒì€ 0ìœ¼ë¡œ ê³ ì •
        print("ì´ì „ :",get_br_lotation_list[i-1])
        print("í˜„ì¬ :",get_br_lotation_list[i])

        # ì²« ë²ˆì§¸ ë°˜ë³µì¸ ê²½ìš°ì—ë§Œ
        if brunch_iteration_count == 0:

            present_paragraph_location = get_br_lotation_list[i]

            paragraph_text_data = tag_data[0:present_paragraph_location]
            print("paragraph_text_data:",paragraph_text_data)

            # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
            cleantext = BeautifulSoup(paragraph_text_data, "lxml").text
            print(cleantext.replace("\n\n", "")) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸

            brunch_iteration_count += 1

        # ë‘ë²ˆì§¸ ë°˜ë³µë¶€í„° ëª¨ë‘ í•´ë‹¹
        else:
            present_paragraph_location = get_br_lotation_list[i]
            previous_paragraph_location = get_br_lotation_list[i-1]

            paragraph_text_data = tag_data[previous_paragraph_location:present_paragraph_location]
            print("paragraph_text_data:",paragraph_text_data)

            # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
            cleantext = BeautifulSoup(paragraph_text_data, "lxml").text
            # cleantext = cleantext.replace("\n\n", "") # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±°
            st.write("ë¬¸ì¥:",cleantext.replace("\n\n", "")) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸

            brunch_iteration_count += 1
            
        sentence_separation_result_list.append(str(cleantext))
        sentence_separation_result_list = list(filter(None, sentence_separation_result_list)) # ë¹ˆìš”ì†Œê°€ ìˆìœ¼ë©´ ì œê±°
        sentence_separation_result_list = [word.strip('\xa0') for word in sentence_separation_result_list ] # íŠ¹ì • ë¬¸ìì—´ '\xa0' ì œê±°
        sentence_separation_result_list = [word.strip('\n') for word in sentence_separation_result_list ] # íŠ¹ì • ë¬¸ìì—´ '\n' ì œê±°

    # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
    # cleantext = BeautifulSoup(tag_data, "lxml").text
    # print(cleantext.replace("\n\n", "")) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸
    st.write("ë¬¸ë‹¨ ë¶„ë¦¬ ê²°ê³¼:",sentence_separation_result_list)
    st.write("ë¬¸ë‹¨ ë¶„ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´:",len(sentence_separation_result_list))

    if len(sentence_separation_result_list) == 0: # ì œëŒ€ë¡œ í¬ë¡¤ë§ë˜ì§€ ì•Šì€ ê²½ìš° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¤í–‰ìœ¼ë¡œ ëŒ€ì²´
        st.write("ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤...")
        sentence_separation_result_list = newspaper_crawler(url)

    browser.close()
    print("-------------------ì™„ë£Œ------------------")

    

    # ë¬¸ì¥ì´ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    return sentence_separation_result_list

def naver_post_crawler(url):
    st.write("ë„¤ì´ë²„í¬ìŠ¤íŠ¸ ë§í¬ì…ë‹ˆë‹¤.")

    playwright = sync_playwright().start() # í¬ë¡¤ë§ìš© í”Œë ˆì´ë¼ì´íŠ¸ ì‹œì‘
    browser = playwright.chromium.launch(headless=True) 
    page = browser.new_page() # ì›¹ë¸Œë¼ìš°ì € ì—´ê¸°
    page.goto(url) # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ url ì£¼ì†Œë¡œ ì´ë™
    ctx = page.locator('xpath=/html/body') # ì˜¤í”ˆí•œ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ html code path ì§€ì •(bodyë¡œ ì§€ì •í•´ì„œ ì „ì²´ ê°€ì ¸ì˜´)
    ctx_inner_html = ctx.inner_html() # ì¡°íšŒí•œ í˜ì´ì§€ì—ì„œ html ì½”ë“œ ê°€ì ¸ì˜¤ê¸°

    # í¬ìŠ¤íŠ¸ì˜ ì œëª©ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
    # ì œëª© íƒœê·¸ëª…ì„ ì„ íƒí•œ ë’¤ class ì´ë¦„ ë¶™ì—¬ì„œ ì ‘ê·¼
    title_tag = page.locator('h3.se_textarea')
    title_text = title_tag.inner_html().replace("  ","")
    st.write("ê²Œì‹œê¸€ ì œëª© : ", title_text)

    # ì•ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ ë³¸ë¬¸ì´ ì‹œì‘ë˜ëŠ” start íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ ë‹¤ìŒë¶€í„° ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì‹œì‘ëœë‹¤(ë³¸ë¬¸)
    under_start_tag_location = ctx_inner_html.find("<div class=\"se_component_wrap sect_dsc __se_component_area")

    # ë’·ë¶€ë¶„ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ end íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ê°€ ìœ„ì¹˜í•œ ê³³ë¶€í„° ë‚´ìš©ì„ ì˜ë¼ë‚¸ë‹¤(ë³¸ë¬¸ì´ ëë‚œ ì´í›„ ë°”ë¡œ ë‹¤ìŒì— ë“±ì¥í•˜ëŠ” íƒœê·¸ë¡œ ì§€ì •)
    over_end_tag_location = ctx_inner_html.find("<div class=\"state_line") 

    # ì¡°íšŒí•œ íƒœê·¸ì˜ ìœ„ì¹˜ë§Œí¼ html ì½”ë“œë¥¼ ì˜ë¼ë‚´ê¸°
    tag_data = ctx_inner_html[under_start_tag_location:over_end_tag_location]

    print("ì˜ë¼ë‚¸ html : ", tag_data)

    # re (ì •ê·œí‘œí˜„ì‹??) ì‚¬ìš©í•´ì„œ ì›í•˜ëŠ” ë¬¸ìì—´ì´ ì „ì²´ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸
    # get_br_lotation_list = [m.start() for m in re.finditer('<br>', tag_data)]
    get_br_lotation_list = [m.start() for m in re.finditer('<p class="se_textarea">', tag_data)]
    print('<p class=\"se_textarea">íƒœê·¸ ìœ„ì¹˜ í™•ì¸:' ,get_br_lotation_list)

    # ë°˜ë³µë¬¸ìœ¼ë¡œ ìœ„ì˜ listë¥¼ ëŒë©´ì„œ, <br> íƒœê·¸ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¼ëƒ„
    # ìë¥¸ ë°ì´í„°ë¥¼ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì„œ ê²°ê³¼ listì— ë”°ë¡œ ì €ì¥
    post_iteration_count = 0
    sentence_separation_result_list = []
    for i in range(len(get_br_lotation_list)):

        # ë§¨ ì²˜ìŒì€ 0ìœ¼ë¡œ ê³ ì •
        print("ì´ì „ :",get_br_lotation_list[i-1])
        print("í˜„ì¬ :",get_br_lotation_list[i])

        # ì²« ë²ˆì§¸ ë°˜ë³µì¸ ê²½ìš°ì—ë§Œ
        if post_iteration_count == 0:

            present_paragraph_location = get_br_lotation_list[i]

            paragraph_text_data = tag_data[0:present_paragraph_location]
            print("paragraph_text_data:",paragraph_text_data)

            # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
            cleantext = BeautifulSoup(paragraph_text_data, "lxml").text
            cleantext = cleantext.replace("\n\n", "")
            cleantext = cleantext.replace("\xa0", " ")
            st.write("ë¶„ë¦¬ëœ ë¬¸ì¥:", cleantext) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸

            post_iteration_count += 1

        # ë‘ë²ˆì§¸ ë°˜ë³µë¶€í„° ëª¨ë‘ í•´ë‹¹
        else:
            present_paragraph_location = get_br_lotation_list[i]
            previous_paragraph_location = get_br_lotation_list[i-1]

            paragraph_text_data = tag_data[previous_paragraph_location:present_paragraph_location]
            print("paragraph_text_data:",paragraph_text_data)

            # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
            cleantext = BeautifulSoup(paragraph_text_data, "lxml").text
            cleantext = cleantext.replace("\n\n", "")
            cleantext = cleantext.replace("\xa0", " ")
            st.write("ë¶„ë¦¬ëœ ë¬¸ì¥:", cleantext) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸
            st.write("ë¶„ë¦¬ëœ ë¬¸ì¥ ê¸¸ì´:", len(cleantext)) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸

            post_iteration_count += 1
        
        sentence_separation_result_list.append(cleantext)
        sentence_separation_result_list = list(filter(None, sentence_separation_result_list)) # ë¹ˆìš”ì†Œê°€ ìˆìœ¼ë©´ ì œê±°
        sentence_separation_result_list = [word.strip('\xa0') for word in sentence_separation_result_list ] # íŠ¹ì • ë¬¸ìì—´ '\xa0' ì œê±°
        sentence_separation_result_list = [word.strip('\n') for word in sentence_separation_result_list ] # íŠ¹ì • ë¬¸ìì—´ '\n' ì œê±°



    # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
    # cleantext = BeautifulSoup(tag_data, "lxml").text
    # print(cleantext.replace("\n\n", "")) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸
    st.write("ë¬¸ë‹¨ ë¶„ë¦¬ ê²°ê³¼:",sentence_separation_result_list)
    st.write("ë¬¸ë‹¨ ë¶„ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´:",len(sentence_separation_result_list))
    browser.close()
    print("-------------------ì™„ë£Œ------------------")

    return sentence_separation_result_list

# ë„¤ì´ë²„ ë¸”ë¡œê·¸ - iframe ì œê±° í›„ blog.naver.com ë¶™ì´ê¸°
def naver_blog_delete_iframe(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
    res = requests.get(url, headers=headers)
    res.raise_for_status()  # ë¬¸ì œì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    soup = BeautifulSoup(res.text, "lxml")

    src_url = "https://blog.naver.com/" + soup.iframe["src"]

    return src_url

# ë„¤ì´ë²„ë¸”ë¡œê·¸ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ - ìŠ¤ë§ˆíŠ¸ ì—ë””í„° 2.0, ONE í¬í•¨
def naver_blog_text_scraping(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
    res = requests.get(url, headers=headers)
    res.raise_for_status()  # ë¬¸ì œì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    soup = BeautifulSoup(res.text, "lxml")

    pragraph_data = ''
    pragraph_data_list = []
    # clean_pragaph_data = ''
    # clean_pragaph_data_list = []
    result_clean_paragraph_list = []

    print("url:",url)

    if soup.find("div", attrs={"class": "se-main-container"}): # ë³¸ë¬¸ ì „ì²´ íƒœê·¸
        text = soup.find(
            "div", attrs={"class": "se-main-container"}).get_text()
        text = text.replace("\n\n", "")


        find_span = soup.select('p > span') # ë¬¸ë‹¨ ë¶„ë¦¬ë¥¼ ìœ„í•´ text ë°ì´í„°ê°€ ë“¤ì–´ê°€ëŠ” span íƒœê·¸ ì„ íƒí•´ì„œ ê°€ì ¸ì˜¤ê¸°
        print("span_data:::::",soup.select('p > span')) # píƒœê·¸ ì•ˆì˜ span íƒœê·¸ë¥¼ ì„ íƒí•´ì„œ ê°€ì ¸ì˜¤ê¸°
        print(type(find_span)) # íƒ€ì…ì€ bs4ì—ì„œ ì œê³µí•˜ëŠ” resultsetì„ -> setì´ë¯€ë¡œ forë¬¸ì„ í†µí•´ ìš”ì†Œë¥¼ ê°€ì ¸ì˜¤ê¸° ê°€ëŠ¥

        # ê²°ê³¼ë¥¼ ë‹´ì„ list
        result_pragaph_list = []

        # span ë°ì´í„° ì•ˆì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì”© ì¶œë ¥í•˜ê¸° ìœ„í•œ ë°˜ë³µë¬¸
        for text_data in find_span:
          clean_span_text = text_data.get_text() # <span> íƒœê·¸ì•ˆì˜ í…ìŠ¤íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
          print(text_data.get_text()) 
          print("íƒ€ì…:",type(text_data.get_text())) #str
          print("í…ìŠ¤íŠ¸ ê¸¸ì´:",len(clean_span_text))  # ìµœì†Œ 1
          
          # í•œ ë¬¸ìê°€ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ì˜ ê²½ìš° == ì •ìƒì ì¸ ë¬¸ë‹¨ì´ë¼ê³  íŒë‹¨
          # í•œ ê¸€ìë§Œ ì¹˜ê³  ì—”í„°ì¹œ ê²½ìš°ë„ í¬í•¨í•˜ë‚˜, ìœ íš¨í•œ ë°ì´í„°ê°€ ê±°ì˜ ì•„ë‹ ê²ƒì´ë¯€ë¡œ ìš°ì„  ê¸€ììˆ˜ë¡œ ì¡°ê±´ì‹ ì§„í–‰
          if len(clean_span_text) != 1: 
            print("clean_span_text:",clean_span_text) 

            result_pragaph_list.append(clean_span_text) 
        
          else:
            unicode = ord(clean_span_text)
            print("ìœ ë‹ˆì½”ë“œ:",ord(clean_span_text))  

            # 8203ì¸ ê²½ìš° == ZeroWidthSpace ë¬¸ìê°€ ì•„ë‹Œ ê²½ìš°ëŠ” ì •ìƒì ìœ¼ë¡œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì‚½ì…
            if unicode != 8203: 
                print("í•œê¸€ì ë°ì´í„°ì…ë‹ˆë‹¤.")

                result_pragaph_list.append(clean_span_text) 

            else:
                print("ê³µë°±ì—†ëŠ” ê³µë°±ë¬¸ìì…ë‹ˆë‹¤.")

                result_pragaph_list.append("$$space$$") 


          print("~!~!~!~!~!~!~!~!~!")
        
        
        
        for text in result_pragaph_list:
            print("ë¬¸ì¥:",text)
        
            if text == "$$space$$": # ê³µë°±ì—†ëŠ” ê³µë°±ë¬¸ìì¸ ê²½ìš°
                pragraph_data = pragraph_data + '\n' # ì—”í„° ì¶”ê°€
                # continue
            else:
                pragraph_data = pragraph_data + text
        
        split_pragraph_data = pragraph_data.split('\n')
        pragraph_data_list.append(split_pragraph_data)
        pragraph_data_list = list(filter(None, pragraph_data_list)) # ë¹ˆìš”ì†Œê°€ ìˆìœ¼ë©´ ì œê±°
        
        print("pragraph_data_list - type:", type(pragraph_data_list))
        
        # 1ì°¨ ë¶„ë¦¬í•œ ë¬¸ë‹¨ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¹ˆ ìš”ì†Œ(=enter) ì œê±°í•˜ê¸° ìœ„í•¨
        for texts in pragraph_data_list:
            print("texts:",texts)
            print("texts:",len(texts))

            for text_data in texts:
                # print("text_data:",text_data)
                # print("text_data:",len(text_data))

                if len(text_data) == 0:
                    continue
                else:
                    result_clean_paragraph_list.append(text_data)






            # if len(i) == 1: # ê¸¸ì´ê°€ 1ì¸ ê²½ìš°
                # print("ê¸¸ì´:::",len(i))

                # if ord(text) != 8203: # nbspê°€ ì•„ë‹Œ ê²½ìš° == ì¼ë°˜ í…ìŠ¤íŠ¸ ë°ì´í„°
                #     clean_pragaph_data = clean_pragaph_data + text
                #     clean_pragaph_data_list.append(text)        
        
    
        print("ë¬¸ë‹¨í™” ê²°ê³¼:",pragraph_data_list)
        print("ë¬¸ë‹¨í™” ê²°ê³¼ - í´ë¦°:",result_clean_paragraph_list)

        # result_pragaph_list = [word.strip('\n') for word in result_pragaph_list ] # íŠ¹ì • ë¬¸ìì—´ ì œê±°
        # result_pragaph_list = [word.strip('\u200b') for word in result_pragaph_list ] # íŠ¹ì • ë¬¸ìì—´ '\u200b' ì œê±°
        # result_pragaph_list = list(filter(None, result_pragaph_list)) # ë¹ˆìš”ì†Œê°€ ìˆìœ¼ë©´ ì œê±°
        # st.write("result_pragaph_list:",result_pragaph_list)

        # for data in pragraph_data_list:
        #     print("data:",data)

        #     if len(data) == 0:
        #         print("data:",data)
        #         print("ê¸¸ì´ê°€ 0ì¸ ë°ì´í„°ì…ë‹ˆë‹¤.")

        # st.write("pragraph_data_list:",pragraph_data_list)
        # st.write("result_clean_paragraph_list:",result_clean_paragraph_list) # ë¹ˆ ìš”ì†Œ ì œê±°í•œ ìµœì¢… list í™”ë©´ì— ì¶œë ¥
        print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~!!!!!!!!!!!!!!!!~~~~~~~~~~~~~~~~~~~~~~~~")


        # return text
        # return result_pragaph_list
        return result_clean_paragraph_list

    elif soup.find("div", attrs={"id": "postViewArea"}):  # ìŠ¤ë§ˆíŠ¸ ì—ë””í„° 2.0ë„ í¬ë¡¤ë§ í•´ì˜¤ê¸° ìœ„í•¨
        text = soup.find("div", attrs={"id": "postViewArea"}).get_text()
        text = text.replace("\n\n", "")
        return text

    else:
        st.write("ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¤í–‰")
        print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¤í–‰")
        result = newspaper_crawler(url)

        return result  # í¬ë¡¤ë§ ì•ˆë˜ëŠ” ê¸€ ì²˜ë¦¬

# ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§í•´ì„œ ì¶œë ¥
def naver_blog_crawler(url):
    st.write("ë„¤ì´ë²„ë¸”ë¡œê·¸ ë§í¬ì…ë‹ˆë‹¤.")

    blog_text = naver_blog_text_scraping(naver_blog_delete_iframe(url))
    print("ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ : ", blog_text)

    # blog_text_split = blog_text.replace('\n', '$$split$$') # ë¶„ë¦¬ìš© íŠ¹ìˆ˜ë¬¸ì ì‚½ì… 
    # blog_text_split_list = blog_text_split.split('$$split$$') # paragraph ë¶„ë¦¬
    # blog_text_split_list = list(filter(None, blog_text_split_list)) # ë¹ˆìš”ì†Œê°€ ìˆìœ¼ë©´ ì œê±°

    # st.write("ë¬¸ë‹¨ ë¶„ë¦¬ëœ ë°ì´í„°:",blog_text_split_list)
    st.write("ë¬¸ë‹¨ ë¶„ë¦¬ëœ ë°ì´í„°:",blog_text)

    print("-------------------ì™„ë£Œ------------------")

    return blog_text

# newspaper ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ëŸ¬
def newspaper_crawler(url):
    # ì…ë ¥ë°›ì€ urlì„ í†µí•´ í¬ë¡¤ë§ í•´ì˜¤ê¸° ìœ„í•œ ì½”ë“œ - newspaper Article ì‚¬ìš©
    url_str = str(url)
    url_strip = url_str.strip()
    page = Article(url_strip, config=config) # ì–¸ì–´ì„¤ì •ì„ í•˜ì§€ ì•Šìœ¼ë©´ ìë™ê°ì§€ëœë‹¤.
    page.download()
    page.parse() # ì›¹í˜ì´ì§€ parse ì§„í–‰

    # í¬ë¡¤ë§ ëœ ê²°ê³¼ì˜ ë³¸ë¬¸(text)ì„ ë³€ìˆ˜ì— ë”°ë¡œ ì €ì¥
    url_crawling_text = page.text
    url_crawling_title = page.title

    # í¬ë¡¤ë§ ì›ë³¸ ê²°ê³¼ ì¶œë ¥
    st.write("ê²Œì‹œê¸€ ì œëª© : ", url_crawling_title)
    st.write("í¬ë¡¤ë§ ê²°ê³¼ : ")
    st.write(url_crawling_text)

    return url_crawling_text

# í¬ë¡¤ë§ëœ ê²°ê³¼ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•˜ê³  ë²ˆì—­í•´ì£¼ëŠ” í•¨ìˆ˜ - ì´ì „ë²„ì „
def google_translator_old(original_crawling_data):
    # detect ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ ìë™ê°ì§€
    language_sensing = detect(original_crawling_data)
    st.write("ì–¸ì–´ê°ì§€ : ", language_sensing)

    # í¬ë¡¤ë§í•œ ì–¸ì–´ê°€ í•œêµ­ì–´, ì¼ë³¸ì–´ì´ë©´ êµ¬ê¸€ ë²ˆì—­ê¸° ì‹¤í–‰(í•œê¸€,ì¼ë³¸ì–´ë§Œ -> ì˜ì–´)
    if language_sensing == 'ko' or language_sensing == 'ja':
        url_crawling_slice = original_crawling_data[0:5000] # êµ¬ê¸€ë²ˆì—­ê¸° ìµœëŒ€ ì œí•œìˆ˜ë¡œ ì¸í•œ ìŠ¬ë¼ì´ìŠ¤

        # ì˜ì–´ë¡œ ë²ˆì—­
        crawling_data_translate = translator.translate(url_crawling_slice, dest='en')

        # í”„ë¡¬í”„íŠ¸ì— ë‚ ë¦¬ê¸° ìœ„í•´ ì •ë¦¬ëœ í¬ë¡¤ë§ ë°ì´í„° ë³€ìˆ˜ì— ì €ì¥
        for_prompt_data = crawling_data_translate.text

        # ë²ˆì—­ëœ ê²°ê³¼ ì¶œë ¥
        st.write("----- url1 í¬ë¡¤ë§ ê²°ê³¼ ë²ˆì—­(en) -----")
        st.write(for_prompt_data)

    # í¬ë¡¤ë§í•œ ì–¸ì–´ê°€ ì˜ì–´ë©´ ë²ˆì—­ ì—†ì´ ê·¸ëŒ€ë¡œ ì§„í–‰ - ê¸´ ê²½ìš° ìŠ¬ë¼ì´ìŠ¤ë§Œ
    elif language_sensing == 'en':
        for_prompt_data = original_crawling_data[0:10000] # chatgpt ìµœëŒ€ ì œí•œìˆ˜ë¡œ ì¸í•œ ìŠ¬ë¼ì´ìŠ¤
    
    # í¬ë¡¤ë§ ë˜ì§€ ì•Šì€ ê²½ìš° == ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° -> ì—ëŸ¬ì²˜ë¦¬
    else:
        st.error("í¬ë¡¤ë§ëœ ê°’ì´ ë„ˆë¬´ ì ê±°ë‚˜ ëª¨í˜¸í•´ì„œ ì–¸ì–´ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ urlì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="ğŸš¨")


    return for_prompt_data

# í¬ë¡¤ë§ëœ ê²°ê³¼ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•˜ê³  ë²ˆì—­í•´ì£¼ëŠ” í•¨ìˆ˜
# ì–¸ì–´ ìë™ìœ¼ë¡œ ê°ì§€, ì˜ì–´ê°€ ì•„ë‹ˆë¼ë©´ ë²ˆì—­ì„ ì§„í–‰í•˜ëŠ” í˜•íƒœë¡œ ìˆ˜ì •
def google_translator(original_text):
    # detect ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ ìë™ê°ì§€

    if len(original_text) != 0:

        language_sensing = detect(original_text)
        st.write("ë²ˆì—­ ì „!!:",original_text)
        
        # í¬ë¡¤ë§í•œ ì–¸ì–´ê°€ ì˜ì–´ê°€ ì•„ë‹ˆë©´ êµ¬ê¸€ ë²ˆì—­ê¸° ì‹¤í–‰
        if language_sensing != 'en':
            # ì˜ì–´ë¡œ ë²ˆì—­
            crawling_data_translate = translator.translate(original_text, dest='en')
            # í”„ë¡¬í”„íŠ¸ì— ë‚ ë¦¬ê¸° ìœ„í•´ ì •ë¦¬ëœ í¬ë¡¤ë§ ë°ì´í„°(.text) ë³€ìˆ˜ì— ì €ì¥
            for_prompt_data = crawling_data_translate.text
        
        # í¬ë¡¤ë§ ë˜ì§€ ì•Šì€ ê²½ìš° == ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° -> ì—ëŸ¬ì²˜ë¦¬
        else:
            for_prompt_data = ''
            st.error("í¬ë¡¤ë§ëœ ê°’ì´ ë„ˆë¬´ ì ê±°ë‚˜, ì–¸ì–´ê°€ ì„ì—¬ìˆê±°ë‚˜, ê°’ì´ ëª¨í˜¸í•´ì„œ ì–¸ì–´ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ urlì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="ğŸš¨")

        st.write("ë²ˆì—­!!: ", for_prompt_data)

    else:
        st.error("detectí•  ë°ì´í„°ì— ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤.")

    return for_prompt_data

# ìš”ì²­ ì‹¤íŒ¨ì‹œ ì¬ì‹œë„ í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def retry_prompt(answer, prompt):
    # ì‘ë‹µëœ ê²°ê³¼ì˜ ê¸¸ì´ê°€ 0ì´ë©´ (ì—†ìœ¼ë©´) 3íšŒê¹Œì§€ ë‹¤ì‹œ ì‹œë„
    if len(answer) == 0:
        st.write("[Error] chatGPTì˜ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.write("5ì´ˆ í›„ ìë™ìœ¼ë¡œ ì¬ìš”ì²­ë©ë‹ˆë‹¤. (1/3).")
        time.sleep(5)

        first_re_answer = chatGPT(prompt).strip()
        st.write("chatGPT 1ì°¨ ì¬ìš”ì²­ ê²°ê³¼ì˜ ê¸¸ì´ : ", len(first_re_answer))
        st.write("chatGPT 1ì°¨ ì¬ìš”ì²­ ê²°ê³¼: ", first_re_answer)

        if len(first_re_answer) == 0:
            st.write("[Error] chatGPTì˜ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.write("10ì´ˆ í›„ ìë™ìœ¼ë¡œ ì¬ìš”ì²­ë©ë‹ˆë‹¤. (2/3).")
            time.sleep(10)

            second_re_answer = chatGPT(prompt).strip()
            st.write("chatGPT 2ì°¨ ì¬ìš”ì²­ ê²°ê³¼ì˜ ê¸¸ì´ : ", len(second_re_answer))
            st.write("chatGPT 2ì°¨ ì¬ìš”ì²­ ê²°ê³¼: ", second_re_answer)

            if len(second_re_answer) == 0:
                st.write("[Error] chatGPTì˜ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.write("15ì´ˆ í›„ ìë™ìœ¼ë¡œ ì¬ìš”ì²­ë©ë‹ˆë‹¤. (3/3).")
                time.sleep(15)

                third_re_answer = chatGPT(prompt).strip()
                st.write("chatGPT 3ì°¨ ì¬ìš”ì²­ ê²°ê³¼ì˜ ê¸¸ì´ : ", len(third_re_answer))
                st.write("chatGPT 3ì°¨ ì¬ìš”ì²­ ê²°ê³¼: ", third_re_answer)

                if len(third_re_answer) == 0:
                    st.write("[Error] ì ì‹œ í›„ ì¬ìš”ì²­ í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.")



# urlì„ ì…ë ¥í•˜ë©´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
def url_crawler(input_prompt, *urls):

    for url in urls:
        # url domain ê°’ìœ¼ë¡œ ì–´ë–¤ ì‚¬ì´íŠ¸ì¸ì§€ íŒë‹¨
        # 1. ìœ íŠœë¸Œ urlì¸ ê²½ìš°
        if 'youtube' in url:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            st.write("ìœ íŠœë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
            youtube_url = url
            search_value = "v="
            index_no = youtube_url.find(search_value)
            video_id = youtube_url[index_no+2:]

            print("video_id:",video_id)
            st.write("video_id:",video_id)

            # ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = youtube_script_crawling(video_id)
            # ë°˜í™˜ê°’ : í¬ë¡¤ë§ëœ ìë§‰ ë°ì´í„° str

            st.write("-"*50)

        # 2. ë¸ŒëŸ°ì¹˜ì¸ ê²½ìš°
        elif 'brunch' in url:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            # ë¸ŒëŸ°ì¹˜ ê¸€ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = brunch_crawler(url)
            # ë°˜í™˜ê°’ : ë¬¸ë‹¨ë³„ ë°ì´í„°ê°€ ë‹´ê¸´ list

        # 3. ë„¤ì´ë²„ ë¸”ë¡œê·¸ì¸ ê²½ìš°
        elif 'blog.naver.com' in url:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            # ë„¤ì´ë²„ë¸”ë¡œê·¸ ê¸€ì„ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = naver_blog_crawler(url)
            # ë°˜í™˜ ê°’ : paragraphë³„ ë°ì´í„°ê°€ ë‹´ê¸´ list

        # 4. ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        elif 'post.naver.com' in url:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            # ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ê¸€ì„ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = naver_post_crawler(url)
            # ë°˜í™˜ê°’ : ë¬¸ë‹¨ë³„ ë°ì´í„°ê°€ ë‹´ê¸´ list

            # ë¬¸ì¥ì´ ë‹´ê¸´ listê°€ ë°˜í™˜ë¨

        # 5. ì´ì™¸ì˜ ê²½ìš°
        else:
            
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("ê¸°íƒ€ ë§í¬ì…ë‹ˆë‹¤.")
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")

            original_crawling_result = newspaper_crawler(url)
            # ë°˜í™˜ê°’ : í¬ë¡¤ë§ëœ ë³¸ë¬¸ ë°ì´í„° str

            # newspaperë¡œ ëª» ê°€ì ¸ì˜¤ëŠ” ê²½ìš° ëŒ€ë¹„ ì½”ë“œ í•„ìš”
        
        # í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë²ˆì—­(en ì´ì™¸ì˜ ì–¸ì–´ì¸ ê²½ìš°) -> ë¬¸ì¥ ë¶„ë¦¬ í•¨ìˆ˜ì— ë²ˆì—­ í¬í•¨ë˜ì–´ ìˆìŒ
        # for_prompt_data_one = google_translator(original_crawling_result)

        # ìë£Œí˜•ì´ listì¸ ê²½ìš° - ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬í•œ ê²½ìš° ë¬¸ë‹¨ë³„ë¡œ ìš”ì•½ ìš”ì²­
        if isinstance(original_crawling_result, list):
            st.write("ìë£Œí˜•ì´ listì…ë‹ˆë‹¤.")

            final_result.append(pragraph_to_chatGPT(input_prompt, original_crawling_result))


        else:
            # í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë¬¸ì¥ ë©ì–´ë¦¬ë¡œ ë¶„ë¦¬ -> ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬ ë¶ˆê°€ëŠ¥í•˜ë©´ ì°¨ì„ ì±…ìœ¼ë¡œ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•˜ê²Œ í•¨
            # í˜„ì¬ëŠ” ë§ˆì¹¨í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬ë§Œ ê°€ëŠ¥
            st.write("ë¬¸ë‹¨ ì¸ì‹ì´ ì–´ë ¤ìš´ ë°ì´í„°ì…ë‹ˆë‹¤.. ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.")
            st.write(input_prompt)
            st.write(original_crawling_result)

            # ë§ˆì¹¨í‘œ ê¸°ì¤€ì˜ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
            final_result.append(separate_by_crawled_data_sentence(input_prompt, original_crawling_result))

    

    st.write("-"*50)


# ë¬¸ì¥ì„ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê¸€ììˆ˜ ê¸°ì¤€ë§Œí¼ ë‚˜ëˆ ì„œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•˜ê¸°
# listì˜ ìš”ì†Œë¥¼ ë¶„ë¦¬í•´ì£¼ëŠ” í•¨ìˆ˜
# lst : ë¶„ë¦¬í•˜ê³ ì í•˜ëŠ” ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸
# n : ëª‡ ê°œì”© ë¶„í• í• ì§€
def list_chunk(lst, n):
    return [lst[i:i+n] for i in range(0, len(lst), n)]


# í¬ë¡¤ë§ëœ ì „ì²´ ë°ì´í„°ë¥¼ ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ë³„ë¡œ ë¶„ë¦¬í•´ì£¼ëŠ” í•¨ìˆ˜  + chatGPT ì— í”„ë¡¬í”„íŠ¸ ë‚ ë¦¬ê¸°
# in : í¬ë¡¤ë§ ëœ ë²ˆì—­ ì „ì˜ original ë°ì´í„° ì „ë¶€
# out : chatGPT ìš”ì²­ ê²°ê³¼
def separate_by_crawled_data_sentence(input_prompt, crawled_data):
    st.write("í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤..")

    # ëª‡ë²ˆ ë°˜ë³µë˜ëŠ”ì§€ ì²´í¬í•˜ê¸° ìœ„í•¨
    roop_count = 0
    # ì²­í¬ë³„ë¡œ í•©ì³ì„œ ìš”ì•½í•œ ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    combine_chatGPT_result = [] # ì¶”í›„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°”ê¿”ë„ ì¢‹ì„ë“¯ 

    # ë‚˜ëˆŒ ë¬¸ì¥ ìˆ˜, í˜„ì¬ëŠ” í•˜ë“œì½”ë”©ìœ¼ë¡œ êµ¬í˜„
    sentence_division = 25
    st.write("ë¶„ë¦¬ë˜ëŠ” ë¬¸ì¥ì˜ ê°¯ìˆ˜ :", sentence_division)

    # ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ ì„œ ë¦¬ìŠ¤íŠ¸ì— ë„£ê¸°
    split_data_remove_enter = crawled_data.replace('\n','') # ì—”í„° ì œê±°
    split_data_token = split_data_remove_enter.split() # ìŠ¤í˜ì´ìŠ¤(ë„ì–´ì“°ê¸°) ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•´ì„œ token ê°¯ìˆ˜ ì•Œì•„ë‚´ê¸°
    split_data = split_data_remove_enter.split('.') # ì˜¨ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬

    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°’(sentence_division)ë§Œí¼ ë¬¸ì¥ì„ ë©ì–´ë¦¬í™” í•´ì¤Œ
    list_chunked = list_chunk(split_data, sentence_division)

    # ë‚˜ë‰˜ì–´ì§„ ì²­í¬ì˜ ì´ ê¸¸ì´ë§Œí¼ ë°˜ë³µí•  ë°˜ë³µë¬¸ 
    for i in range(len(list_chunked)):
        roop_count = roop_count + 1
        st.write("roop_count: ",roop_count)

        # í•©ì³ì§„ ë¬¸ì¥ì„ ë‹´ì„ ë³€ìˆ˜ ì„ ì–¸ - ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ìŠ¤íŠ¸ë§ ë³€ìˆ˜ì— ë‹´ê¸°
        combine_list = []
        combine_string = ''
        
        # ê¸°ì¡´ì˜ í•©ì¹˜ëŠ” ê³¼ì •ì€ í•„ìš” ì—†ìŒ
        for j in range(len(list_chunked[i])):
            # combine_list.append(list_chunked[i][j]) # ë¦¬ìŠ¤íŠ¸ì— ë„£ì–´ í•©ì¹˜ê¸° (ë°©ë²• 1)
            combine_string = combine_string + list_chunked[i][j] + '.' # ìŠ¤íŠ¸ë§ìœ¼ë¡œ í•©ì¹˜ê¸° (ë°©ë²• 2)

        print("combine_list: ", combine_list)
        # st.write("combine_string: ", combine_string)
        # print("ë²ˆì—­: ", google_translator(combine_string))

        # ë²ˆì—­ ì™„ë£Œëœ ë¬¸ì¥ ì²­í¬ ì €ì¥
        combine_string_translate_data = google_translator(combine_string)

        # í”„ë¡¬í”„íŠ¸ì— ìš”ì²­í•˜ëŠ” ë¶€ë¶„ 
        prompt_command = input_prompt + combine_string_translate_data
        st.write(len(prompt_command))

        # chatGPTì— ìš”ì²­í•˜ê¸°
        st.write("chatGPT ì‹¤í–‰ì¤‘...")
        result = chatGPT(prompt_command).strip()
        st.write("chatGPT ìš”ì²­ê²°ê³¼: ", result)
        st.write(len(result))

        # ë§Œì•½ ê²°ê³¼ì˜ ê¸¸ì´ê°€ 0ì´ë¼ë©´ == ì˜¤ë¥˜ê°€ ë‚˜ì„œ ê²°ê³¼ê°€ ì‘ë‹µë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¬ì‹¤í–‰
        retry_prompt(result, prompt_command)

        # ê²°ê³¼ê°€ 0ì´ ì•„ë‹ˆë¼ë©´ == ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ë©´ ìµœì¢…ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ê²°ê³¼ë¥¼ ë¦¬í„´í•´ì¤€ë‹¤.
        if len(result) != 0:
            result_to_kr = translator.translate(result, dest='ko').text
            st.write("chatGPT ìš”ì²­ê²°ê³¼(ë²ˆì—­): ", result_to_kr)
            combine_chatGPT_result.append(result_to_kr)
        st.write("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

    # ê° ì²­í¬ì˜ ìš”ì•½ëœ ê²°ê³¼ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•´ì¤Œ -> ìµœì¢… í•œë²ˆë§Œ ì¶œë ¥ë˜ë„ë¡ ì£¼ì„ì²˜ë¦¬
    # st.write("____________________________")
    # st.write("ì „ì²´ ê²°ê³¼: ", combine_chatGPT_result)

    # return combine_string_translate_data
    return combine_chatGPT_result

# ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì§„ ë°ì´í„°ë¥¼ chatGPT promptì— ë‚ ë¦¬ê¸° ìœ„í•œ í•¨ìˆ˜
def pragraph_to_chatGPT(input_prompt, paragraph_list):
    # ëª‡ë²ˆ ë°˜ë³µë˜ëŠ”ì§€ ì²´í¬í•˜ê¸° ìœ„í•¨
    roop_count = 0
    # ì²­í¬ë³„ë¡œ í•©ì³ì„œ ìš”ì•½í•œ ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    combine_chatGPT_result = [] # ì¶”í›„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°”ê¿”ë„ ì¢‹ì„ë“¯ 
    paragraph_translate_data = []

    print(paragraph_list)
    
    # ë‚˜ë‰˜ì–´ì§„ ì²­í¬ì˜ ì´ ê¸¸ì´ë§Œí¼ ë°˜ë³µí•  ë°˜ë³µë¬¸ 
    for i in range(len(paragraph_list)):
        roop_count = roop_count + 1
        st.write("roop_count: ",roop_count)

        # í•©ì³ì§„ ë¬¸ì¥ì„ ë‹´ì„ ë³€ìˆ˜ ì„ ì–¸ - ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ìŠ¤íŠ¸ë§ ë³€ìˆ˜ì— ë‹´ê¸°
        # combine_list = []
        # combine_string = ''
        
        # listê°€ ì´ë¯¸ í•©ì³ì ¸ ìˆìœ¼ë¯€ë¡œ ë¬¸ì¥ë¼ë¦¬ í•©ì¹  í•„ìš”ëŠ” ì—†ìŒ
        # for j in range(len(paragraph_list[i])):
        #     # combine_list.append(list_chunked[i][j]) # ë¦¬ìŠ¤íŠ¸ì— ë„£ì–´ í•©ì¹˜ê¸° (ë°©ë²• 1)
        #     combine_string = combine_string + list_chunked[i][j] + '.' # ìŠ¤íŠ¸ë§ìœ¼ë¡œ í•©ì¹˜ê¸° (ë°©ë²• 2)

        # print("combine_list: ", combine_list)
        # st.write("combine_string: ", combine_string)
        # print("ë²ˆì—­: ", google_translator(combine_string))

        # ë²ˆì—­ ì™„ë£Œëœ ë¬¸ì¥ ì²­í¬ ì €ì¥
        paragraph_translate_data = google_translator(paragraph_list[i])

        # í”„ë¡¬í”„íŠ¸ì— ìš”ì²­í•  ë°ì´í„° ( ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°’ + ë¬¸ë‹¨ )
        prompt_command = input_prompt + paragraph_translate_data
        st.write(len(prompt_command))

        # chatGPTì— ìš”ì²­í•˜ê¸°
        st.write("chatGPT ì‹¤í–‰ì¤‘...")
        result = chatGPT(prompt_command).strip()
        st.write("chatGPT ìš”ì²­ê²°ê³¼: ", result)
        st.write(len(result))

        # ë§Œì•½ ê²°ê³¼ì˜ ê¸¸ì´ê°€ 0ì´ë¼ë©´ == ì˜¤ë¥˜ê°€ ë‚˜ì„œ ê²°ê³¼ê°€ ì‘ë‹µë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¬ì‹¤í–‰
        retry_prompt(result, prompt_command)

        # ê²°ê³¼ê°€ 0ì´ ì•„ë‹ˆë¼ë©´ == ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ë©´ ìµœì¢…ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ê²°ê³¼ë¥¼ ë¦¬í„´í•´ì¤€ë‹¤.
        if len(result) != 0:
            result_to_kr = translator.translate(result, dest='ko').text
            st.write("chatGPT ìš”ì²­ê²°ê³¼(ë²ˆì—­): ", result_to_kr)
            combine_chatGPT_result.append(result_to_kr)
        st.write("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

    # ê° ì²­í¬ì˜ ìš”ì•½ëœ ê²°ê³¼ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•´ì¤Œ -> ìµœì¢… í•œë²ˆë§Œ ì¶œë ¥ë˜ë„ë¡ ì£¼ì„ì²˜ë¦¬
    # st.write("____________________________")
    # st.write("ì „ì²´ ê²°ê³¼: ", combine_chatGPT_result) # ìµœì¢…ì ìœ¼ë¡œ í•œ ë²ˆë§Œ ì¶œë ¥ë  ìˆ˜ ìˆê²Œë” ìˆ˜ì • í•„ìš”

    # return paragraph_translate_data
    return combine_chatGPT_result

# í¬ë¡¤ë§ í”„ë¡œê·¸ë¨ ì‹¤í–‰ìš© í•¨ìˆ˜
def start_button(first_url, second_url, third_url, fourth_url, fifth_url, url_count, input_prompt):

    # ì…ë ¥ê°’ì´ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ë²„íŠ¼ ë¹„í™œì„±í™”
    disabled_flag = True

    # ì…ë ¥ê°’ì´ ìƒê¸°ë©´ ë²„íŠ¼ í™œì„±í™” - ì¡°ê±´ 3ê°œ ë§ì¶°ì„œ ìˆ˜ì • í•„ìš”
    if len(first_url) != 0:
        disabled_flag = False

    # ë²„íŠ¼ í´ë¦­ì‹œ ì‹¤í–‰
    if st.button("ì‹œì‘", disabled=disabled_flag):
        
        # url 1ê°œë§Œ ì…ë ¥í•˜ëŠ” ê²½ìš°
        if url_count == 1:
            st.write("1")
            url_crawler(input_prompt, first_url)

        # urlì„ 2ê°œ ì…ë ¥í•˜ëŠ” ê²½ìš°
        elif url_count == 2:
            st.write("2")
            url_crawler(input_prompt, first_url, second_url)

        # url 3ê°œ ì…ë ¥í•˜ëŠ” ê²½ìš°
        elif url_count == 3:
            st.write("3")
            url_crawler(input_prompt, first_url, second_url, third_url)

        elif url_count == 4:
            st.write("4")
            url_crawler(input_prompt, first_url, second_url, third_url, fourth_url)

        elif url_count == 5:
            st.write("5")
            url_crawler(input_prompt, first_url, second_url, third_url, fourth_url, fifth_url)
        
        st.write("ìµœì¢…:",final_result)
        st.write("ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    else:
        st.write(" ")


# UI êµ¬í˜„ìš© ë©”ì¸ í•¨ìˆ˜
def main():

    # button í•¨ìˆ˜ì— ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸° ì„ ì–¸
    input_prompt = ""
    first_url = ""
    second_url = ""
    third_url = ""
    fourth_url = ""
    fifth_url = ""
    # original_crawling_result = ''

    # ì œëª©
    st.title('URLë¡œ í¬ë¡¤ë§í•´ì„œ chatGPTì— ëª…ë ¹í•˜ê¸°')
    st.write("ìµœì¢… ì—…ë°ì´íŠ¸ : 2023.02.28 ")

    # ì•ˆë‚´ ë¬¸êµ¬
    st.info("""URL í¬ë¡¤ë§ í”„ë¡œê·¸ë¨ ë™ì‘ ê³¼ì •\n
        1. í¬ë¡¤ë§í•˜ê³ ì í•˜ëŠ” URL ì…ë ¥
        2. í¬ë¡¤ë§ ì§„í–‰
        3. í¬ë¡¤ë§ ëœ ë°ì´í„°ê°€ ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° googletransë¡œ ë²ˆì—­
        4. ë²ˆì—­ëœ ê²°ê³¼ë¥¼ chatGPT promptì— ë‹´ì•„ì„œ ì „ì†¡
        (ì „ì†¡í˜•íƒœ : [ì‚¬ìš©ìê°€ ì…ë ¥í•œ prompt] + í¬ë¡¤ë§ ëœ en ë°ì´í„° )
        5. ìš”ì²­ ê²°ê³¼ ì¶œë ¥ (kr)
        """, icon="â„¹ï¸")

    # ë‚´ìš©
    st.write("1. í¬ë¡¤ë§ í•  URLì˜ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ê³  ê°’ì„ ì…ë ¥í•˜ì„¸ìš”.")

    url_count = st.radio(
        "í¬ë¡¤ë§í•  URL ê°œìˆ˜ ì„ íƒ",
        (1, 2, 3, 4, 5))

    if url_count == 1:
        first_url = st.text_input('URL ì…ë ¥ì¹¸ 1')
        st.write("ì…ë ¥í™•ì¸ : ", first_url)

    elif url_count == 2:
        first_url = st.text_input('URL ì…ë ¥ì¹¸ 1')
        st.write("ì…ë ¥í™•ì¸ : ", first_url)

        second_url = st.text_input('url ì…ë ¥ì¹¸ 2')
        st.write("ì…ë ¥í™•ì¸ : ", second_url)

    elif url_count == 3:
        first_url = st.text_input('URL ì…ë ¥ì¹¸ 1')
        st.write("ì…ë ¥í™•ì¸ : ", first_url)

        second_url = st.text_input('url ì…ë ¥ì¹¸ 2')
        st.write("ì…ë ¥í™•ì¸ : ", second_url)

        third_url = st.text_input('url ì…ë ¥ì¹¸ 3')
        st.write("ì…ë ¥í™•ì¸ : ", third_url)

    elif url_count == 4:
        first_url = st.text_input('URL ì…ë ¥ì¹¸ 1')
        st.write("ì…ë ¥í™•ì¸ : ", first_url)

        second_url = st.text_input('url ì…ë ¥ì¹¸ 2')
        st.write("ì…ë ¥í™•ì¸ : ", second_url)

        third_url = st.text_input('url ì…ë ¥ì¹¸ 3')
        st.write("ì…ë ¥í™•ì¸ : ", third_url)

        fourth_url = st.text_input('url ì…ë ¥ì¹¸ 4')
        st.write("ì…ë ¥í™•ì¸ : ", fourth_url)

    else:
        first_url = st.text_input('URL ì…ë ¥ì¹¸ 1')
        st.write("ì…ë ¥í™•ì¸ : ", first_url)

        second_url = st.text_input('url ì…ë ¥ì¹¸ 2')
        st.write("ì…ë ¥í™•ì¸ : ", second_url)

        third_url = st.text_input('url ì…ë ¥ì¹¸ 3')
        st.write("ì…ë ¥í™•ì¸ : ", third_url)

        fourth_url = st.text_input('url ì…ë ¥ì¹¸ 4')
        st.write("ì…ë ¥í™•ì¸ : ", fourth_url)

        fifth_url = st.text_input('url ì…ë ¥ì¹¸ 5')
        st.write("ì…ë ¥í™•ì¸ : ", fifth_url)


    st.write("")

    st.write("2. ChatGPTì— ìš”ì²­í•  promptë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì…ë ¥í•œ prompt ë’¤ì— í¬ë¡¤ë§ëœ ê²°ê³¼ê°€ ë¶™ìŠµë‹ˆë‹¤.)")
    input_prompt = st.text_input(label='ex) Summarize the following ...', value="Summarize the following ")
    st.write("ì…ë ¥í™•ì¸ : ", input_prompt)

    start_button(first_url, second_url, third_url, fourth_url, fifth_url, url_count, input_prompt)

# íŒŒì¼ì„ ì‹¤í–‰ì‹œí‚¤ë©´ main í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë„ë¡ í•¨
if __name__ == '__main__':
    main()