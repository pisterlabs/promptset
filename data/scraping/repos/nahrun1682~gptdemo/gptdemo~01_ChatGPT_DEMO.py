import os
import logging
from dotenv import load_dotenv
import streamlit as st

from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import ChatMessage

from libs.web_research_retriever import web_research_retriever

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path)
else:
    logger.error("ç’°å¢ƒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    st.error("ç’°å¢ƒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

openai_api_key = os.environ.get("OPENAI_API_KEY")
if not openai_api_key:
    logger.error("OpenAI API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.error("OpenAI API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

# Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "chat_log" not in st.session_state:
    st.session_state.chat_log = []

if "messages" not in st.session_state:
    st.session_state["messages"] = [ChatMessage(role="assistant", content="ãªã‚“ã§ã‚‚èã„ã¦ã­")]

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

#ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
st.title('ğŸ¦œChatGPT DEMO')
st.write('githubï¼šhttps://github.com/nahrun1682/gptdemo')
# model_name = st.sidebar.radio(
#     "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ(1106ãŒç¾åœ¨æœ€æ–°ç‰ˆ):",
#     ("gpt-3.5-turbo", "gpt-4", "gpt-3.5-turbo-1106","gpt-4-1106-preview"),
#     index=2)
# temperature = st.sidebar.slider("Temperature(å¤§ãã„ã»ã©æ­£ç¢ºã€ä½ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ):", min_value=0.0, max_value=1.0, value=1.0, step=0.1)

#ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®æŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
with st.sidebar:
    st.header('è¨­å®š')
    
    with st.expander("ãƒ¢ãƒ‡ãƒ«é¸æŠ"):
        model_name = st.radio(
            "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ(1106ãŒç¾åœ¨æœ€æ–°ç‰ˆ):",
            ("gpt-3.5-turbo", "gpt-4", "gpt-3.5-turbo-1106", "gpt-4-1106-preview"),
            index=3
        )

    with st.expander("ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š"):
        temperature = st.slider(
            "Temperature(å¤§ãã„ã»ã©æ­£ç¢ºã€ä½ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ):", 
            min_value=0.0, max_value=1.0, value=1.0, step=0.1
        )

# ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

if prompt := st.chat_input():
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))
    st.chat_message("user").write(prompt)

    with st.chat_message("assistant"):
        try:
            stream_handler = StreamHandler(st.empty())
            llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=model_name,temperature=temperature,streaming=True, callbacks=[stream_handler])
            # print(model_name)
            response = llm(st.session_state.messages)
            st.session_state.messages.append(ChatMessage(role="assistant", content=response.content))
        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        
