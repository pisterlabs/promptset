"""
	å†å²è®°å½•æ¨¡å—
		What?
			ä¿ç•™å¯¹è¯çš„ä¸Šä¸‹æ–‡
		What?
			1.ç”¨äº chain ã€agent ç­‰ç»“æ„
			2.å¯ä»¥å¯¹ memory è¿›è¡Œä¿®æ”¹
			3.å¯ä»¥åˆ©ç”¨æ•°æ®åº“å¯¹å†å²è®°å½•è¿›è¡Œå­˜å‚¨, æ¯”å¦‚ mongoDB æˆ– AWS
"""


from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import OpenAI


# åˆ›å»º OpenAI è¯­è¨€æ¨¡å‹çš„å®ä¾‹
llm = OpenAI()


# å®šä¹‰æ¨¡æ¿ï¼Œå®ƒå°†åŒ…å«èŠå¤©å†å²å’Œäººç±»çš„è¾“å…¥
template = """
	ä½ æ˜¯ä¸€ä¸ªèŠå¤© BOT, èƒ½å¤Ÿä¿ç•™èŠå¤©çš„ä¸Šä¸‹æ–‡ã€‚
 
 	{chat_history}
  
    äººç±»:{human_input}
    BOT:
"""

# åˆ›å»º PromptTemplate å®ä¾‹

prompt = PromptTemplate(
	input_variables=["chat_history", "human_input"], 
 	template=template
)


# åˆå§‹åŒ–èŠå¤©å†å²
chat_history = ""


# æ¨¡æ‹Ÿä¸€äº›äººç±»è¾“å…¥
# human_inputs = ["ä½ æ˜¯æˆ‘çš„å°çŒ«å«å˜Ÿå˜Ÿï¼", "ä½ ä¼šå–µå–µå«", "ä½ å¯ä»¥å¸®æˆ‘æ‹¿å¤–å–!"]
human_inputs = [""] # ğŸ”¥å¯ä»¥æŠŠç”¨æˆ·çš„è¾“å…¥å­˜ä¸ºå†å²è®°å½•æˆ–è€…ã€ğŸŒŸå­˜å…¥æ•°æ®åº“ã€‘ï¼


# å¯¹äºæ¯ä¸ªäººç±»è¾“å…¥ï¼Œç”Ÿæˆå¹¶æ‰“å°æœºå™¨äººçš„å›å¤
# å¾ªç¯ï¼Œç›´åˆ°ç”¨æˆ·é€‰æ‹©ã€åœæ­¢ã€‘
while True:
    # ğŸŒŸè·å–ç”¨æˆ·çš„è¾“å…¥
    human_input = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'é€€å‡º'ä»¥ç»“æŸå¯¹è¯ï¼‰: ")
    
    # æ£€æŸ¥æ˜¯å¦é€€å‡º
    if human_input.lower() == 'é€€å‡º':
        break
    
    
    # 1.æ›´æ–°èŠå¤©å†å²ä»¥åŒ…æ‹¬æ–°çš„äººç±»è¾“å…¥ ï½œ 2.ç”Ÿæˆæ–°çš„å›å¤
    chat_history += f"äººç±»:{human_input}\n" 
	
	# ğŸŒŸã€å¸¦æœ‰å†å²è®°å½•çš„æç¤ºè¯ï¼ã€‘ä½¿ç”¨æ¨¡æ¿å’Œè¾“å…¥ç”Ÿæˆæç¤º
    generated_prompt = prompt.format(
		chat_history=chat_history, 
		human_input=human_input
	)
	
	# ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤
    res = llm.generate([generated_prompt]) # ğŸŒŸ åŒ…è£…æˆåˆ—è¡¨
	
	# ä»å“åº”ä¸­æå–æ–‡æœ¬
    bot_response_text = res.generations[0][0].text  # æå–ç¬¬ä¸€ä¸ª Generation å¯¹è±¡çš„æ–‡æœ¬
	
	# ä¸ºä¸‹ä¸€ä¸ªæœºå™¨äººçš„å¾ªç¯å›å¤æ·»åŠ åˆ°èŠå¤©å†å²ä¸­
    chat_history += f"BOT:{bot_response_text}\n"

	# æ‰“å°æœºå™¨äººçš„å›å¤
    print(f"BOT:{bot_response_text}")